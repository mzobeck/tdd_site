[
  {
    "objectID": "docs/research/index.html",
    "href": "docs/research/index.html",
    "title": "Research Projects",
    "section": "",
    "text": "TL;DR: We modeled future von Willebrand Factor (VWF) levels based on previous VWF levels and other covariates to describe a decision threshold for retesting for von Willebrand disease (VWD) in adolescent females. We found that for an initial level of 80%, there was around an 8% chance the patient could still be found to have VWD, and at an initial level of 90%, there was around a 3% chance of VWD on retesting. These findings can help physicians decide who and when to retest for VWD. I made the above graph and find it quite pleasing.\nCitation: Cohen CT, Zobeck M*, Powers JM. Initial von Willebrand factor antigen values in adolescent females predict future values. Haemophilia 2023;29:1547–55. https://doi.org/10.1111/hae.14865. (*I was co-first author)\n\n\n\n\n\n\n\n\n\nTL;DR: We described single nucleotide polymorphisms that were associated with creatinine increased after high-dose methotrexate or with delayed methotrexate clearance. Some of these associations had not been previously described, others replicated previous findings (a rarity in the pharmacogenomics literature!). This project advance our understanding of who is at most risk for methotrexate toxicity and will hopefully help with the development of strategies to avoid toxicities before they develop. I made the above graphics for the paper and really like style.\nCitation: Zobeck M, Bernhardt MB, Kamdar KY, Rabin KR, Lupo PJ, Scheurer ME. Novel and replicated clinical and genetic risk factors for toxicity from high-dose methotrexate in pediatric acute lymphoblastic leukemia. Pharmacotherapy: The Journal of Human Pharmacology and Drug Therapy 2023;43:205–14. https://doi.org/10.1002/phar.2779.\n\n\n\n\n\n\n\n\n\nTL;DR: We described single nucleotide polymorphisms that were associated with nephrotoxicty from high-dose methotrexate that was severe enough to require glucarpidase, an extremely expensive drug that helps us avoid kidney failure from methotrexate. We found a novel association between a variant in the ABCC4 gene and requiring glucarpidase (this association was reproduced in the other methotrexate study above). We also found that Hispanic patients are at much higher risk for requiring glucarpidase compared to non-Hispanic patients, an association we are still trying to unravel.\nCitation: Zobeck M, Bernhardt MB, Kamdar KY, Rabin KR, Lupo PJ, Scheurer ME. Novel risk factors for glucarpidase use in pediatric acute lymphoblastic leukemia: Hispanic ethnicity, age, and the ABCC4 gene. Pediatric Blood & Cancer 2021;68:e29036. https://doi.org/10.1002/pbc.29036."
  },
  {
    "objectID": "docs/research/index.html#spotlight-publications",
    "href": "docs/research/index.html#spotlight-publications",
    "title": "Research Projects",
    "section": "",
    "text": "TL;DR: We modeled future von Willebrand Factor (VWF) levels based on previous VWF levels and other covariates to describe a decision threshold for retesting for von Willebrand disease (VWD) in adolescent females. We found that for an initial level of 80%, there was around an 8% chance the patient could still be found to have VWD, and at an initial level of 90%, there was around a 3% chance of VWD on retesting. These findings can help physicians decide who and when to retest for VWD. I made the above graph and find it quite pleasing.\nCitation: Cohen CT, Zobeck M*, Powers JM. Initial von Willebrand factor antigen values in adolescent females predict future values. Haemophilia 2023;29:1547–55. https://doi.org/10.1111/hae.14865. (*I was co-first author)\n\n\n\n\n\n\n\n\n\nTL;DR: We described single nucleotide polymorphisms that were associated with creatinine increased after high-dose methotrexate or with delayed methotrexate clearance. Some of these associations had not been previously described, others replicated previous findings (a rarity in the pharmacogenomics literature!). This project advance our understanding of who is at most risk for methotrexate toxicity and will hopefully help with the development of strategies to avoid toxicities before they develop. I made the above graphics for the paper and really like style.\nCitation: Zobeck M, Bernhardt MB, Kamdar KY, Rabin KR, Lupo PJ, Scheurer ME. Novel and replicated clinical and genetic risk factors for toxicity from high-dose methotrexate in pediatric acute lymphoblastic leukemia. Pharmacotherapy: The Journal of Human Pharmacology and Drug Therapy 2023;43:205–14. https://doi.org/10.1002/phar.2779.\n\n\n\n\n\n\n\n\n\nTL;DR: We described single nucleotide polymorphisms that were associated with nephrotoxicty from high-dose methotrexate that was severe enough to require glucarpidase, an extremely expensive drug that helps us avoid kidney failure from methotrexate. We found a novel association between a variant in the ABCC4 gene and requiring glucarpidase (this association was reproduced in the other methotrexate study above). We also found that Hispanic patients are at much higher risk for requiring glucarpidase compared to non-Hispanic patients, an association we are still trying to unravel.\nCitation: Zobeck M, Bernhardt MB, Kamdar KY, Rabin KR, Lupo PJ, Scheurer ME. Novel risk factors for glucarpidase use in pediatric acute lymphoblastic leukemia: Hispanic ethnicity, age, and the ABCC4 gene. Pediatric Blood & Cancer 2021;68:e29036. https://doi.org/10.1002/pbc.29036."
  },
  {
    "objectID": "docs/research/index.html#other-publications",
    "href": "docs/research/index.html#other-publications",
    "title": "Research Projects",
    "section": "Other Publications",
    "text": "Other Publications\nFor most of these, I served as the primary analyst who conducted the statistical analysis and helped to interpret the results.\n\nMcEvoy MT, Siegel DA, Dai S, Okcu MF, Zobeck M, Venkatramani R, et al. Pediatric rhabdomyosarcoma incidence and survival in the United States: An assessment of 5656 cases, 2001–2017. Cancer Medicine 2023;12:3644–56. https://doi.org/10.1002/cam4.5211.\nHolmes DM, Matatiyo A, Mpasa A, Huibers MHW, Manda G, Tomoka T, et al. Outcomes of Wilms tumor therapy in Lilongwe, Malawi, 2016–2021: Successes and ongoing research priorities. Pediatric Blood & Cancer 2023;70:e30242. https://doi.org/10.1002/pbc.30242.\nHarris RD, Bernhardt MB, Zobeck M, Taylor OA, Gramatges MM, Schafer ES, et al. Ethnic-specific predictors of neurotoxicity among patients with pediatric acute lymphoblastic leukemia after high-dose methotrexate. Cancer 2023;129:1287–94. https://doi.org/10.1002/cncr.34646.\nCohen CT, Zobeck M, Kim TO, Sartain SE, Raffini L, Srivaths L. Adolescent acquired thrombotic thrombocytopenic purpura: An analysis of the Pediatric Health Information System database. Thrombosis Research 2023;222:63–7. https://doi.org/10.1016/j.thromres.2022.12.011.\nCohen CT, Zobeck M, Han H, Spinner JA, Powers JM, Lee-Kim Y, et al. Bleeding outcomes and management of supratherapeutic episodes secondary to warfarin in children: A single center 10-year experience. Thrombosis Research 2023;228:148–50. https://doi.org/10.1016/j.thromres.2023.06.013.\nPrudowsky ZD, Bledsaw K, Staton S, Zobeck M, DeJean J, Johnson-Bishop L, et al. Chlorhexidine gluconate (CHG) foam improves adherence, satisfaction, and maintains central line associated infection rates compared to CHG wipes in pediatric hematology-oncology and bone marrow transplant patients. Pediatric Hematology and Oncology 2022;0:1–13. https://doi.org/10.1080/08880018.2022.2090644.\nHoward TS, Valdes SO, Zobeck M, Lam WW, Miyake CY, Rochelson E, et al. Ripple mapping: A precise tool for atrioventricular nodal reentrant tachycardia ablation. Journal of Cardiovascular Electrophysiology 2022;33:1183–9. https://doi.org/10.1111/jce.15491.\nAgarwal S, Cohen CT, Zobeck M, Jacobi PM, Sartain SE. Downregulation of thrombomodulin-thrombin-activated protein C pathway as a mechanism for SARS-CoV-2 induced endotheliopathy and microvascular thrombosis. Thrombosis Update 2022;8:100116. https://doi.org/10.1016/j.tru.2022.100116.\nMullikin D, Ranch D, Khalfe Y, Lucari B, Zobeck M, Assanasen C, et al. Hispanic ethnicity is associated with prolonged clearance of high dose methotrexate and severe nephrotoxicity in children and adolescents with acute lymphoblastic leukemia. Leukemia & Lymphoma 2020;61:2771–4. https://doi.org/10.1080/10428194.2020.1783445.\nKnadler JJ, Zobeck M, Masand P, Sartain S, Kyle WB. In Utero Aortic Arch Thrombosis Masquerading as Interrupted Aortic Arch: A Case Report and Review of the Literature. Pediatr Cardiol 2019;40:658–63. https://doi.org/10.1007/s00246-019-02068-5."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nThe Epistemic Arc - a conceptual map about how to learn from data in the presence of uncertainty\n\n\n13 min\n\n\n\nStatistics & Heuristics\n\n\n\n\n\n\n\nJun 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPediatric cancer treatment abandonment: a tragic but preventable event\n\n\n5 min\n\n\n\nGlobal PHO\n\n\n\n\n\n\n\nApr 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to measure anything in Global Health: 5. Calibration techniques\n\n\n3 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nApr 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to measure anything in Global Health: 4. Calibrate your judgment\n\n\n3 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nApr 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to measure anything in Global Health: 3. The Definition of Risk\n\n\n3 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nApr 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to measure anything in Global Health: 2. The universal approach to measurement\n\n\n3 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nApr 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to measure anything in Global Health: 1. Foundations of Measurement\n\n\n4 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nApr 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistributed intelligence is the key to success for high-performing health systems in Global Health\n\n\n3 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nApr 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic beasts and how to slay them - Part 1\n\n\n4 min\n\n\n\nConfident Uncertainty\n\n\n\n\n\n\n\nMar 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the two types of uncertainty will help you live more confidently in a stochastic world\n\n\n4 min\n\n\n\nConfident Uncertainty\n\n\n\n\n\n\n\nMar 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData science is a saddle for the epistemic rodeo: healthcare as a complex system\n\n\n4 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nMar 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet the data flow: how to move information to the people who need it in a Global Health organization\n\n\n4 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nMar 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThree essential quality control practices to improve the reliability of Data Science in Global Health\n\n\n3 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nMar 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAristotle’s sage advice to improve Data Science in Global Health\n\n\n3 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nMar 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to walk the unsexy path toward sophisticated Data Science in Global Health: Data Management\n\n\n4 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nMar 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Science has a garbage problem\n\n\n2 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nMar 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10 ways Global Health organizations can use routine data to improve patient care\n\n\n2 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nMar 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA simple method to monitor a catchment area and improve disease diagnosis rates\n\n\n3 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nMar 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThree Steps To Improve How You Measure Patient Outcomes\n\n\n5 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nMar 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThree Reasons Global Health needs Data Science\n\n\n4 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nMar 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe single best way to cure more childhood cancers\n\n\n2 min\n\n\n\nGlobal PHO\n\n\n\n\n\n\n\nMar 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nS-Values - Part 1\n\n\n11 min\n\n\n\nStatistics & Heuristics\n\n\n\n\n\n\n\nNov 20, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Winner’s Curse\n\n\n9 min\n\n\n\nStatistics & Heuristics\n\n\n\n\n\n\n\nDec 4, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Global Childhood Cancer Puzzle - Edge 1.3\n\n\n28 min\n\n\n\nGlobal PHO\n\n\n\n\n\n\n\nNov 27, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Global Childhood Cancer Puzzle - Edge 1.2\n\n\n15 min\n\n\n\nGlobal PHO\n\n\n\n\n\n\n\nNov 27, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Global Childhood Cancer Puzzle - Edge 1.1\n\n\n20 min\n\n\n\nGlobal PHO\n\n\n\n\n\n\n\nNov 27, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/epistemic_limits/limit_zoo/index.html",
    "href": "docs/epistemic_limits/limit_zoo/index.html",
    "title": "The Limit Zoo",
    "section": "",
    "text": "It’s a zoo full of cages, and we are in each one."
  },
  {
    "objectID": "docs/posts/2022-04-04-htma-2-universal_approach/index.en.html",
    "href": "docs/posts/2022-04-04-htma-2-universal_approach/index.en.html",
    "title": "How to measure anything in Global Health: 2. The universal approach to measurement",
    "section": "",
    "text": "If you want to use data to improve treatment delivery in global health, all of your measurements should support decisions.\nFor example, suppose you run an oncology clinic and consider introducing a new treatment, drug X, into your current treatment regimen for Burkitt lymphoma (BL). It has been proven in studies in the United States to improve outcomes by 10% in BL. It is also expensive, has side effects that may increase toxicity, and will take additional clinic capacity and new workflows to administer. Should you start to offer drug X for BL?\nTo decide, use the universal approach to measurement to reduce uncertainty about the possible outcomes.\nThis material comes from How to Measure Anything (HTMA), the legendary book by Douglas Hubbard that, you guessed it, teaches you to measure anything. Check out the book if you like this material."
  },
  {
    "objectID": "docs/posts/2022-04-04-htma-2-universal_approach/index.en.html#define-a-decision-problem-and-the-relevant-uncertainties",
    "href": "docs/posts/2022-04-04-htma-2-universal_approach/index.en.html#define-a-decision-problem-and-the-relevant-uncertainties",
    "title": "How to measure anything in Global Health: 2. The universal approach to measurement",
    "section": "1. Define a decision problem and the relevant uncertainties",
    "text": "1. Define a decision problem and the relevant uncertainties\nPrecisely specify the decision you are trying to make - “Should we add drug X to the treatment for intermediate and high-risk BL or continue with current therapy?” Also, specify the unknowns that will impact the decision - “We need to know the improvement in survival, cost, possible toxicities and required supportive therapies, and requirements and cost associated with administration of the drug.” Relate how the values of the variables combine to produce the decision. Be very specific."
  },
  {
    "objectID": "docs/posts/2022-04-04-htma-2-universal_approach/index.en.html#determine-what-you-know-now",
    "href": "docs/posts/2022-04-04-htma-2-universal_approach/index.en.html#determine-what-you-know-now",
    "title": "How to measure anything in Global Health: 2. The universal approach to measurement",
    "section": "2. Determine what you know now",
    "text": "2. Determine what you know now\nUse the content experts to describe and quantify the current level of uncertainty. HTMA recommends calibrating the content experts through a series of exercises to have them estimate their 90% uncertainty intervals for a variety of estimation tasks. We’ll cover this more in upcoming essays."
  },
  {
    "objectID": "docs/posts/2022-04-04-htma-2-universal_approach/index.en.html#compute-the-value-of-additional-information",
    "href": "docs/posts/2022-04-04-htma-2-universal_approach/index.en.html#compute-the-value-of-additional-information",
    "title": "How to measure anything in Global Health: 2. The universal approach to measurement",
    "section": "3. Compute the value of additional information",
    "text": "3. Compute the value of additional information\n“Information has value because it reduces risk in decisions.” Identify high-value variables to measure by estimating how much value you will gain by being more certain about the variables listed in step 1. How would it affect your decision if your uncertainty about the total cost of the drug went from $500-$5,000 per patient to $800-$1500 per patient? HTMA has much to say on this, which we will cover in the future."
  },
  {
    "objectID": "docs/posts/2022-04-04-htma-2-universal_approach/index.en.html#apply-the-relevant-measurement-instruments-to-high-value-measurements",
    "href": "docs/posts/2022-04-04-htma-2-universal_approach/index.en.html#apply-the-relevant-measurement-instruments-to-high-value-measurements",
    "title": "How to measure anything in Global Health: 2. The universal approach to measurement",
    "section": "4. Apply the relevant measurement instruments to high-value measurements",
    "text": "4. Apply the relevant measurement instruments to high-value measurements\nApply simple measurement methods to reduce your uncertainty about the high-value variables identified in steps 1-4. There are many excellent, low-cost ways to accomplish this task, which I will address in time."
  },
  {
    "objectID": "docs/posts/2022-04-04-htma-2-universal_approach/index.en.html#make-a-decision-and-act-on-it",
    "href": "docs/posts/2022-04-04-htma-2-universal_approach/index.en.html#make-a-decision-and-act-on-it",
    "title": "How to measure anything in Global Health: 2. The universal approach to measurement",
    "section": "5. Make a decision and act on it",
    "text": "5. Make a decision and act on it\nAfter measurement, integrate what you know into the decision pathways in step 1 and estimate the balance of risk versus reward for the different decisions.\nThen decide and have confidence in your glorious theory-informed, evidence-based, measurement-backed decision to improve your patients’ care."
  },
  {
    "objectID": "docs/posts/2022-03-22-data-science-and-the-epistemic-rodeo/index.en.html",
    "href": "docs/posts/2022-03-22-data-science-and-the-epistemic-rodeo/index.en.html",
    "title": "Data science is a saddle for the epistemic rodeo: healthcare as a complex system",
    "section": "",
    "text": "Healthcare is complicated. Healthcare is also complex. To use data science to improve treatment delivery in low-resource settings, it is important to understand in what way healthcare is complicated and in what was it is complex.\nWe will turn to the Cynefin (pronounced ku-nev-in) framework to understand the difference between complicated and complex contexts."
  },
  {
    "objectID": "docs/posts/2022-03-22-data-science-and-the-epistemic-rodeo/index.en.html#clinical-care-is-complicated",
    "href": "docs/posts/2022-03-22-data-science-and-the-epistemic-rodeo/index.en.html#clinical-care-is-complicated",
    "title": "Data science is a saddle for the epistemic rodeo: healthcare as a complex system",
    "section": "Clinical care is complicated",
    "text": "Clinical care is complicated\nDoctors and nurses require incredible expertise to do their job well. We know a lot about the human body and about a multitude of disease states. Providers use that knowledge to gather data about the patient’s clinical presentation to arrive at a correct diagnosis or to identify effective treatments. Expert knowledge and experience is the key.\nImportantly, although each individual patient’s body is unique and ever changing, the application of biological knowledge in the in the context of diagnosis and treatment is a stable process. Diseases act in more-or-less in predictable ways. When diseases are unpredictable, the steady application of clinical knowledge can provide further answers and treatments."
  },
  {
    "objectID": "docs/posts/2022-03-22-data-science-and-the-epistemic-rodeo/index.en.html#healthcare-systems-are-complex",
    "href": "docs/posts/2022-03-22-data-science-and-the-epistemic-rodeo/index.en.html#healthcare-systems-are-complex",
    "title": "Data science is a saddle for the epistemic rodeo: healthcare as a complex system",
    "section": "Healthcare systems are complex",
    "text": "Healthcare systems are complex\nThe healthcare system that is built to deliver care to patients is comprised of many moving parts that are hierarchically nested within functional units that interact in nonlinear ways. Hospitals, clinics, government agencies, nongovernment organizations are all key types of entities that comprise the system. Within each entity there are doctors, nurses, politicians, administrators, and many other types of actors. These entities and actors must perform many functions to deliver medical goods and services to patients. Some actors can react to the activity of others, such as changes in insurance coverage or new legislation, causing unpredictable perturbations throughout the system.\nFrom the perspective of an individual clinic or hospital, these outside forces influence the treatment centers ability to care for patients in many ways. Medications must be secured, equipment must be serviced, staffing must be adequately maintained, and information systems must be kept up to date. The center’s ability to maintain these services depends on both the proper functioning of internal operations and the continued management of the ever changing external forces. Add on top of all of that the occasional pandemic or other chaotic force that emerges from the darkness to completely disrupt operations, and it can feel is if you are in an epistemic rodeo, holding on for dear life as new information tosses you about."
  },
  {
    "objectID": "docs/posts/2022-03-22-data-science-and-the-epistemic-rodeo/index.en.html#data-science-helps-you-navigate-complex-systems",
    "href": "docs/posts/2022-03-22-data-science-and-the-epistemic-rodeo/index.en.html#data-science-helps-you-navigate-complex-systems",
    "title": "Data science is a saddle for the epistemic rodeo: healthcare as a complex system",
    "section": "Data science helps you navigate complex systems",
    "text": "Data science helps you navigate complex systems\nTaking another insight from the Cynefin framework, in complex contexts the best way to act is to probe, sense, and respond. Probing implies that you have sensors in the system that provide information on how it evolves over time. This equips you with the ability to experiment with small changes to see how the system responds. Sensing is the act of gathering the information about the change in the system to the small changes. This should give sufficient information to know how to respond more effectively to guide the system to the desired outcome. The probe-sense-respond process is similar to several PDSA cycles in QI, but rapid sensing and flexibility with responses are key.\nData science provides all of the required equipment to succeed in the epistemic rodeo by allowing you to probe-sense-respond across all of your organizational complexities. Like a saddle on a bull, data science will stabilize your knowledge and give you something to hold on to as you start to feel the kick from the next challenge. Soon you’ll understand how your complex systems behaves and be able to direct it to gallop wherever you need it to go."
  },
  {
    "objectID": "docs/posts/2021-11-20-s-values/index.en.html",
    "href": "docs/posts/2021-11-20-s-values/index.en.html",
    "title": "S-Values - Part 1",
    "section": "",
    "text": "Imaging you find a coin on the ground. Ever the thrifty type, you are delighted to find this treasure, so you pick it up and give it three celebratory flips. You notice it lands on heads after each flip, so you decide to give it one more. It lands on heads again! You do quick mental math and decide that the probability of getting four heads in a row if the coin is fair is \\(\\frac{1}{16}\\). Curious, but not impossible. You give it another flip. Heads. You get more suspicious. Another flip. Heads! Now you’re fairly confident something funny is going on with the coin. You continue to flip and get heads until you feel your level of confidence that you found a trick coin reaches some level of certainty, at which point your confidence grows no further and you begin to expect heads with each flip.\nReady for the big reveal in this story? What you actually encountered while walking down that street is a p-value!\nYes, THAT p-value. The one where we try really hard to have values less than 0.05 so that you we can imbue our research with the authority of statistics and have it published. The reason why the situation doesn’t look like a p-value is because it is mathematically transformed and conceptually situated away from the context where you normally encounter it, in journal articles usually accompanied by stars (***) and hyperbolic causal conclusions (although more research is necessary). The version of the p-value in this story is called an s-value, and it is an incredibly helpful tool to build intuition about what p-values mean. The best part is that s-values will help you more accurately interpret p-values while doing almost no math!"
  },
  {
    "objectID": "docs/posts/2021-11-20-s-values/index.en.html#introduction",
    "href": "docs/posts/2021-11-20-s-values/index.en.html#introduction",
    "title": "S-Values - Part 1",
    "section": "",
    "text": "Imaging you find a coin on the ground. Ever the thrifty type, you are delighted to find this treasure, so you pick it up and give it three celebratory flips. You notice it lands on heads after each flip, so you decide to give it one more. It lands on heads again! You do quick mental math and decide that the probability of getting four heads in a row if the coin is fair is \\(\\frac{1}{16}\\). Curious, but not impossible. You give it another flip. Heads. You get more suspicious. Another flip. Heads! Now you’re fairly confident something funny is going on with the coin. You continue to flip and get heads until you feel your level of confidence that you found a trick coin reaches some level of certainty, at which point your confidence grows no further and you begin to expect heads with each flip.\nReady for the big reveal in this story? What you actually encountered while walking down that street is a p-value!\nYes, THAT p-value. The one where we try really hard to have values less than 0.05 so that you we can imbue our research with the authority of statistics and have it published. The reason why the situation doesn’t look like a p-value is because it is mathematically transformed and conceptually situated away from the context where you normally encounter it, in journal articles usually accompanied by stars (***) and hyperbolic causal conclusions (although more research is necessary). The version of the p-value in this story is called an s-value, and it is an incredibly helpful tool to build intuition about what p-values mean. The best part is that s-values will help you more accurately interpret p-values while doing almost no math!"
  },
  {
    "objectID": "docs/posts/2021-11-20-s-values/index.en.html#s-values",
    "href": "docs/posts/2021-11-20-s-values/index.en.html#s-values",
    "title": "S-Values - Part 1",
    "section": "S-values",
    "text": "S-values\nMathematically an s-value is defined as \\(s = -log_2(p)\\) (the mathematical details of the development of this transformation are outside the scope of this post, but you can see Cole, Edwards, and Greenland (2021) for a more robust discussion). The result of this transformation can be interpreted as saying, “if I were to flip a fair coin s times, I would expect to get heads on all flips with probability p.” For example, if I flip a fair coin and get heads 4 times in a row, that corresponds to a probability of getting that outcome of 0.06 . Another way to phrase this: if I did 100 trials of 4 coin flips, only 6 would result in 4 heads. This transformation also works if we start with probability. If I tell you that I saw someone flip a coin and get heads each time and that the probability of this outcome using a fair coin was only around 0.01, you could plug it into the equation tell me that probability corresponds to roughly 7 heads in a row (with rounding because fraction of coin flips are hard to visualize).\nAn s-value can also be described as a measure of surprise that you feel when flipping a coin that you expect to be fair and seeing it land on heads some number of times in a row. Yes, it is best described as a feeling. Your brain and your body have been tuned to create expectations of outcomes. When an outcome happens that defies your expectations, you feel surprised! The surprise in coin flipping is that you expect some mixture of heads and tails, but when only heads keeps coming up, that violation of your expectation creates a feeling of surprise. Another way to say it is that s-values quantify your feeling of surprise when your data and your expectation about a hypothesis about a binary outcome do not match. The hypothesis in the coin flipping example is that the coin is fair. Amazingly, this scenario can teach us about p-values because feeling surprised at a mismatch between a hypothesis and data applies to any situation where p-values are used to reason about a hypothesis.\n\nExample\nWe will work through an example and then explore some very useful properties of s-values.\nLet’s say we read a journal article where the researchers claimed to have surveyed a random sample of 120 people and found that more people prefer cats to dogs and reported the difference with p= 0.045. Under the Null Hypothesis Significance Testing methods, the p-value is telling you is that if we live in a world where people do not in fact have a preference between cats or dogs, we would expect to see the difference that was reported in this study or a more extreme difference 4.5% of the time. For Reasons, a p-value of less than 0.05 has long been celebrated as the “significance” threshold, which is shorthand for “publishable” and also for “I can extend my life in academia a few more months.”\nS-values can help improve our interpretation of this number and move us away from a threshold for significance/publishable/keeping-my-job. For p=0.045, the corresponding s-value is 4.47 which can round down to 4 . This p-value provides roughly the same amount of surprise against the null hypothesis (no difference between cat and dog preferences) as flipping a fair coin and seeing it land on heads 4 times in a row. Is that a lot of evidence against the null hypothesis? It’s something, but it doesn’t feel like a lot.\n\n\nS-values change slowly until they don’t\nOne of the most important insights from the s-value perspective is that the difference between a “significant” and “non-significant” finding is very small. Suppose in the cat-dog preference study the p=value was 0.055. That corresponds to an s-value of 4.18 , which again rounds to 4. Although there is a small fractional difference in s-values, the intuition is that both p-values of 0.045 and 0.055 are roughly similar to the surprise you feel when flipping a fair coin and seeing heads 4 times in a row. Why then is a threshold of 0.05 special? It’s not. It’s just one value along a continuum of evidence that corresponds to a continuous amount of surprise against the null hypothesis.\nLet’s now consider the possible s-values you could get along the whole range of p-values from 0 to 1. The graph below demonstrates this relationship.\n\n\n\n\n\n\n\n\n\nWe start with p-values on the x-axis, and with a p-value of 1 on the left because it produces an s-value of 0. Arranging the graph this way demonstrates that evidence against the null hypothesis accumulates as p-values gets smaller. The blue line on the chart represents p=0.05. Notice how slowly the s-values rise over the interval from 1 to 0.25. In this range, an absolute difference of 0.75 on the probability scale corresponds to just two coin tosses/s-value units (technically known as bits). Even though the value of the p-value varies quite a bit in this range, there just isn’t much to make you feel surprised about the null hypothesis that is contained in these numbers. On the right side of the graph you can see that evidence accumulates exponentially as the p-value gets smaller. Let’s zoom in on that region in the graph below.\n\n\n\n\n\n\n\n\n\n\n\nHere we’re starting on the left with a p-value of 0.1. I’ve added a blue horizontal line to depict the s-value of 4.32 that corresponds to a p-value of 0.05. Zooming in to this area again shows that s-values increase relatively slowly in the region of 0.05 compared to much smaller values on the right. Below is a table of p/s-values in this region to make the increase clear.\n\n\n\n\n\n\n\n\n\nP-Value\nS-Value\n\n\n\n\n0.10\n3.32\n\n\n0.09\n3.47\n\n\n0.08\n3.64\n\n\n0.07\n3.84\n\n\n0.06\n4.06\n\n\n0.05\n4.32\n\n\n0.04\n4.64\n\n\n0.03\n5.06\n\n\n0.02\n5.64\n\n\n0.01\n6.64\n\n\n\n\n\n\n\nThe graph and table demonstrate that a 0.01 decrease in p-values starting from 0.10 still results in a relatively small incremental increase in s-values. Invoking coin flips again, 3.64 heads in a row with a fair coin (p=0.08) and 4.32 heads in a row (p=0.05) just don’t feel very different. They both round to 4 heads in a row. When I said “a relatively small increase”, I meant relative to the scale of meaningful evidence against the null hypothesis. By the time the p-value is 0.01, the number of heads in a row round to 7, which does feel like a much more surprising result compared to 4 heads in a row.\nNotice that as the p-values get smaller, the s-value increase starts to accelerate. Going from p=0.1 to p=0.01 (an absolute difference of 0.09) results in a 3.3 increase in s-values. This is very different than in the range of p-values from 1 to 0.25 (absolute difference of 0.75) that resulted in an increase in s-values of 2. Let’s make this trend clearer in the table below.\n\n\n\n\n\n\n\n\n\nP-Value\nS-Value\n\n\n\n\n0.1\n3.3\n\n\n0.01\n6.6\n\n\n0.001\n10.0\n\n\n0.0001\n13.3\n\n\n0.00001\n16.6\n\n\n\n\n\n\n\nThis table demonstrates that with small p-values, even though the absolute differences in p-values is shrinking (for example, the difference between 0.01 and 0.001 is 0.009 ), evidence is accumulating much more rapidly. When deciding about evidence against the null, there is a BIG difference between a p-value of 0.1 and 0.01. There is also a BIG difference between 0.01 and 0.001. Even though both of these numbers feel “small,” 0.001 contains 50% more evidence against the null hypothesis than 0.01. Humans are bad about reasoning with small numbers. Once we get to a certain number of decimals, we throw up our hands and think “what’s the difference between small and smaller?” When talking about feelings of surprise against a null hypothesis, the difference is huge because the information lives in these small values, which are the easiest for us humans to overlook!"
  },
  {
    "objectID": "docs/posts/2021-11-20-s-values/index.en.html#conclusion",
    "href": "docs/posts/2021-11-20-s-values/index.en.html#conclusion",
    "title": "S-Values - Part 1",
    "section": "Conclusion",
    "text": "Conclusion\nThis post defined s-values and discussed four important tasks that they accomplish.\nS-values transform p-values into a measure of surprise against a hypothesis. In this post, we focused primarily on the null hypothesis.\n\nS-values correspond to the psychologically familiar situation of the surprise that you feel when flipping a coin that you think is fair and getting heads a certain number of times in a row. This builds intuition of how p-values provide evidence against the null hypothesis.\nS-values demonstrate that the magnitude of the absolute difference in p-values do not correspond to the difference in s-values across the range of possible p-values.\nS-values increase slowly over the range of p-values that surround the traditional cutoff values of p=0.05, demonstrating that p-value cutoffs are largely arbitrary and values in this range provide similar feelings of surprise against a null hypothesis.\nThe increase in S-values will accelerate as p-values become smaller, demonstrating that information against the hypothesis accumulates in small p-values, contrary to the naive intuition that small absolute differences on the probability scale when the values are small are not overly meaningful.\n\nI hope this discussion is helpful to calibrate your intuition about what p-values signify. See these references below to learn more (Rafi and Greenland 2020; Greenland 2019)."
  },
  {
    "objectID": "docs/posts/2022-03-13-gh_needs_ds/index.en.html",
    "href": "docs/posts/2022-03-13-gh_needs_ds/index.en.html",
    "title": "Three Reasons Global Health needs Data Science",
    "section": "",
    "text": "Global Health needs Data Science.\nThese two disciplines were made for each other. While each field accomplishes amazing things in isolation, when combined together their complementary strengths unlock progress that was previously impossible.\nI’m using the term “Data Science” because it is flashy enough to catch your eye and broad enough to encompass the many specializations that are the key to success: data collection and management, informatics tools, data engineering, operational analytics, statistics, and machine learning.\nNow, let me give you three reasons why Global Health needs Data Science (and one reason why it’s so hard to do well)."
  },
  {
    "objectID": "docs/posts/2022-03-13-gh_needs_ds/index.en.html#efficient-health-organization-operations-save-lives",
    "href": "docs/posts/2022-03-13-gh_needs_ds/index.en.html#efficient-health-organization-operations-save-lives",
    "title": "Three Reasons Global Health needs Data Science",
    "section": "1. Efficient health organization operations save lives",
    "text": "1. Efficient health organization operations save lives\nIn resource-constrained settings, every resource needs to be used well.\nHealthcare providers need to use their knowledge to care for the patients that most need their expertise, medications need to be given where they are most effective (and before they expire!), respiratory support equipment needs to be available as soon as a patient needs it, and money needs to be spent on what is useful and not wasted on what is useless.\nData science gives organizations the capabilities to use resources well. Many data scientists work in companies where their primary job is to perform an alchemical transformation of data into efficient operational insights using the magic of math. In companies, this translates to increased profits. In global health, this translates to saved lives."
  },
  {
    "objectID": "docs/posts/2022-03-13-gh_needs_ds/index.en.html#learning-health-systems-accelerate-progress",
    "href": "docs/posts/2022-03-13-gh_needs_ds/index.en.html#learning-health-systems-accelerate-progress",
    "title": "Three Reasons Global Health needs Data Science",
    "section": "2. Learning health systems accelerate progress",
    "text": "2. Learning health systems accelerate progress\nTo improve health outcomes, it is key to have a system where healthcare providers, administrators, and others are equipped with the tools and knowledge required to learn about what is not going well and conduct improvement projects to change it\nUnfortunately, this sounds good on paper but is very difficult to do. The reality of working to improve a system that may not want to improve when you are already very busy with other responsibilities means progress is slow and frustrating.\nData Science can help push projects forward even in systems that resist change. Data Science provides the tools to efficiently collect data, accurately measure what matters, and effectively communicate it to people who need to know. These advantages are even more important in resource-constrained settings where time and organizational support are forever in short supply."
  },
  {
    "objectID": "docs/posts/2022-03-13-gh_needs_ds/index.en.html#healthcare-is-an-unstable-complex-adaptive-system",
    "href": "docs/posts/2022-03-13-gh_needs_ds/index.en.html#healthcare-is-an-unstable-complex-adaptive-system",
    "title": "Three Reasons Global Health needs Data Science",
    "section": "3. Healthcare is an unstable complex adaptive system",
    "text": "3. Healthcare is an unstable complex adaptive system\nThere is nothing linear or simple about working in healthcare. The provision of care is complex because it requires many people with specialized knowledge to work together in a system that has the physical infrastructure they need. Treatment is nonlinear because health status can change rapidly or in unexpected ways and providers need to be able to respond accordingly. Improving health systems is difficult because any small perturbation of the system may result in no change at all or in large, unexpected, and sometimes harmful changes.\nThe best way to make decisions in a complex environment is to probe the system (induce small changes), sense the changing state of the system, and then respond to the changes. To do this, decision-makers need the tools to probe-sense-respond efficiently. Data Science equips organizations with tools needed to create this signal network."
  },
  {
    "objectID": "docs/posts/2022-03-13-gh_needs_ds/index.en.html#data-science-capabilities-and-data-scientists-take-a-lot-of-care-and-feeding",
    "href": "docs/posts/2022-03-13-gh_needs_ds/index.en.html#data-science-capabilities-and-data-scientists-take-a-lot-of-care-and-feeding",
    "title": "Three Reasons Global Health needs Data Science",
    "section": "Data Science capabilities (and Data Scientists) take a lot of care and feeding",
    "text": "Data Science capabilities (and Data Scientists) take a lot of care and feeding\nThe reason why high-caliber Data Science is not available in many global health organizations is that it takes a lot of time, money, and investment from leadership to make it successful. Collecting clean data (especially if there is no EMR available) is incredibly hard. Assuring the accuracy of the data is even harder. As data are stored over time, people’s trust in the data wanes because their knowledge of its nuances decays. Data analyses are often done poorly because the devil is in the details. While people pay lip service to the importance of statistics, few have the patience or fortitude to engage the epistemic rodeo that is wrangling and taming uncertainty from observational data. And…machine learning isn’t very useful in Global Health right now. Sorry to be the bearer of bad news."
  },
  {
    "objectID": "docs/posts/2022-03-13-gh_needs_ds/index.en.html#data-science-global-health",
    "href": "docs/posts/2022-03-13-gh_needs_ds/index.en.html#data-science-global-health",
    "title": "Three Reasons Global Health needs Data Science",
    "section": "Data Science + Global Health",
    "text": "Data Science + Global Health\nAlthough it is hard, Data Science can be done well in Global Health. With time, effort, and the right perspective, Global Health organizations can reap the incredible rewards from careful data management and thoughtful analytics.\nOver the next 30 days, I will be writing essays that explore how to best do Data Science in Global Health well. Follow along if you want to learn more and reach out to let me know more about your experiences with Data Science + Global Health!"
  },
  {
    "objectID": "docs/posts/2020-11-27-gccp-1-1/index.en.html",
    "href": "docs/posts/2020-11-27-gccp-1-1/index.en.html",
    "title": "The Global Childhood Cancer Puzzle - Edge 1.1",
    "section": "",
    "text": "The following originally appeared on my podcast about global PHO - Global Health and Childhood Cancer\n\n\nThis is part one of a three part essay about the global burden of childhood cancer. You can find part 2 here and part 3 here.\n\n\nThe Global Oncology Puzzle\nHave you ever completed a very difficult puzzle? Perhaps it was thousands of small pieces with colors that blend together, so that most of the pieces appeared very similar. Do you remember the overwhelmed feeling you had when you first dumped the pieces onto the table? “Good grief”, you may have thought, “this puzzle is impossible.” But, after the moment of defeat, you look at the picture on the box and see what the puzzle could be. Perhaps it will be an amazing waterfall, or city illuminated at night, or a basket full of kittens, and you know that at the end of all your hard work the pieces will come together, and the picture will be complete. So, thinking of that future day when you will feel the satisfying snap of the last piece falling into place, you sit down and get to work.\nFor many people, global pediatric oncology (GPO) is like a giant, overwhelming puzzle. The complexity of the problem and the way the pieces are scattered may leave one feeling lost how to begin. However, despite its difficulty, a community of dedicated doctors, nurses, cancer survivors, parents, politicians, and many others has been working to solve it. Over the last three decades, this community has made considerable progress toward understanding the necessary conditions for successful childhood cancer treatment anywhere in the world. Now, new research has completely changed the perception of childhood cancer as a global health concern.\nAs a result of this new information, the shape of the puzzle is changing. Whereas before, many pieces had been put together, but there was no structure that unified the whole, now most of the edge pieces are in place, and the outline of the puzzle can be seen in its entirety. The edges have given the puzzle a definite structure, which helps the global oncology community understand both how the different completed parts sit in relation to each other and what important information is still missing.\nIn a series of essays, I want to present to you a broad understanding of what I’m calling the edges of the GPO puzzle: those essential concepts without which one cannot fully understand pediatric oncology as a global health concern. Concretely, we will review the emerging research that clarifies the magnitude of the problem of childhood cancer, demonstrates effective solutions exist that can save lives today, quantifies the costs associated with treatment, and charts a clear path forward. To discuss the first edge, we will directly address the question, “what is the global burden of pediatric cancer?”\nMuch of the information in these essays will not be new to people who are involved in the field. Even if he or she hasn’t read the research, much of it is in line with the experience and intuition of the GPO community. If you consider yourself in this group, my hope is that you take from these essays a distilled set of concepts that can serve as visual and viral representations of the field, which you can use to communicate its importance to others.\nAs for the interested but unfamiliar reader, this information may be surprising. When one sits down at the table to work on the puzzle and looks at the chaos of the scattered pieces, it’s easy to be overwhelmed. This feeling may even be accompanied by belief in the futility of completing the puzzle. I have certainly had the thought that the goal of successfully treating children with cancer anywhere in the world is too complex, too big, and too expensive. It also may be hard to see how you can contribute to completing the puzzle. Being overwhelmed at the problem is understandable, which is why I hope to demonstrate that significant progress has already been made and more lives can be saved, through the GPO community’s tireless efforts in caring for kids, an effort in which anyone can take part\nWith that said, let’s go tackle the first edge. I’ll address the rest of the edges in future essays.\n\n\n\n\n\n\n\n\n\n\n\nEdge #1: The Global Burden of Pediatric Cancer\nNo one would deny that childhood cancer is a terrible disease, and people broadly agree that it is horrible that children have to endure it. However, it is also a rare disease, and one may justly wonder how big a problem it is compared to the many other concerning diseases that confront the global health community. This is a reasonable question that serves as the starting place for constructing a more complete picture of GPO. In this visual essay will ask and answer the question what is the total burden of disease due to pediatric cancer?\nRecent research has provided a striking answer to this question, which is: a whole lot more than we previously thought! I’ll explain what this surprising answer means in a minute, but first let me explain why this question is very difficult to answer and why the global cancer community’s understanding of disease burden is changing.\n\n\nHow do you measure cancer burden?\nHow can we answer this question? Should we count the total number of kids diagnosed with cancer who are alive in the world right now? Or maybe we should only count new cases this year? Maybe we should we look at the total number of deaths due to cancer this year? But what about the kids who develop cancer but are never diagnosed, how do we measure those cases if we don’t directly count them? Also, cancer causes much more suffering than just loss of life. Is there a way to factor in the amount of suffering when trying to quantify the burden of disease? You can see from these questions that the first thing researchers must do is to decide what aspect of cancer burden is worth measuring.\nThese questions are very difficult for researchers to answer for a single country, and they are even more complex when considering the entire world. To answer these questions correctly, researchers require a large amount of information. It would be nice if complete data about the number of cancer diagnoses or deaths due to cancer were available, but the unfortunate truth is that there are many holes in the available data. Comprehensive data collection requires a large and expensive system, and there are many countries that are limited in their ability to start and maintain one. Other countries track cases diagnosed at different hospitals, but they do not count every hospital and every patient in the country, so their numbers are also incomplete. To make things even more complicated, researchers know that many cases around the world are not even diagnosed, and if they are to produce reliable numbers about the amount of pediatric cancer in the world, then they must be able to measure the cases they know are happening but never identified.\nFrom these problems, you can see that there are two big reasons why quantifying cancer burden is hard to do: 1. We could measure it in different ways that mean different things. 2. We are missing much of the information we need to make good measurements.\nBecause of these difficulties, estimates of the total burden of childhood cancer have changed over the years. The GPO community has worked hard to improve their methods, and we now have more reliable numbers that tell a very surprising story.\n\n\nWhat is the total burden of disease due to pediatric cancer?\nAs I said above, the answer from our improved estimates is: there is a whole lot more cancer than we previously thought! Let me explain what has changed and why this is so surprising.\n\nThe total number of new cases every year\nDue to the limitations in measuring cancer burden previously described, the best estimates for how many children from the age 0-14 developed cancer each year was 200,000. The technical term for this measure is incident cases. However, recently researchers developed a new method to estimate incident cases and reported that there are in fact 400,000 children who develop cancer every year. In other words, the previous estimates were half as much as the newer estimate (see figure 1)!\n\nFig. 1 - Animated\n\n\n\n\n\n\n\nClick for static image\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy are the numbers so different? Part of the reason is the way the researchers calculated the number of cases for countries with missing data by comparing them to the number of cases for countries with really good data differently than previous estimates. Also, with the newer method, researchers were able to better account for the patients who developed cancer but were never diagnosed. Previous methods had trouble estimating this number well, but the new method is much better at it. In fact, the researchers found that about 175,000 kids with cancer are not even diagnosed every year! That means they develop cancer, but either they never seek medical care or the fact that they have cancer is never recognized by healthcare personnel. Another way to look at this number is that more than 4 out of every 10 kids with cancer in the world are never diagnosed.\nThese numbers summarize what we know about the number of new cases per year for the entire world. We can say that the average proportion of kids who are not diagnosed is 4 out of 10. However, that actual proportion of kids who are not diagnosed in a specific country or regions can be very different. For example, the researchers estimated that in western Africa 57% of cases, almost 6 out of every 10 children with cancer, were never diagnosed and in south Asia it was 5 out of 10 kids. Figure 2 shows the estimates of total number of new cases per year by different region and how many cases are diagnosed and undiagnosed.\n\n\nFig. 2 - Animated\n\n\n\n\n\n\n\nClick for static image\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs the researchers looked at the trends in proportion of kids who are diagnosed across countries, they noticed that countries that are more economically challenged, labeled as low-income and middle-income countries (LMIC for short), tend to have higher proportions of kids with cancer who are not diagnosed. This is an important fact to notice because 9 out of every 10 kids with cancer in the world live in LMICs! Putting these pieces of evidence together, we can say that the vast majority of children who develop cancer live in countries where they have a high chance of never being diagnosed.\nThe researchers of this paper didn’t stop with these estimates. They calculate that if the rates for non-diagnosis do not change between 2015 and 2030, then about 3 million children will not receive a diagnosis for their life-threatening disease. 3 million children, most with diseases that can be cured, will never have the opportunity to receive treatment, never have the chance at living a long healthy life, and never even know what was breaking their bodies. This is a daily average around 550 nondiagnosed children with cancer. To put this in perspective, if an average school bus holds 55 kids, then this number is the rough equivalent of 10 school buses filled with children driving away and disappearing every single day for 15 years.\n\n\nFig. 3 - Animated\n\n\n\n\n\n\n\nClick for static image\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Technical Note: it is worth pointing out that the numbers mentioned above and below are estimates and there is a degree of uncertainty about them that is not represented in the displayed data. The fact of the matter is that the lack of reliable data leaves a lot of questions. For instance, only 5% of kid in Africa live in countries with comprehensive registries. How do we decide what to do with the remaining 95%? The researches mentioned above had a specific modeling strategy to fill in the gaps, but that is not the only way. For instance the International Incidence of Childhood Cancer (IICC3) has a different methodolgy that produces slightly different numbers. For the purposes of this essay it is sufficient to acknowledge these difference, and continue exploring the estimates the Harvard researchers produced.).\n\nContinue reading:\nPart 2\nPart 3\nSubscribe to my podcast about global PHO at GHCCpod.com"
  },
  {
    "objectID": "docs/posts/2022-03-17-data-science-garbage/index.en.html",
    "href": "docs/posts/2022-03-17-data-science-garbage/index.en.html",
    "title": "Data Science has a garbage problem",
    "section": "",
    "text": "Data science has a data problem.\nThe vast edifice of technologies, visualizations, statistics, machine learning algorithms that make up the data science empire are built on a foundation that can either be as solid as a rock or as unstable as shifting sand."
  },
  {
    "objectID": "docs/posts/2022-03-17-data-science-garbage/index.en.html#the-foundation-of-all-of-data-science-is-data-quality",
    "href": "docs/posts/2022-03-17-data-science-garbage/index.en.html#the-foundation-of-all-of-data-science-is-data-quality",
    "title": "Data Science has a garbage problem",
    "section": "The foundation of all of data science is data quality",
    "text": "The foundation of all of data science is data quality\nIf your data are garbage, then your analyses may as well be toxic waste. You are better off hiring a hard Sci-Fi novelist to imagine what insights you might have gotten from possible analyses than performing any of your own. You can literally produce negative information (information that moves you further from your goal) using bad data."
  },
  {
    "objectID": "docs/posts/2022-03-17-data-science-garbage/index.en.html#garbage-data-is-a-big-problem-in-global-health",
    "href": "docs/posts/2022-03-17-data-science-garbage/index.en.html#garbage-data-is-a-big-problem-in-global-health",
    "title": "Data Science has a garbage problem",
    "section": "Garbage data is a big problem in Global Health",
    "text": "Garbage data is a big problem in Global Health\nInformation technologies are limited or nonexistent, and data collection is performed manually. There may be limited time and resources to dedicate toward the inglorious task of data management. Leadership may not understand the value of clean data and may encourage shortcuts that compromise quality. To put it bluntly, data management is boring, hard to do well, and just not sexy compared to the rest of data science.\nThis is a problem when resources and time are scarce because the wasted effort to collect garbage data is effort that could have been spent on something more valuable."
  },
  {
    "objectID": "docs/posts/2022-03-17-data-science-garbage/index.en.html#garbage-data-today-will-poison-data-science-tomorrow",
    "href": "docs/posts/2022-03-17-data-science-garbage/index.en.html#garbage-data-today-will-poison-data-science-tomorrow",
    "title": "Data Science has a garbage problem",
    "section": "Garbage data today will poison data science tomorrow",
    "text": "Garbage data today will poison data science tomorrow\nAs bad analyses accrue over time, a perception will emerge in the organization that data collection is administrative busywork, and the data are untrustworthy and useless for practical applications. This will make future efforts toward data science even more difficult."
  },
  {
    "objectID": "docs/posts/2022-03-17-data-science-garbage/index.en.html#the-good-news-is-that-garbage-data-can-be-cleaned-up",
    "href": "docs/posts/2022-03-17-data-science-garbage/index.en.html#the-good-news-is-that-garbage-data-can-be-cleaned-up",
    "title": "Data Science has a garbage problem",
    "section": "The good news is that garbage data can be cleaned up!",
    "text": "The good news is that garbage data can be cleaned up!\nWith a little effort, your organization can collect beautiful, high-quality data! Thoughtful data management designed to work within the constraints of the organization’s data collection infrastructure is key. Even if time and resources are limited, you can still ask and answer incredibly important questions from the data in a way that is useful for practice application and builds trust among people who lost confidence in data-driven insights.\nIn my next post, I’ll give you a few key steps to collect and maintain beautiful, gloriously-insightful data."
  },
  {
    "objectID": "docs/posts/2022-04-06-htma-4-calibrate-your-judgement/index.en.html",
    "href": "docs/posts/2022-04-06-htma-4-calibrate-your-judgement/index.en.html",
    "title": "How to measure anything in Global Health: 4. Calibrate your judgment",
    "section": "",
    "text": "Your personal calibration is one of the most important measurement skills you’ve never heard of.\nDifficult measurement problems in Global Health require experts to provide an initial estimate of uncertainty about the variables in the process that produce an outcome. For instance, suppose to want to use a new treatment, drug X, for a cancer, but we are concerned the drug will cause harm through serious side effects. As a first step to measure this concern, an expert should estimate her 90% uncertainty interval of the probability that the drug causes harm.\nWhat in the world is an uncertainty interval, and how will we know if the expert’s estimates are any good?\nLike the last few posts, this content is from the delightful book How to Measure Anything by Douglas Hubbard.\n\nUncertainty intervals are ranges of estimated variable values\nThese ranges should capture all of the plausible values that a particular variable may have. If we say it is a 90% uncertainty interval, then the range of values should be wide enough so that if 100 such intervals were constructed, 90 of them would contain the correct value. These intervals should capture the expert’s uncertainty about the value while still signifying the range of plausible values the variable may take.\nFor the example with drug X, our expert may use her clinical experience with the drug in other contexts to estimate the rate of rare but serious side effects is between 1 per 100 doses to 10 per 100 doses.\nGreat, we have an interval, but how do we know if it is truly a 90% uncertainty interval?\n\n\nExperts’ judgment should be calibrated\nCalibration means that the uncertainty interval performs as expected.\nCalibrated 70% uncertainty intervals mean that truly 7 in 10 such intervals contain the true value. If 4/10 intervals contained the true value, then the expert is overconfident. If 9/10 intervals contain the true values, then the expert might be underconfident.\nSimilarly, for true/false questions, calibration can be estimated by assigning your subjective probability that your answer choice is correct. If you answer questions that you are 60% confident in, that means you believe you would answer 6 in 10 such questions correctly. If you answer 4/10 such questions correctly, you’re overconfident, if you answer 9/10 correctly, you’re underconfidence.\n\n\nCalibrate yourself\nHow do you calibrate yourself?\nTest your calibration. No literally, I mean answer questions and produce 90% uncertainty intervals or estimate your confidence in your true/false answers and see how you do.\nHere are two questions from HTMA to get you started:\n\nAssing a 90% uncertainty interval: How many inches long is the average business card?\nIndicate true or false and the probability you’re correct: A gallon of oil weighs less than a gallon of water.\n\nRandom questions, but if you think through what you already know, you can come up with a range of plausible values or a probabilistic estimate of your confidence.\nThese are just example questions. At HTMA’s website, you can find several tests of both types of questions and their answers so that you can assess your own calibration!\nThere is a lot more to this story and calibration can certainly go wrong. We will discuss more in future posts, but for now, I encourage you to go take a test and assess your own calibration.\nI’ll admit, I was woefully overconfident on my first go around.\nBut that is why we calibrate!"
  },
  {
    "objectID": "docs/posts/2022-03-25-stochastic-beasts-1/index.en.html",
    "href": "docs/posts/2022-03-25-stochastic-beasts-1/index.en.html",
    "title": "Stochastic beasts and how to slay them - Part 1",
    "section": "",
    "text": "What do the Balrogs, huge earthquakes, and nuclear war have in common?\nThey are all unpredictable demons that emerge from the abyss to destroy your future. While Balrogs may be mythical beasts from Middle Earth, earthquakes and nuclear war are very real threats.\nIn my last post, I discussed the two types of uncertainty, aleatory and epistemic. I described them with nice and tidy examples, consistent with how they are usually presented in classroom settings. These tame illustrations were a good start to illustrate the concepts so that you can recognize them when you come across them.\nUnfortunately, real life is not tame. There are radical forms of both aleatory and epistemic uncertainty. These are events that are completely unpredictable, although in slightly different ways.\nIt is good to understand these forms of uncertainty so that you can face them confidently when they come.\nIn this post, we will discuss slaying aleatory beasts and in the next post cover epistemic monsters."
  },
  {
    "objectID": "docs/posts/2022-03-25-stochastic-beasts-1/index.en.html#radical-aleatory-uncertainty",
    "href": "docs/posts/2022-03-25-stochastic-beasts-1/index.en.html#radical-aleatory-uncertainty",
    "title": "Stochastic beasts and how to slay them - Part 1",
    "section": "Radical aleatory uncertainty",
    "text": "Radical aleatory uncertainty\nThe frequency of earthquakes follows an inverse power law as a function of its strength. In other words, stronger earthquakes happen much less frequently than weaker earthquakes. This means you can live in faulty California your whole life and never experience a strong earthquake, although you will probably experience many small and medium-size earthquakes in that time.\nThis is radical aleatory uncertainty: big events that happen infrequently but conform to mathematical laws. There are underlying physical reasons why earthquakes conform to an inverse power law, which means we can make scientific models that describe their occurrence. Their infrequency also makes them unpredictable, and therein lies the problem.\nRadical aleatory uncertainty can lull you to sleep. It’s easy to think that because you’ve done fine in small earthquakes, you’ll be fine in a big one, but those are two completely different beasts. It’s tempting to save money by building less resilient housing when an earthquake hasn’t knocked anything down in a while.\nAlso, you can take a false sense of security from the fact that we can model the earthquake scientifically, therefore we must have some special insight into how to predict them. Describing their frequency and predicting the next occurrence are two wildly different activities, and we cannot do the latter."
  },
  {
    "objectID": "docs/posts/2022-03-25-stochastic-beasts-1/index.en.html#slaying-aleatory-beasts",
    "href": "docs/posts/2022-03-25-stochastic-beasts-1/index.en.html#slaying-aleatory-beasts",
    "title": "Stochastic beasts and how to slay them - Part 1",
    "section": "Slaying aleatory beasts",
    "text": "Slaying aleatory beasts\nThere is nothing we can do about earthquakes. We aren’t going to geoengineer them away. This applies to many things that exhibit radical aleatory uncertainty. So what to do?\nStay awake. The threats are real and describable mathematically, so take reasonable precautions against them. Take out insurance to protect against rare but serious catastrophes. Keep emergency supplies stocked if you live in an area exposed to natural disasters. Hedge risky in investments with safer bets. Do not over-optimize to your present situation but spend a little extra effort to build in robustness. Advocate for policies and vote for politicians that take small-but-consequential risks seriously and who won’t give in to political expediency.\nAlso, don’t let these uncertainties dominate your life. Stay alert to their possibility, but take seriously the fact that they are rare. If you’re thinking about the structural integrity of the restaurant you’re at with your date, then the beast is winning. Aleatory anxiety is a very real threat to your well-being. To ward it off, I find it helpful to understand the beast in its rare-but-serious mathematical context.\nThese stochastic beasts are hard to slay, there is no doubt about it. The experience of radical uncertainty is a fundamental part of what makes us human. It may not always be fun to face them, but it is possible to live confidently in the knowledge that you can handle the aleatory beasts come your way."
  },
  {
    "objectID": "docs/posts/2022-03-18-unsexy-data-management/index.en.html",
    "href": "docs/posts/2022-03-18-unsexy-data-management/index.en.html",
    "title": "How to walk the unsexy path toward sophisticated Data Science in Global Health: Data Management",
    "section": "",
    "text": "Data science has a data problem.\nIn my last essay, I argued that because data management is boring, hard to do well, and just not sexy compared to the rest of data science, garbage data is a big problem in global health.\nThe good news is that with a little effort your organization can collect beautiful, high-quality data! Here are 5 steps to design an optimal system for low-resource settings, where data are manually collected from paper charts by limited numbers of people.\n\nStep 1: Identify the key questions your data need to answer\nQuestions are the shining light at the end of the data management path. They should guide you every step of the way. Let the questions come from the people who will use your insights. Talk to your leadership, healthcare providers, program staff, and even patients. Even if you only answer one question, if it is valuable enough to the organization then the cost required to answer it will be worth its weight in gold. Find the right question to answer.\n\n\nStep 2: Develop a framework to understand how to answer the question\nYour team needs to understand the various factors that influence the question you are trying to answer. There are several types of frameworks that can be helpful. Google “conceptual framework for monitoring and evaluation”, “results framework”, or “logic models” to learn more. Content experts should be intimately involved in the construction of the framework.\n\n\nStep 3 - KEY STEP!: Select the right variables to answer the question\nThis step is crucial for your success. From the framework, you should understand which variables will be important to answer the question. If you have limited resources to collect data, then you need to be extremely choosy about which variables to collect. Here are four points of advice for variable choice:\n\nKeep the variables per patient limited. An extremely rough guestimate is 20-30 data points per patient x 15 patients per week is probably the max for one person, although this can vary widely depending on the complexity of the data, how well the data are stored in the chart, and how easy the data are to enter into your database.\nChoose high signal, low noise variables. Dates are great. Easy to define, low measurement error, and chock full of information so that they can be combined in a lot of ways to give precise time spans. Things that are easy to identify and count are also nice (new cancer patients, easy to count; number of new pneumonia diagnoses each week, hard to count).\nFind the right level of specificity. If your data are too high level, they won’t give a useful answer to your question. If they are too granular, the complexity will grind down the quality over time unless you have a lot of help. It is better to have a few accurate data points than a lot of garbage.\nBeware dynamic variables. Variables that change over time are a different animal to collect than variables you can collect once and store forever. Patient outcomes are a necessary dynamic variable that is worth collecting. Collect more at your own risk.\n\n\n\nStep 4: Make a data dictionary and share it widely\nAs precisely as you are able, define each variable. This is important especially when nonclinical personnel are collecting data. Precise definitions cut down on measurement error, which will make your analyses more accurate and precise. Provide standard answer choices as much as possible. Enforce a data convention. I recommend YYYY-MM-DD for all dates.\n\n\nStep 5: Create a database. Avoid spreadsheets.\nSpreadsheets always seem like a good idea at first, but they have too many degrees of freedom and are highly likely to mutate over a year or two of use. Investigate Microsoft Access, REDCap, or DHIS2 to see if they meet your needs. The latter two are free and amazing. There are other software solutions out there as well. Make sure you have API access (your future sophisticated data science self will thank you). It is worth your time and effort to learn to use one and integrate it into your organization’s operations.\nEach of these steps are powerful and incredibly effective in isolation, but together they will produce beautiful, high-quality data that will answer many questions to improve the care you provide to your patients."
  },
  {
    "objectID": "docs/posts/2022-03-16-use-routine-data/index.en.html",
    "href": "docs/posts/2022-03-16-use-routine-data/index.en.html",
    "title": "10 ways Global Health organizations can use routine data to improve patient care",
    "section": "",
    "text": "Global Health needs Data Science because efficient and effective healthcare delivery is the key to success.\nHere are 10 examples of how Data Science capabilities can improve your organization’s operations and the care you provide to patients.\nAll of these ideas are possible to do well even if you don’t have an electronic medical record. All it takes is careful data collection of a limited number of variables. You don’t need big data to do Data Science successfully in Global Health, but you do need good data."
  },
  {
    "objectID": "docs/posts/2022-03-16-use-routine-data/index.en.html#data-science-global-health-ideas",
    "href": "docs/posts/2022-03-16-use-routine-data/index.en.html#data-science-global-health-ideas",
    "title": "10 ways Global Health organizations can use routine data to improve patient care",
    "section": "Data Science + Global Health Ideas",
    "text": "Data Science + Global Health Ideas\n\nForecast patient volumes, drug requirements, and medical equipment use\nIdentify risk factors for patients who might not return for needed care\nMonitor health outcomes disparities by age, sex, race, religion, etc.\nCreate a real-time dashboard of statistical process control charts\nAnalyze the referral pathways for patients to arrive at the facility\nUse geospatial information to map and monitor disease clusters\nCreate high-level dashboards of key indicators for leadership\nMeasure inpatient census and calculate nurse:patient ratios\nMonitor safety event frequency and flag increases early\nUse algorithms to predict clinical deterioration events"
  },
  {
    "objectID": "docs/posts/2022-03-16-use-routine-data/index.en.html#simple-measurements-create-powerful-insights",
    "href": "docs/posts/2022-03-16-use-routine-data/index.en.html#simple-measurements-create-powerful-insights",
    "title": "10 ways Global Health organizations can use routine data to improve patient care",
    "section": "Simple measurements create powerful insights",
    "text": "Simple measurements create powerful insights\nThere are tremendous opportunities to improve healthcare delivery in resource-constrained settings through the careful collection of a handful of variables. With a little time and diligence, you too can wrangle your data into useful insights about how to improve care for your patients."
  },
  {
    "objectID": "docs/posts/2022-03-12-cure-more-kids/index.en.html",
    "href": "docs/posts/2022-03-12-cure-more-kids/index.en.html",
    "title": "The single best way to cure more childhood cancers",
    "section": "",
    "text": "The best way to cure more childhood cancers is to develop better treatments, Right?\n…Right?\nImplicitly or explicitly, this is what most people believe. And it’s no wonder given our astounding success in developing effective therapies over the last six decades. In 1960, survival for acute lymphoblastic leukemia, a type of blood cancer, was &lt;10%. Then a group of heroic parents, doctors, nurses, scientists, philanthropists, and politicians joined forces and conducted the first large collaborative clinical trial to identify better treatments. Over the decades, we saw survival rise to 30% in the early 70s, 75% in the 80s, and over 85% today! It’s not just leukemia. In high-income countries like the United States, we can cure 85% of all cancer cases in kids. This is an astounding medical and scientific success story!\nUnfortunately, this success has created a belief that the most effective way to save lives now is to conduct more trials.\nThis is not true."
  },
  {
    "objectID": "docs/posts/2022-03-12-cure-more-kids/index.en.html#most-children-with-cancer-are-not-cured-because-they-do-not-receive-the-treatments-that-can-cure-them.",
    "href": "docs/posts/2022-03-12-cure-more-kids/index.en.html#most-children-with-cancer-are-not-cured-because-they-do-not-receive-the-treatments-that-can-cure-them.",
    "title": "The single best way to cure more childhood cancers",
    "section": "Most children with cancer are not cured because they do not receive the treatments that can cure them.",
    "text": "Most children with cancer are not cured because they do not receive the treatments that can cure them.\nAround 90% of children with cancer live in low- and middle-income countries where over half of cases may not be diagnosed, where treatment may not be available, and where cure rates can be as low as 10%.\nThis is a stunning disparity.\nWe, the medical community, have developed treatments that can cure up to 85% of cancer cases, yet the vast majority of children do not receive the treatments that we know they need.\nTo save more lives today, the most pressing question to answer is:\n\nHow do we deliver effective treatments to the kids who need them?"
  },
  {
    "objectID": "docs/posts/2022-03-12-cure-more-kids/index.en.html#focusing-on-care-delivery-shows-us-the-most-important-challenges-to-overcome",
    "href": "docs/posts/2022-03-12-cure-more-kids/index.en.html#focusing-on-care-delivery-shows-us-the-most-important-challenges-to-overcome",
    "title": "The single best way to cure more childhood cancers",
    "section": "Focusing on care delivery shows us the most important challenges to overcome",
    "text": "Focusing on care delivery shows us the most important challenges to overcome\n\nMedication availability\nTreatment quality\nSupportive care\nSupply chains\nStaffing\n\nThese are very different types of problems than developmental therapeutics, but the answer to each one can save many lives.\nDon’t get me wrong, we need better therapies so that we can cure every child with cancer. We need clinical trials.\nBut to bring about a day when every child in the world receives the best treatments possible, the global oncology community must answer the treatment delivery question."
  },
  {
    "objectID": "docs/posts/2022-04-02-distributed_intelligence/index.en.html",
    "href": "docs/posts/2022-04-02-distributed_intelligence/index.en.html",
    "title": "Distributed intelligence is the key to success for high-performing health systems in Global Health",
    "section": "",
    "text": "Who would you rather have care for you: the world’s smartest doctor in a poorly-function health system or a mediocre doctor in a high-functioning health system?\nTake the high-functioning health system every time.\nGenius doctors are still humans who can move and think only at the speed of a single person.\nEven if the doctor has limitless energy, she can only see so many patients in one day, make so many diagnoses, and fill so many prescriptions. Be ready for the long wait to see her.\nOf course, even the best doctor makes mistakes, but without a system to back her up, there is no safety net when she fails without a system to back her up.\nEvery doctor occasionally needs management advice from colleagues, but being the best of the bunch there will be nowhere to whom she can turn.\nShe needs backup. She needs a system."
  },
  {
    "objectID": "docs/posts/2022-04-02-distributed_intelligence/index.en.html#what-is-distributed-intelligence",
    "href": "docs/posts/2022-04-02-distributed_intelligence/index.en.html#what-is-distributed-intelligence",
    "title": "Distributed intelligence is the key to success for high-performing health systems in Global Health",
    "section": "What is distributed intelligence?",
    "text": "What is distributed intelligence?\nThe International Society of Learning Sciences gives a helpful outline of the concept:\n\nDistributed intelligence means that the resources that enable and mediate activity are distributed in configuration across people, environments, situations, and time.\nIntelligence is assembled and accomplished rather than possessed.\nTherefore, the boundary unit of analysis for learning is different with this orientation since intelligence “comes to life” in human activity.\n\nThis view acknowledges that no one person is smart enough to know everything that needs to be known or can move fast enough to do everything that needs to be done. Instead, each person acts together with others to accomplish the system’s goal.\nIntelligence is found in the motion of the whole system and not its constituent parts. In such systems, information is often distributed in the form of workflows, best practices, reference documents, technologies, teams, and cultures."
  },
  {
    "objectID": "docs/posts/2022-04-02-distributed_intelligence/index.en.html#distributed-intelligence-in-healthcare.",
    "href": "docs/posts/2022-04-02-distributed_intelligence/index.en.html#distributed-intelligence-in-healthcare.",
    "title": "Distributed intelligence is the key to success for high-performing health systems in Global Health",
    "section": "Distributed intelligence in healthcare.",
    "text": "Distributed intelligence in healthcare.\nThe totality of clinical knowledge in existence far exceeds the expertise of any single physician. When a doctor encounters a diagnosis he is unfamiliar with, he must call for help from a specialist.\nEven for specialists, there are times when a diagnosis requires a rarely used therapy, and she must consult institutional guidelines on how to order and administer a specific medicine.\nIf a patient is taking other medications, guidelines may not be sufficient to dose the medicine properly, and a pharmacist must be consulted who references an online interaction database.\nThe patient must be scheduled in an infusion room to receive the medication. The presence of the patient, the medicine, a nurse who can give it, and equipment for the infusion must be coordinated by the facilities staff.\nWhile the patient receives the medication, the nurse must monitor his vital signs and alerts a physician if any measurement exceeds the boundaries derived from research and past clinical experience encoded into clinical guidelines.\nWhere is the intelligence here? It is in the entire description. The patient was diagnosed and received the needed therapy because of the interacting actions between the doctor, specialist, guidelines, pharmacist, database, scheduling system, nurse, and clinical guidelines.\nThe system knew how to diagnose and treat the patient even though no single person did."
  },
  {
    "objectID": "docs/posts/2022-04-02-distributed_intelligence/index.en.html#data-science-distributes-intelligence-in-a-health-system.",
    "href": "docs/posts/2022-04-02-distributed_intelligence/index.en.html#data-science-distributes-intelligence-in-a-health-system.",
    "title": "Distributed intelligence is the key to success for high-performing health systems in Global Health",
    "section": "Data science distributes intelligence in a health system.",
    "text": "Data science distributes intelligence in a health system.\nAs I have written previously, data science is essential in global health because it allows information to flow where it is needed in the system, unlocking its intelligence and allowing patients to receive the care they require."
  },
  {
    "objectID": "docs/posts/2022-04-10-pediatric_abandonment/index.en.html",
    "href": "docs/posts/2022-04-10-pediatric_abandonment/index.en.html",
    "title": "Pediatric cancer treatment abandonment: a tragic but preventable event",
    "section": "",
    "text": "A tragic reality that pediatric oncology providers in resource-constrained commonly encounter: after a child with cancer starts treatment, many families leave and never return for care.\nThe global oncology community has given this event a specific label: treatment abandonment.\nTo explain why treatment abandonment happens and how to prevent it, let me tell you a story. The narrative is fictional, but it is based on the common experiences that families report while their child is undergoing cancer treatment.\nPablo was a five-year-old boy who lived with his mom and two younger sisters in a rural village in Central America.\nHe liked to do things most little boys like to do – kick a ball around in the street, chase his sisters with bugs, and help his mom care for their chickens. One day he developed a fever and became very pale. Over the next two weeks, the fevers didn’t stop, so his mom took him to the clinic in his village, where he saw a nurse who decided to send them for blood tests at the nearest hospital.\nIt was two days before his mom could secure a ride to make the normally two-hour journey to the hospital that took four hours because rain turned the dirt roads leading out of the village to mud. Workers at the hospital took blood from Pablo and after a few hours, a somber-looking doctor told his mom the bad news.\n\nThe doctor was concerned Pablo had a life-threatening blood cancer called “leukemia”.\nPablo’s mom had a 3rd-grade education. She primarily spoke the indigenous language of her village and was merely conversational in Spanish, her doctor’s primary language. The doctor spent an hour talking to her about many things, much of which she didn’t understand. She knew her son had a disease in his blood, and she knew it was life-threatening, but she still wasn’t clear how he got it and feared negative spiritual forces were at work.\nThe doctors started treatment in the hospital, and after a few days, Pablo started looking much better, almost back to normal. He was discharged from the hospital and told he had appointments to come back every week for the next several months. Their family had government insurance that covered part of the hospital stay, but they would still have to pay the costs of travel, food, and lodging for every visit.\nPablo’s aunt and uncle lived in the city where the hospital was located.\n\n\nPablo and his mom moved in with his aunt and uncle, their 4 kids, 2 dogs, and a coop full of chickens to save money.\nHis sisters had to stay with their grandmother back in the village.\nAfter five weeks of treatment, his aunt was convinced that he was cured. “He looks fine,” she would tell his mom, “how could he still possibly be sick?” She would also insist that many doctors are greedy and give people medicine only to make money. Over time, his mother did notice that Pablo seemed to look sicker after visiting the doctor and receiving treatment. Perhaps his aunt was right, she would think to herself.\n\n\nBy week 12 of treatment, Pablo’s mom felt she was near a breaking point.\nShe lost her job because of her long absence, and they were dependent on financial support from extended family members who had little disposable income. Living in the full house with their relatives was cramped and noisy, and the family felt like a burden. Pablo was sick most of the time and would cry at night because he missed his sisters.\nHis mom tried to talk to the doctors several times about their difficulties, but she felt she didn’t get her point across in her broken Spanish, and it appeared to her that the busy doctors did not have time to understand her problems.\nEventually, his mother heard about a traditional religious healer near their village whose treatments were much cheaper. She talked with several family members who told her the traditional healer had helped them when they were sick. This was enough to help her make the decision.\n\n\nThat day they packed up and returned to their home. They said nothing to their doctors, nor did they return to finish treatment.\nCancer treatment can be a brutal journey, but if the entire treatment is not finished, the patient is at extremely high risk for the aggressive return of their disease.\nThis narrative illustrates the corrosive effects of cancer on the whole family.\n\nFinancial difficulties\nGeographic barriers to care\nDisrupted life rhythms and social strain\nPoor communication with the healthcare team\nPoor understanding of cancer and how to successfully treat it\nReliance on extended family for material and financial support\n\n\n\nThe answer to treatment abandonment is to understand the difficulties of a family’s journey and support them through it.\nThere is nothing overly complex here. Families need support during the treatment journey. Financial assistance, material support with food and housing assistance, improved family education and communication with the healthcare team, and initiatives about how communities can support families through treatment all help to dramatically reduce treatment abandonment rates. There is a significant body of published research that demonstrates the efficacy of all of these interventions.\nThe link between family support and the successful completion of treatment is so strong, that many providers argue it should be prioritized as important as medicine or surgery.\n\n\nSupporting families financially, psychologically, socially, and spiritually is an essential part of pediatric cancer treatment."
  },
  {
    "objectID": "docs/epistemic_limits/index.html",
    "href": "docs/epistemic_limits/index.html",
    "title": "Epistemic Limits",
    "section": "",
    "text": "We’re going to talk about all the things."
  },
  {
    "objectID": "docs/epistemic_limits/absolute_limits/index.html",
    "href": "docs/epistemic_limits/absolute_limits/index.html",
    "title": "Absolute Limits",
    "section": "",
    "text": "Explanation: These are limits for any Learner within this universe, no matter their intellectual capacity. These are fundamental limits that govern all relationships that obtain in this universe.\n\n\n\n\n\n\n\n\nType\nDescription\nExamples\n\n\n\n\nLogical Limits\nLimits to what can be proven or known through formal reasoning\nThe liar paradox, barber paradox\n\n\nPhysical Limits\nLimits to what can be observed or measured due to the laws of physics\nThe uncertainty principle, the speed of light, the no-cloning theorem\n\n\nMathematical Limits\nLimits to what can be proven or calculated using mathematical methods\nGödel’s incompleteness theorems, detecting randomness?\n\n\nComputational Limits\nLimits to what can be computed or predicted by an algorithm\nThe Halting Problem, factoring large numbers, solving the traveling salesman problem\n\n\nPhenomenological Limits\nLimits on what can be known in experience.\nBrain in a vat, solipsism"
  },
  {
    "objectID": "docs/epistemic_limits/absolute_limits/index.html#first-order-absolute-limits",
    "href": "docs/epistemic_limits/absolute_limits/index.html#first-order-absolute-limits",
    "title": "Absolute Limits",
    "section": "",
    "text": "Explanation: These are limits for any Learner within this universe, no matter their intellectual capacity. These are fundamental limits that govern all relationships that obtain in this universe.\n\n\n\n\n\n\n\n\nType\nDescription\nExamples\n\n\n\n\nLogical Limits\nLimits to what can be proven or known through formal reasoning\nThe liar paradox, barber paradox\n\n\nPhysical Limits\nLimits to what can be observed or measured due to the laws of physics\nThe uncertainty principle, the speed of light, the no-cloning theorem\n\n\nMathematical Limits\nLimits to what can be proven or calculated using mathematical methods\nGödel’s incompleteness theorems, detecting randomness?\n\n\nComputational Limits\nLimits to what can be computed or predicted by an algorithm\nThe Halting Problem, factoring large numbers, solving the traveling salesman problem\n\n\nPhenomenological Limits\nLimits on what can be known in experience.\nBrain in a vat, solipsism"
  },
  {
    "objectID": "docs/epistemic_limits/absolute_limits/index.html#second-order-absolute-limits",
    "href": "docs/epistemic_limits/absolute_limits/index.html#second-order-absolute-limits",
    "title": "Absolute Limits",
    "section": "Second-Order Absolute Limits",
    "text": "Second-Order Absolute Limits\nExplanation: These are limits for any Learner within this universe, no matter their intellectual capacity. These are derivative of the Absolute Epistemic Limits in that they would not obtain in this universe if the absolute epistemic limits did not obtain. However, the absolute epistemic limits could still obtain without these. Learners may wish to engage in the activities represented by these limits, but their knowledge will always be limited due to interaction between these activities at the first-order absolute limits.\n\n\n\n\n\n\n\n\nType\nDescription\nExamples\n\n\n\n\nRetrodictive Limits\nLimits to what can be known or understood about past events or cultures\nThe limits of historical knowledge, the bias of historical sources\n\n\nPredictive Limits\nLimits to how accurately we can predict future events or behaviors\nThe limits of prediction, chaos theory\n\n\nScientific Limits\nLimits to what can be known or understood through scientific inquiry\nThe limits of scientific knowledge, the difficulty of studying complex phenomena\n\n\nTechnological Limits\nLimits imposed by the technology used to measure or observe phenomena\nThe limits of resolution, signal-to-noise ratio\n\n\nCommunication Limits\nLimits to what can be communicated or understood through symbolic representation\nThe limits of translation, the difficulty of expressing certain concepts in language"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Doctor's Dialectic",
    "section": "",
    "text": "Dī-ə-ˈlek-tik: A method of examining and discussing opposing ideas in order to find the truth.\n\nA dialectic is a conversation between people with opposing views who wish to find truth through reasoned arguments.\nThis is an ancient form of collective reasoning. Popularized by Socrates in 400 BC, it remains a crucial method for forming social consensus and driving scientific progress.\nAmid the noise and nonsense of modern life, a thoughtful dialectic can help us discover truth and meaning in a time when both are increasingly difficult to find."
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "The Doctor's Dialectic",
    "section": "",
    "text": "Dī-ə-ˈlek-tik: A method of examining and discussing opposing ideas in order to find the truth.\n\nA dialectic is a conversation between people with opposing views who wish to find truth through reasoned arguments.\nThis is an ancient form of collective reasoning. Popularized by Socrates in 400 BC, it remains a crucial method for forming social consensus and driving scientific progress.\nAmid the noise and nonsense of modern life, a thoughtful dialectic can help us discover truth and meaning in a time when both are increasingly difficult to find."
  },
  {
    "objectID": "docs/about/index.html",
    "href": "docs/about/index.html",
    "title": "The Doctor's Dialectic",
    "section": "",
    "text": "Twitter\n  \n  \n    \n     Github\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email"
  },
  {
    "objectID": "docs/about/index.html#welcome-to-the-doctors-dialectic",
    "href": "docs/about/index.html#welcome-to-the-doctors-dialectic",
    "title": "The Doctor's Dialectic",
    "section": "Welcome to The Doctor’s Dialectic",
    "text": "Welcome to The Doctor’s Dialectic\nHi, I’m Dr. Mark Zobeck. I’m a physician who cares for children with cancers and blood disorders around the world. When not caring for patients, I also conduct research. My primary focus is on creating clinical prediction models to improve treatment outcomes for pediatric cancers. I also data science to improve the delivery of medical care in low- and middle-income countries. I love writing about and teaching statistics. Personally, I have a bias toward the Bayesian. For better or worse, I’m also given to musing about the metaphysical and epistemological foundations of science. You’ll find some weird fusion of all of these things on this blog. Welcome!"
  },
  {
    "objectID": "docs/about/index.html#why-the-theme-of-the-dialectic",
    "href": "docs/about/index.html#why-the-theme-of-the-dialectic",
    "title": "The Doctor's Dialectic",
    "section": "Why the theme of the dialectic?",
    "text": "Why the theme of the dialectic?\nI think there are things that we can only learn through slowly turning opposing viewpoints over and over in our heads. There is truth in the tension between ideas. As the world becomes increasingly fast paced and polarized, this practice is harder to find. As generative algorithms produce exponential amounts of entertaining refuse, the patient exploration of ideas is being swallowed in noise.\nThis blog is my attempt to engage in this type of discussion with the reader and with myself."
  },
  {
    "objectID": "docs/about/index.html#obligatory-disclaimers",
    "href": "docs/about/index.html#obligatory-disclaimers",
    "title": "The Doctor's Dialectic",
    "section": "Obligatory disclaimers:",
    "text": "Obligatory disclaimers:\n\nNothing you read here constitutes medical advice. For medical questions, please consult your doctor.\nAll views expressed are my own and do not represent the views of my employer or any organizations with which I am affiliated."
  },
  {
    "objectID": "docs/posts/2022-04-07-htma-5-calibration-techniques/index.en.html",
    "href": "docs/posts/2022-04-07-htma-5-calibration-techniques/index.en.html",
    "title": "How to measure anything in Global Health: 5. Calibration techniques",
    "section": "",
    "text": "In my previous essay, I discussed how calibrating your judgments is a crucial step when measuring anything in global health.\nFor instance, if you want to estimate the likelihood a new cancer treatment causes severe side effects, then a calibrated estimator can offer a 90% uncertainty interval about a plausible range of side effect rates as a starting place for measurement.\nAlthough it is essential in the measurement process, calibration can be a tricky state to achieve."
  },
  {
    "objectID": "docs/posts/2022-04-07-htma-5-calibration-techniques/index.en.html#here-are-5-recommendations-for-how-to-achieve-calibration-with-your-judgment.",
    "href": "docs/posts/2022-04-07-htma-5-calibration-techniques/index.en.html#here-are-5-recommendations-for-how-to-achieve-calibration-with-your-judgment.",
    "title": "How to measure anything in Global Health: 5. Calibration techniques",
    "section": "Here are 5 recommendations for how to achieve calibration with your judgment.",
    "text": "Here are 5 recommendations for how to achieve calibration with your judgment.\nThis material comes from Douglas Hubbards marvelous book, How to Measure Anything. I recommend you read the book if these posts interest you. You will find much more thorough explanations of these concepts there.\n\n1. The equivalent bet test\nAsk yourself, would you rather accept a bet to win $1000 if the true value falls in your uncertainty interval or accept a bet to spin a roulette wheel with a wedge of 10% of its area where you win nothing and 90% of its area where you win $1000. If you are calibrated, you should be indifferent between these bets. If you prefer your uncertainty interval, you’re overconfident. If you prefer the wheel, you’re underconfident.\n\n\n2. The premortem test\nExplain what would be wrong with your interval if the true value turns out to be outside your interval. If you can come up with a plausible failure mode, then perhaps your interval is too narrow.\n\n\n3. Test the limits\nExam the lower limit, then the upper limit of the interval. For a 90% interval, 95% of the plausible values should be above the lower limit. Does your limit reflect that estimate? Then move to the upper limit and assess it its plausible that 95% of values are below that limit. This helps to avoid anchoring bias, something I found myself susceptible to as I started exploring calibration.\n\n\n4. The absurdity test\nStart with an absurdly wide range and progressively narrow it, explaining why it can’t be that wide along the way. This test helps to locate the edge of our knowledge about the estimate by eliminating things that are obviously not correct.\n\n\n5. Repetition and feedback\nGo to the HTMA website and download the calibration tests to take, or make up your own questions, estimate the answers, and grade them. Calibration is not a skill that comes naturally to most people, but is a skill that must be practiced. There’s a large body of literature on the topic you can also reference, something I was unfamiliar with until recently.\nIt takes a little time and effort, but soon you can calibrate your judgment to measure anything in global health that will improve the care you provide to your patients!"
  },
  {
    "objectID": "docs/posts/2022-06-04-epistemic-arc-intro/index.en.html",
    "href": "docs/posts/2022-06-04-epistemic-arc-intro/index.en.html",
    "title": "The Epistemic Arc - a conceptual map about how to learn from data in the presence of uncertainty",
    "section": "",
    "text": "Learning from data is hard.\nThe world is full of noise and nonsense masquerading as “insights” from data analyses.\nIf its so easy to go wrong, how can we reliably learn from data and avoid the many common analytical shenanigans that can ruin our results?\nI recently purchased and watched the videos for Frank Harrell’s incredible statistics short course, Regression Modeling Strategies(for the more advanced stats enthusiasts, this course outlines an incredible method for principled statistical analyses). As part of the course, Drew Levy gave a wonderful presentation about model selection using causal models, and, almost as an aside, he included a graphic about his framework for how he conceptualizes analyses of observational data.\nI was struck by the elegance of his framework. It concisely summarizes the steps in the learning process and identifies the many danger zones that can ruin your learning (click here to read more from Drew Levy’s website). It functions as a great conceptual roadmap for how to reliably learn from data while avoiding analytical shenanigans\nIn this post, I will adapt and expand the framework to explain how it provides a unified roadmap for learning from data. This framework is flexible and applies just as well to observational epidemiological studies as to randomized trials, physical experiments, any other branch of science, and even in our daily lives.\nThis will be a high level overview. In future posts, I will explore specific danger zones in more detail. Beyond organizing important statistical principals, this framework has important philosophical and psychological consequences. I also aspire to explore theses areas more in future posts."
  },
  {
    "objectID": "docs/posts/2022-06-04-epistemic-arc-intro/index.en.html#introduction",
    "href": "docs/posts/2022-06-04-epistemic-arc-intro/index.en.html#introduction",
    "title": "The Epistemic Arc - a conceptual map about how to learn from data in the presence of uncertainty",
    "section": "",
    "text": "Learning from data is hard.\nThe world is full of noise and nonsense masquerading as “insights” from data analyses.\nIf its so easy to go wrong, how can we reliably learn from data and avoid the many common analytical shenanigans that can ruin our results?\nI recently purchased and watched the videos for Frank Harrell’s incredible statistics short course, Regression Modeling Strategies(for the more advanced stats enthusiasts, this course outlines an incredible method for principled statistical analyses). As part of the course, Drew Levy gave a wonderful presentation about model selection using causal models, and, almost as an aside, he included a graphic about his framework for how he conceptualizes analyses of observational data.\nI was struck by the elegance of his framework. It concisely summarizes the steps in the learning process and identifies the many danger zones that can ruin your learning (click here to read more from Drew Levy’s website). It functions as a great conceptual roadmap for how to reliably learn from data while avoiding analytical shenanigans\nIn this post, I will adapt and expand the framework to explain how it provides a unified roadmap for learning from data. This framework is flexible and applies just as well to observational epidemiological studies as to randomized trials, physical experiments, any other branch of science, and even in our daily lives.\nThis will be a high level overview. In future posts, I will explore specific danger zones in more detail. Beyond organizing important statistical principals, this framework has important philosophical and psychological consequences. I also aspire to explore theses areas more in future posts."
  },
  {
    "objectID": "docs/posts/2022-06-04-epistemic-arc-intro/index.en.html#the-epistemic-arc",
    "href": "docs/posts/2022-06-04-epistemic-arc-intro/index.en.html#the-epistemic-arc",
    "title": "The Epistemic Arc - a conceptual map about how to learn from data in the presence of uncertainty",
    "section": "The Epistemic Arc",
    "text": "The Epistemic Arc\n\nThe above figure depicts the Epistemic Arc, which describes the process that anyone must follow when learning from data in the presence of uncertainty.\nThe first highlight is that there is a main character to this story: you. You are the hero or heroine of this epistemic adventure! You are a player on the statistical stage. You are an inseparable part of the learning process. You are the Learner.\nEach box represents a specific object that is required for the learning process (a noun). Each arrow depicts a step that must be performed to move from one object to the next (a verb). Let’s zoom in on each step to define the objects involved and the actions performed during it.\n\nStep 1. Nature -&gt; Population\n\n\nDefinitions\nStep 1: The process of nature or a physical process producing a population or system the Learner wishes to learn more about.\nNature or Physical Process: I define nature as whatever physical stuff exists upon which the drama of physical reality unfolds. Nature is the generator of the physical world the Learner encounters and wishes to know more about. A physical process is a mechanistic derivative of nature, perhaps built by humans, that may itself produce interesting phenomena to learn about. Examples: Nature - quarks, neurons, cats, quasars; Physical process - factory that produces bowling pins, a pendulum swinging in a clock.\nPopulation or System: A population is a distinct collection of entities derived from nature that is interesting to the Learner. A system is a set of interacting entities carved out of the larger system of physical reality. Examples: Population - Orange cats in Austin, Texas, children with ADHD, all the weeds that grow between sidewalk cracks; System - trees that talk to each other using fungi, academic medical education, ATP synthesis in your body from the electron transport chain.\n\n\nExplanation\nGoal: The Learner identifies what he or she wishes to learn about either in nature or in a population or system that emerges from it. This is where the Learner formulates the questions to answer with data. (e.g. For the weed between sidewalk cracks example, perhaps the question is: which type of pesticide most effectively kills weeds between sidewalk cracks?)\nCaution: Note this is a highly philosophical step, even if it seems straightforward to the Learner. There are no populations or systems in nature. These are concepts that the Learner overlays onto nature. Why does this matter? Because we, the Learners, are making analytical decisions about the most fundamental elements in the epistemic arc by defining the population or system to study ourselves (What is the definition of a weed? What is a crack? What is a sidewalk? Does a driveway count? Where will study the weeds?).\nBeware: This step sets the entire trajectory of the epistemic arc the Learner will follow. Precise questions yield precise answers. Vague questions produce vacuous answers that can be stretched, misshapen, and filled with whatever meaning one desires. Squishy questions may help get publications, but we won’t be any closer to learning truth.\nBonus: Sander Greenland recently argued that the discipline of statistics has explicitly causal foundations. In his paper, he says, “Whether answering the most esoteric scientific questions or the most mundane administrative ones, and whether the question is descriptive, causal, or purely predictive, causal reasoning will be crucially involved (albeit often hidden to ill effect in equations and assumptions used to get the”results”). I think this is correct and this step demonstrates and contextualizes his argument.\n\n\n\nStep 2. Population -&gt; Sample\n\n\nDefinitions\nStep 2: The process of identifying (no data collection yet) a sample that can be further scrutinized and designing a process that can deliver appropriate inferences about the population or the natural process that generates it from the data collected from the sample.\nSample (noun): A subset of the population or a toy model of the system. These are the members of the population for whom the Leaner has the ability to collect data and directly learn from.\n\n\nExplanation\nGoal: The goal is for the Learner to identify what sample will generate data that is most informative for the questions and then to design the process to learn from the data in a way that will account fo the limitations of using an imperfect sample to learn about a (possibly theoretical) population. In science, this is study design - e.g. does the Learner need to conduct an RCT, case-control study, cross-sectional study, etc.\nCaution: Randomness plays a big role in in our ability to learn about our population of interest from the sample. A random sample from the population as in a survey helps to ensure the sample is reflective of the population. A sample that is randomized to a treatment as in a randomized clinical trial helps to ensure what we learn about the causal process we are studying. Often randomness cannot be ensured at the level of the sample. In this case, study design and methods of causal inference help to make the sample reflects the population well enough to answer our question.\nCaution 2: Confusing terminology - to sample (verb) is one method of obtaining a sample (noun), but that’s only one way. Sometimes your sample is given to you, such as obtaining all medical records for a given disease from an electronic medical record. In this case you have a sample, a subset of a larger population (all patients with the disease), but you didn’t sample the population for it.\nBeware: Many crippling problems arise in this step even though you have not collected any data! Selection bias occurs when there’s a systematic reason your sample does not reflect your population. Confounding occurs when you choose a sample for whom data about an important feature of the sample that affects the relationship between the treatment/exposure and the outcome will be unavailable or uncollected. If your analysis does not have data about this feature, then what you learn could be systematically different than the truth.\nBonus: Directed acyclic graphs (DAGs) are a great way to graphically encode relevant information about the casual system, population, and sample from steps 1 and 2. I recommend their development in this step. DAGs can display not just confounding, but also selection bias (and here) and many other design considerations.\n\n\nStep 3. Sample -&gt; Data\n\n\n\nDefinitions\nStep 3: The process of measuring features of the sample (i.e. collect data).\nData: A collection of measurements of variables (features) that describe different interesting aspects of the sample.\n\n\nExplanation\nGoal: The Learner measures values of the variables accurately!\nCaution: Which variables are measured matters both because of the potential for confounding as discussed in step 2, as well as the potential to induce collider bias that could emerge in step 5. How variables are measured also matters. For example, continuous variables contain much more information than binary variables, so if the Learner chooses to classify a continuous variable as a binary variable (e.g. blood pressure as high or low instead of the systolic/diastolic measurements), then the Learner loses information before any analysis has been performed.\nBeware: More crippling problems arise here. Measurement error refers to different ways of inaccurately collecting or recording data (e.g. measuring blood pressure in a screaming 3-year-old will not be accurate). Information biases are a related and overlapping group of problems that occur during the conduct of the study that will make the analysis inaccurate. Incomplete or missing data can ruin an otherwise well designed study. Dichotomania (as discussed in the caution) can be so damaging that it prevent the Learner from answering his or her question when they would have been able to if the variables were measured as continuous or ordinal.\n\n\n\nStep 4. Data -&gt; Analysis\n\n\nDefinitions\nStep 4: The process of wrangling and transforming the raw data into a format that can be fed into an analysis.\nAnalysis: The method that will be used to answer the question (in the next step) comprised of the transformed data, the relationships between the data (e.g. which is a predictor and which is the outcome variable), and the mathematical and algorithmic machinery that will perform the computational operations.\n\n\nExplanation\nGoal: The Learner transforms the data accurately and in a way that maximally preserves information so that it can be analyzed.\nCaution: How data are transformed matters, just like how they are measured matters. If a variable is measured as continuous and than transformed into a binary variable, that will harm the analysis. Also, many errors are induced in this step because of disorganization, clumsy human hands, and non-reproducible data management. Reproducible research methods are very practical and very valuable for preventing these errors.\nBeware: Dichotomania can crop up again here! Also, if you happen upon an interesting data set, it is very tempting to start at this step. This is a fatal mistake. Many of the concerns we’ve discussed (e.g. selection bias, confounding, measurement error) occur before or during data generation. Steps 1-3 always proceed step 4, no matter how big is your data or how fancy is your machine learning technique, and if you are unsure of the quality of steps 1-3, consider step 4 fatally compromised.\n\n\n\nStep 5. Analysis -&gt; Inference\n\n\nDefinitions\nStep 5: The process of conducting the analysis and answering the question using the data collected from the sample.\nInference or Prediction: Inference is the result of using data from the sample to learn about the population or system of interest. Prediction is using the data from the sample to understand what may be true about the population in the future. These are generally perceived as separate but related learning tasks, and the Learner’s goal may only be one or the other.\n\n\nExplanation\nGoal: The Learner conducts the analysis to produce inferences or predictions that he or she will use to answer the question.\nCaution: This is where technical statistical and mathematical considerations dominate. Whole PhD theses are written over very specific technical details in this step. Let the Learner be careful and seek appropriate technical support.\nBeware: Due to the technical nature of this step, many sneaky risks lurk here. Model assumptions, model specification, and model selection refer to the process of developing statistical models that reflect in math the most important concepts required to answer the question. The bias-variance trade off is an important decision about how flexible a model should be to reflect the observed data. P-hacking and researchers degrees of freedom/garden of forking paths are all non-mathematical methods of massaging the data (intentionally or unintentionally) to get an interesting or publishable result (here is a blog post that discusses many of the preceding points). Also this is where you can induce bias by adding variables into your analysis, called collider bias(are you frightened?). Analysis workflows (Bayesian workflows here, and here, RMS workflow - chapter 4, particularly 4.12 - and here) can help guide you safely through this treacherous valley of death and deceit.\n\n\n\nStep 6. Inference -&gt; Decision\n\n\nDefinitions\nStep 6. The process of using the inference along with other information that the Learner knows to draw conclusions about the question and then decide how to act given what has been learned.\nDecision & Action: These are the decisions and actions that result from what was learned. These are the answers to the questions “now what do I do with what I learned?”\n\n\nExplanation\nGoal: The Learner considers the inferences or predictions that result from the analysis against other information that the Learner knows (i.e. the results of many other epistemic arcs) and against the costs and benefits of different outcomes of decisions to decide what to is the best course of action based on the new knowledge.\nCaution: Inferences and predictions are not decisions. These are very different concepts that are often conflated together. Their conflation leaves out two crucial elements. One is that that you, the Leaner, bring many other learned things to this step (I conceptualize it as the intersection of many intersecting epistemic arcs). This additional information is relevant for decision making. The second is that costs and benefits from outcomes of decisions are nowhere reflected in this learning process (they also require their own epistemic arcs!; this blog gives a helpful primer on decision curve analysis).\nBeware: This is where human psychology with all of its frailty and biases can dominate. We all can fall prey to confirmation bias and many other cognitive biases, to motivated reasoning, and to dishonest reasoning. The consequence of us being an inseparable part of the learning process is that we bring our psychological and moral baggage along with us :/"
  },
  {
    "objectID": "docs/posts/2022-06-04-epistemic-arc-intro/index.en.html#conclusion",
    "href": "docs/posts/2022-06-04-epistemic-arc-intro/index.en.html#conclusion",
    "title": "The Epistemic Arc - a conceptual map about how to learn from data in the presence of uncertainty",
    "section": "Conclusion",
    "text": "Conclusion\n\n\n\n\n\nStep by step, we’ve walked along the Epistemic Arc to learn from data in the presence of uncertainty. Along the way, we’ve noted some areas where we must use caution and many explosive areas we must navigate with extreme care. We’ve also seen how the Learner is not only the primary agent doing the learning, but also a necessary actor in the learning process. The Epistemic Arc unifies these disparate philosophical, psychological, and statistical concepts into a single framework that we can use as a roadmap to conduct principled data analysis and avoid producing noise and nonsense that masquerade as “insights”."
  },
  {
    "objectID": "docs/posts/2020-12-04-winners-curse/index.en.html",
    "href": "docs/posts/2020-12-04-winners-curse/index.en.html",
    "title": "The Winner’s Curse",
    "section": "",
    "text": "When reading a study, have you ever found yourself skeptical of the “statistically significant” findings for reasons you can’t quite identify? You’ve read the methods section, and it seems technically sound. The study question was interesting, the alternative hypothesis was plausible, the variables were sensibly chosen, the data was collected appropriately for the type of study with no apparent selection bias, and the statistical analysis was thoughtfully performed and interpreted with great care to not overgeneralize the findings. Despite all of the study’s virtues, the low buzz of skepticism continued to rumble in the back of your mind. Where does this skepticism come from?\nIt may be that you have an intuitive grasp of an inherent problem with significance testing that makes inconsequential findings seem impressive. Filtering results based on significance testing can induce a bias that inflates effect sizes, even if the study itself is conducted according to the highest standards of rigor. This overconfidence can produce large, publishable findings that will never replicate in follow-up investigations. This phenomenon is called “The Winner’s Curse”, which takes its name from the finding that winners of auctions tend to overpay for their winnings. Even if you are a thoughtful researcher who cares about replicability, you can sill lose when you think you’ve won due to the possible overconfidence induced by significance testing.\nLet’s look at an example to build intuition about when a finding may be at risk from suffering The Winner’s Curse.\nThis example is taken from Andrew Gelman’s and colleagues’ wonderful book Regression and Other Stories. For a different take on a similar phenomenom, see Gelman’s blog. For a more technical treatment of the subject, see this paper."
  },
  {
    "objectID": "docs/posts/2020-12-04-winners-curse/index.en.html#that-fuzzy-feeling",
    "href": "docs/posts/2020-12-04-winners-curse/index.en.html#that-fuzzy-feeling",
    "title": "The Winner’s Curse",
    "section": "",
    "text": "When reading a study, have you ever found yourself skeptical of the “statistically significant” findings for reasons you can’t quite identify? You’ve read the methods section, and it seems technically sound. The study question was interesting, the alternative hypothesis was plausible, the variables were sensibly chosen, the data was collected appropriately for the type of study with no apparent selection bias, and the statistical analysis was thoughtfully performed and interpreted with great care to not overgeneralize the findings. Despite all of the study’s virtues, the low buzz of skepticism continued to rumble in the back of your mind. Where does this skepticism come from?\nIt may be that you have an intuitive grasp of an inherent problem with significance testing that makes inconsequential findings seem impressive. Filtering results based on significance testing can induce a bias that inflates effect sizes, even if the study itself is conducted according to the highest standards of rigor. This overconfidence can produce large, publishable findings that will never replicate in follow-up investigations. This phenomenon is called “The Winner’s Curse”, which takes its name from the finding that winners of auctions tend to overpay for their winnings. Even if you are a thoughtful researcher who cares about replicability, you can sill lose when you think you’ve won due to the possible overconfidence induced by significance testing.\nLet’s look at an example to build intuition about when a finding may be at risk from suffering The Winner’s Curse.\nThis example is taken from Andrew Gelman’s and colleagues’ wonderful book Regression and Other Stories. For a different take on a similar phenomenom, see Gelman’s blog. For a more technical treatment of the subject, see this paper."
  },
  {
    "objectID": "docs/posts/2020-12-04-winners-curse/index.en.html#losing-while-winning",
    "href": "docs/posts/2020-12-04-winners-curse/index.en.html#losing-while-winning",
    "title": "The Winner’s Curse",
    "section": "Losing while winning",
    "text": "Losing while winning\nSuppose I conduct a study looking at a blood pressure lowering medicine in kids less than 3 years old. I’ll run a randomized, double-blind, placebo controlled trial and look at the difference of means of the two groups. I’m only interested in the medicine’s ability to lower blood pressure, so I plan to do a 1-sided t-test for the hypothesis that the placebo group’s blood pressure minus the treatment group’s is greater than 0 (i.e. treatment lowers blood pressure) and test for significance at the \\(\\alpha = 0.05\\) level.\nSince we have omniscient control of this fake study, suppose the true effect of the medicine will on average lower the blood pressure by 2 mm Hg. So, in reality the treatment “works” even if the effect is small. In the real world the null hypothesis of no association should be rejected since we know the true effect of the treatment is different than the placebo. Crucially, suppose I won’t care that the children scream bloody murder half the time I try to measure the blood pressure (the joys of pediatrics) and will accept whatever readings I get. No one’s got time to try to convince a 2-year-old that blood pressure machines aren’t scary. I anticipate that the standard deviation will be rather large in both groups.\nStill in statistician god-mode, let’s suppose under these conditions with the sample size and the variability of the data, the standard error of the mean for this study will be 8 mm Hg.\nNow the central questions: given the true effect of 2 mm Hg and the study standard error of 8 mm Hg, how large of an effect will we need to estimate to reject the null hypothesis? Let’s look at the following figure to find out.\n\n\n\n\n\nThe black curve in the figure is the sampling distribution for the estimated difference in means between the placebo and treatment groups. This is the curve we would get if we ran the exact same study 10,000 times with different samples from the same population and calculated the average difference between the groups each time. Since we can’t do this is in real life, I simulated it on my computer. The true difference that we set is 2 mm Hg (blue line) and the standard error from the study is 8 mm Hg, which is why the curve is so spread out. Since the kids scream half the time the blood pressure is measured, the difference in means can vary from one simulation to the next. This is why some studies could give us a negative effect estimate (i.e. the treatment raises blood pressure) or why the size of the effect estimate could be much larger than 2 mm Hg. In fact, the shaded red region starting at 12 mm Hg represents the estimates at or greater than the 95th percentile of all possible estimates if the null hypothesis was true. This area represents “statistically significant” results and comprises 8% of possible studies. In other words, there is an 8%, or roughly 1 in 12, chance my study estimates an effect size in this region.\nSuppose I complete this study and get very excited because I got an average difference of 16 mm Hg (p = 0.04)! I’ll publish the finding straight away in the fanciest journal I can find. Why is this a problem? If we take an average difference of 16 mm Hg as the effect size, then it is 8 times larger than the real effect of 2 mm Hg and in reality this impressive looking finding is clinically meaningless! Lowering the blood pressure by 16 mm Hg for a 3 year old can be the difference between severe hypertension and a safe level, but lowering it by 2 mm Hg doesn’t mean much at all.\nThis finding also won’t replicate on follow-up investigations. Or even worse it will replicate in a way that reifies the inflated estimates. Suppose 12 similar follow-up studies were done, then it’s likely 1 of the other 12 will also give “significant” findings of effect. It’s possible that, due to publication bias, only the two significant studies are published, or at least published in a journal of note. One can read the literature and then conclude that these two technically sound studies that each giving similar effect estimates must represent good evidence of meaningful effect!"
  },
  {
    "objectID": "docs/posts/2020-12-04-winners-curse/index.en.html#avoiding-the-curse",
    "href": "docs/posts/2020-12-04-winners-curse/index.en.html#avoiding-the-curse",
    "title": "The Winner’s Curse",
    "section": "Avoiding the Curse",
    "text": "Avoiding the Curse\nThe Winner’s Curse can cause the size of “statistically significant” effect estimates to be inflated and can turn meaningless true differences into meaningful-but-false-but-publishable findings. A study is more at risk of suffering from a Winner’s Curse when the true effect size is small or the variability of the outcome variable is large. If you work in or read the literature from a field where effect sizes are small and/or measurement error is large, such as psychology, sociology, political science, or epidemiology, then you should take note of this danger!\nTo look for markings of the Curse, first think about how precisely can the outcome of interest be measured. Can it be measured precisely like a physics experiment? Or is there expected noise in the measurement, like measuring blood pressure in a screaming 2-year-old? If the outcome tends toward the imprecise range of the spectrum, the Curse is more likely. Also estimate for yourself a range of plausible effect sizes from the intervention being studied. If you think most plausible effect estimates are small, again the Curse is more likely. Finally think about how large is the possible measurement error compared to the true effect. If the supposed measurement error is many times larger than the plausible true effect size (like 8:2 in our example), the Winner’s Curse is going to haunt any of your “significant” findings."
  },
  {
    "objectID": "docs/posts/2020-11-27-gccp1-3/index.en.html",
    "href": "docs/posts/2020-11-27-gccp1-3/index.en.html",
    "title": "The Global Childhood Cancer Puzzle - Edge 1.3",
    "section": "",
    "text": "The following originally appeared on my podcast about global PHO - Global Health and Childhood Cancer\n\n\nThis is part three of a three part essay about the global burden of childhood cancer. If you haven’t read from the beginning, you may want to start from part 1 or check out part 2 if you missed that.\n\n\nMeasuring the health consequences of cancer\nLet’s think for a minute what the measures that we’ve discussed are telling us. Incident cases tells us about the number of new cases that occur in the world and survival tells us the final outcome for someone who has the disease. These measures tell us about the beginning and the end of the cancer journey, but there are many things that happen between those two points that are worth measuring, particularly the suffering and loss of wellbeing that result from cancer. Now you may be wondering, “how in the world do we measure loss of wellbeing? Loss compared to what?” The answer is that researchers measure it against the expected level of health for a patient if they never developed cancer.\nLet me provide an example to clarify these concepts. Suppose there is a restaurant with 100 people eating in it. Suppose most people have a lovely time drinking and chatting with friends; however, ten unlucky people order the chicken special that was undercooked by the chef and immediately get terrible food poisoning. If we were interested in quantifying the burden of food poisoning in our 100 patrons, we could say the number of incident cases was 10 per evening at the restaurant. Also suppose, tragically, 2 people out of the 10 cases die from their food poisoning but the rest survive and completely recover. Both die within the first month, so that seems like a good timeframe for reference. We can say that 1-month survival of this terrible food poisoning is 8 in 10 or 80%. These numbers give us a good snapshot of the situation, but there is still something missing.\nIf you’ve ever had food poisoning, then you know that there is much more to the experience than these number describe. The illness can bring terrible stomach pains, vomiting, and other messy symptoms. The suffering from it can be intense and may last a few hours to weeks. Suppose you were one of the people who got food poisoning at this restaurant and then recovered. If someone described the impact of the disease as, “10 people got sick that night and 2 died”, you would think this does not describe the experience well for you or anyone else who got sick. To better capture the experience of the illness, the suffering it causes seems like an important part to describe.\nTo measure this experience, researchers have developed a tool called the “Disability-Adjusted Life Year”, or DALY for short. DALYs quantify suffering along two dimensions, the loss of wellbeing for patients living with a disease and the years of life lost for patients who die from it. Let’s take these one at a time.\nTo measure loss of wellbeing (technical term: the years lived with disability), researchers have to first decide what is normal wellbeing. They can make the assumption that, roughly speaking, people who never develop the disease and have similar health to the patient before the patient is diagnosed can act as a good point of reference for the health the patient would have experienced if the patient never developed the disease in the first place. Then they measure the actual health of the patient with the disease against the healthy people. There are many conditional assumptions in this concept, which makes it confusing. We can simplify it to say that researchers measure the difference in health between people without the disease and a person with the disease. How do you put a number on the health state of a person with a disease? The most common way researchers do this is they ask a bunch of people to rate different states of health relative to full health and then they decide on an average “value” of the diseased health state relative to healthy state. Then researchers subtract the “value” of the health state of a person with the disease from the “value” of full health over time to calculate an estimate of lost wellbeing.\nMeasuring years of life lost is more straightforward. Researchers can say that the life expectancy (average lifespan) for a bunch of people similar to the patient but who do not have the disease is the life expectancy of the patient of the patient if they never developed the disease. Researchers then take the life expectancy and subtract the age at which a patient dies, and they have an estimate of the years of life that were lost due to the disease.\nFigure 6 shows an example of the health states for three different types of people in our example. The top line is a healthy person who never develops food poisoning, the middle line is a perspon who develops food poisoning and then recovers, and the bottom line is a person who develops food poisoning and eventually dies. The difference in area between the top line and each of the other two lines are the loss of wellbeing for those patients (pink colored areas on second row of figure 6). The difference between the life expectancy (blue line on the graph) and the age at which the bottom line reaches a health state of zero is the years of life lost.\n\nFig. 6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow that we know how to measure DALYs, the next question is, “what are they used for?” DALYs allow for two important comparisons. The first is that they allow researchers to measure the consequences of a disease relative to where it occurs in a lifespan. The years of life lost for a person who is 100 years old and develops cancer compared to a person who is 5 years old are very different and DALYs give researchers a way to put a number on that difference. The second thing DALYs allow is the comparison of very different diseases. With them, we can think about questions such as, “how do the health consequences of a common cold compare to cancer”. Colds are very, very common, somewhat uncomfortable, and most of the time get better on their own. Cancer is the exact opposite. If we wanted to, we could calculate DALYs for each, which could allow us to compare the health consequences and burden of these two very difference diseases.\nIt is important to point out that DALYs are very rough estimates of the experience of having a disease. They give researchers a way to put a number to this experience, so they can track it over time and compare it across other diseases. What DALYs are not is an actual statement about what living with cancer or any other disease is really like. We could write a 10,000-page novel try to convey the true experience of just one child with cancer, and still not capture the truth. The experience of cancer is a profound trial to endure and no researcher wants to cheapen that. DALYs are imperfect tools used by limited humans to try to measure an experience that defies description for the sake of helping those experiencing it. Furthermore, there are well known critiques of DALYs and what they do and do not measure that are worth exploring to better both what specifically they are measuring and what are their limits.\nLet us now turn to measuring the burden childhood cancer using DALYs. A group of researchers recently published the most comprehensive estimate of DALYs due to childhood cancer to date.\\(^3\\) The most important finding from this paper corrects a common wrong belief that goes “because childhood cancer is rare, its total burden is low compared to other diseases.” The researchers reported that childhood cancer caused 11.5 million DALYs in 2017. While this single number may be difficult to interpret, compared to the other most common childhood diseases, childhood cancer ranked 9th globally. In upper-middle income countries, it was the 3rd largest cause of DALYs, falling only behind congenital birth defects and lower respiratory infections. See figure 7 to see the comparisons. They also compared DALYs to adult cancers and found that, globally, childhood cancer ranks 6th among all types of cancers (see figure 8) and in low and low-middle income countries, it is ranked 1st! Although childhood cancer is less common compared to other disease, the researchers demonstrated it is still responsible for a large amount of disease burden in the world.\n\n\nFig. 7 - DALYs due to pediatric diseases\n\n\n\n\n\n\n\n\n\n\n\nFig. 8 - DALYs due to cancers\n\n\n\n\n\n\n\n\n\nThey also looked at groups of countries in the world are most responsible for the greatest number of DALYs. The researches grouped countries by their “sustainable development index” (SDI), a number that gives a rough assessment of a country’s level of development. Consistent with the findings we’ve discussed so far, they found that lower SDI countries are responsible for the large majority of DALYs. Figure 9 shows total number of DALYs by SDI grou. In this figure, we again see the trend that the regions in the world that are least prosperous are the ones that suffer the highest burden of disease.\n\n\nFig. 9 - Animated\n\n\n\n\n\n\n\n\n\n\n\nClick for static image\n\n\n\n\n\n\n\n\n\n\n\n\n\nLooking at GPO through the lens of DALYs not only helps us better understand the magnitude of the disease, but also informs discussion about how to set health priorities. The sad reality is that we live in a world of limited time, money, and resources. Because of these limitations, health officials, politicians, and other people who set healthcare policies in a country, have to make hard decisions about where to focus their efforts and resources. If decision makers do not have comprehensive information about the burden of diseases in their country, then they will not be able to make the best decision that will benefit the most people. As we have seen, comparisons of DALYs across diseases are a necessary measure to inform compassionate decisions.\nA good example of this situation would be a health planner in an upper-middle income country that may need to decide whether to buy more cancer medicines or tuberculosis medicines for pediatric patients. According to the DALY estimates, for the average upper-middle income country pediatric cancer is the 3rd largest source of DALYs and tuberculosis is the 59th. This seems like an important piece of information for the health planner to know when making the decision. I’m certainly not saying that this is the only component to the decision. She would also need to consider costs, implementation feasibility, social context and a host of other factors that are not mentioned here. But I am saying that making the best decision that does the greatest amount of good is extremely difficult and can only be done when comprehensive information about the burden of the disease is available.\nWe spent time developing an intuition for what DALYs are because they are so important for understanding this edge of the GPO puzzle. Although more rare than other disease of childhood, it is responsible for enormous suffering due to how lethal it is for young patients and how debilitating it can be for those who survive. DALYs help to describe what should be the case - that in 2017 there should have been 11.5 million healthy life years lived that were not. Understanding what the world would look like if every child with cancer receives optimal care helps put the tragedy of GPO in a new perspective that better conveys the magnitude of the problem compared to only looking at the total number of new cases or deaths per year. Lastly, DALYs are helpful to understand because they play an important role in health policy and allow people who have to make really hard decisions to be well informed. Anyone interested in addressing GPO through advocacy and health policy need to be comfortable with these concepts so they can speak the same language as the decision makers they seek to impact.\n\n\n\nSummary of Edge #1 – The global burden of childhood cancer\nWe’ve come a long way in this discussion. We saw that LMICs suffer the most from pediatric cancer, where 90% of children with cancer live. We established that GPO is much more common than was previously thought, with around 400,000 new cases of pediatric cancer per year. We also identified the ongoing catastrophe of non-diagnosis, which results in around 43% of kids with cancer, or 175,000 children, never even being diagnosed. If things do not change, in the next 15 years, 3 million children with cancer will never be diagnosed nor have a chance at cure - the equivalent of 10 school buses of children disappearing every day for 15 years. We also saw that the average 5-year survival around the world is about 37%, but ranges between 8% and 83% with the vast majority of low survival rates in LMICs. The picture only worsens when we add patients who are not diagnosed and the teenage AYA patients. Measuring DALYs due to cancer demonstrated that GPO sits among the top sources of ill health in the world, ranking number 9 globally for all pediatric diseases, number in 3 in middle-income countries, and number 6 globally when compared adult cancers. See table 1 for a summary of these numbers. Thanks to the hard work of the people in the GPO community, we now have a clear picture of the magnitude of the burden of childhood cancer. Fitting these pieces together has shown the world that the suffering due to GPO is enormous and needs to be addressed now.\n\n\n\n\n\n\n\n\n\n\nTable 1. Summary statistics for the global burden of childhood cancer\n\n\n\nStatistic\nValue\n\n\n\n\nTotal incident cases of childhood cancer per year\n400,000\n\n\nPercent of kids with cancer that live in low- and middle-income countries\n90%\n\n\nNumber of non-diagnosed cases of childhood cancer per year\n175,000\n\n\nNon-diagnosed cases as a percent of total childhod cancer cases\n43%\n\n\nNumber of children who will not be diagnosed with cancer between 2015-2030\n3,000,000\n\n\nGlobal average 5-year survival for childhood cancer (not including undiagnosed)\n37%\n\n\nGlobal rank of childhood cancer when compared to other sources of pediatric DALYs\n9th\n\n\nGlobal Rank of childhood cance when compared to other sources of cancer-related DALYs\n6th\n\n\n\n\n\n\n\nThis exploration of the burden of disease is the first edge in the childhood cancer puzzle. Now that we have an increasingly clear picture of how much cancer there is and the suffering it causes, the next question is what to do about it? It is natural to feel saddened and overwhelmed by these numbers, and it may seem as though the problem is too big to be fixed, but I have good news: for the last three decades, the GPO community has been demonstrating that we can improve care, and we now know the right intervention can significantly improve survival in a short period of time. Answers already exist, it’s just a matter of putting the puzzle pieces together. How to improve outcomes is the second edge to the puzzle and will be the topic of the next visual essay.\n\nReferences\n\nWard ZJ, Yeh JM, Bhakta N, Frazier AL, Atun R. Estimating the total incidence of global childhood cancer: a simulation-based analysis. The Lancet Oncology. 2019;20(4):483-493.\nWard ZJ, Yeh JM, Bhakta N, Frazier AL, Girardi F, Atun R. Global childhood cancer survival estimates and priority-setting: a simulation-based analysis. The Lancet Oncology. 2019.\nForce LM, Abdollahpour I, Advani SM, et al. The global burden of childhood and adolescent cancer in 2017: an analysis of the Global Burden of Disease Study 2017. The Lancet Oncology. 2019;20(9):1211-1225.\n\n\nSubscribe to my podcast about global PHO at GHCCpod.com"
  }
]