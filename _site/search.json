[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nGratitude reduces the complexity of modern life to make it more meaningful\n\n\n6 min\n\n\n\nMusings & Meditations\n\n\n\n\n\n\n\nJan 29, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Path to Useful and Trustworthy Clinical Prediction Models\n\n\n8 min\n\n\n\nClinical Prediction Models\n\n\n\n\n\n\n\nJan 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExternal Controls in Oncology Clinical Trials\n\n\n21 min\n\n\n\nCausation & Clinical Trials\n\n\n\n\n\n\n\nJan 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGratitude reduces the complexity of modern life to make it more meaningful\n\n\n6 min\n\n\n\nMusings & Meditations\n\n\n\n\n\n\n\nJan 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Epistemic Arc - a conceptual map about how to learn from data in the presence of uncertainty\n\n\n13 min\n\n\n\nStatistics & Heuristics\n\n\n\n\n\n\n\nJun 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPediatric cancer treatment abandonment: a tragic but preventable event\n\n\n5 min\n\n\n\nGlobal PHO\n\n\n\n\n\n\n\nApr 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHow to measure anything in Global Health: 5. Calibration techniques\n\n\n3 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nApr 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHow to measure anything in Global Health: 4. Calibrate your judgment\n\n\n3 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nApr 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHow to measure anything in Global Health: 3. The Definition of Risk\n\n\n3 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nApr 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHow to measure anything in Global Health: 2. The universal approach to measurement\n\n\n3 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nApr 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHow to measure anything in Global Health: 1. Foundations of Measurement\n\n\n4 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nApr 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nDistributed intelligence is the key to success for high-performing health systems in Global Health\n\n\n3 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nApr 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic beasts and how to slay them - Part 1\n\n\n4 min\n\n\n\nConfident Uncertainty\n\n\n\n\n\n\n\nMar 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the two types of uncertainty will help you live more confidently in a stochastic world\n\n\n4 min\n\n\n\nConfident Uncertainty\n\n\n\n\n\n\n\nMar 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nData science is a saddle for the epistemic rodeo: healthcare as a complex system\n\n\n4 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nMar 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nLet the data flow: how to move information to the people who need it in a Global Health organization\n\n\n4 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nMar 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nThree essential quality control practices to improve the reliability of Data Science in Global Health\n\n\n3 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nMar 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nAristotle’s sage advice to improve Data Science in Global Health\n\n\n3 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nMar 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHow to walk the unsexy path toward sophisticated Data Science in Global Health: Data Management\n\n\n4 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nMar 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nData Science has a garbage problem\n\n\n2 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nMar 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n10 ways Global Health organizations can use routine data to improve patient care\n\n\n2 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nMar 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nA simple method to monitor a catchment area and improve disease diagnosis rates\n\n\n3 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nMar 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nThree Steps To Improve How You Measure Patient Outcomes\n\n\n5 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nMar 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nThree Reasons Global Health needs Data Science\n\n\n4 min\n\n\n\nData Science + Global Health\n\n\n\n\n\n\n\nMar 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nThe single best way to cure more childhood cancers\n\n\n2 min\n\n\n\nGlobal PHO\n\n\n\n\n\n\n\nMar 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nS-Values - Part 1\n\n\n11 min\n\n\n\nStatistics & Heuristics\n\n\n\n\n\n\n\nNov 20, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Winner’s Curse\n\n\n9 min\n\n\n\nStatistics & Heuristics\n\n\n\n\n\n\n\nDec 4, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Global Childhood Cancer Puzzle - Edge 1.3\n\n\n28 min\n\n\n\nGlobal PHO\n\n\n\n\n\n\n\nNov 27, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Global Childhood Cancer Puzzle - Edge 1.2\n\n\n15 min\n\n\n\nGlobal PHO\n\n\n\n\n\n\n\nNov 27, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Global Childhood Cancer Puzzle - Edge 1.1\n\n\n20 min\n\n\n\nGlobal PHO\n\n\n\n\n\n\n\nNov 27, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/about/index.html",
    "href": "docs/about/index.html",
    "title": "The Doctor's Dialectic",
    "section": "",
    "text": "Twitter\n  \n  \n    \n     Github\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email"
  },
  {
    "objectID": "docs/about/index.html#welcome-to-the-doctors-dialectic",
    "href": "docs/about/index.html#welcome-to-the-doctors-dialectic",
    "title": "The Doctor's Dialectic",
    "section": "Welcome to The Doctor’s Dialectic",
    "text": "Welcome to The Doctor’s Dialectic\nHi, I’m Dr. Mark Zobeck. I’m a physician who cares for children with cancers and blood disorders around the world. When not caring for patients, I also conduct research. My primary focus is creating clinical prediction models to improve treatment outcomes for pediatric cancers. I also use data science to improve the delivery of medical care in low- and middle-income countries. I love writing about and teaching statistics. I confess that I am biased toward the Bayesian. For better or worse, I’m also given to musing about the metaphysical and epistemological foundations of science. You’ll find some weird fusion of these things on this blog. Welcome!"
  },
  {
    "objectID": "docs/about/index.html#why-the-theme-of-the-dialectic",
    "href": "docs/about/index.html#why-the-theme-of-the-dialectic",
    "title": "The Doctor's Dialectic",
    "section": "Why the theme of the dialectic?",
    "text": "Why the theme of the dialectic?\nThere are things that we can only learn by slowly turning opposing viewpoints over and over in our heads. Truth is found in the tension between ideas. This practice is harder to find as the world becomes increasingly fast-paced and polarized. As generative algorithms produce exponential amounts of entertaining refuse, the patient exploration of ideas is being swallowed in noise\nThis blog is my attempt to engage in this type of discussion with the reader and with myself."
  },
  {
    "objectID": "docs/about/index.html#obligatory-disclaimers",
    "href": "docs/about/index.html#obligatory-disclaimers",
    "title": "The Doctor's Dialectic",
    "section": "Obligatory disclaimers:",
    "text": "Obligatory disclaimers:\n\nNothing you read here constitutes medical advice. For medical questions, please consult your doctor.\nAll views expressed are my own and do not represent the views of my employer or any organizations with which I am affiliated."
  },
  {
    "objectID": "docs/epistemic_limits/menagerie/absolute_limits/halting_problem/index.html",
    "href": "docs/epistemic_limits/menagerie/absolute_limits/halting_problem/index.html",
    "title": "The Halting Problem",
    "section": "",
    "text": "The Halting Problem is a first-order absolute epistemic limit. It says that it is impossible for there to exist a computer program that identifies whether any possible computer program and its input will halt as opposed to run forever. This impossibility is shown through a contradiction that results from running the program on itself. If the program says it halts, then it doesn’t, and if it says it doesn’t halt, then it does. Therefore, such a program can’t exist.\nThe halting problem demonstrates an absolute limit on an our ability to know the results of certain computational tasks. The only way to know if any possible program will halt is to run it and see. For many algorithms that do not halt during the time we observe them, we will never know if they do or do not halt. To say for sure, we would need to run them for an infinite amount of time, which is time we do not have. Therefore, we have no way of deciding which programs in the space of all possible programs will hault and which will not."
  },
  {
    "objectID": "docs/epistemic_limits/menagerie/absolute_limits/halting_problem/index.html#summary",
    "href": "docs/epistemic_limits/menagerie/absolute_limits/halting_problem/index.html#summary",
    "title": "The Halting Problem",
    "section": "",
    "text": "The Halting Problem is a first-order absolute epistemic limit. It says that it is impossible for there to exist a computer program that identifies whether any possible computer program and its input will halt as opposed to run forever. This impossibility is shown through a contradiction that results from running the program on itself. If the program says it halts, then it doesn’t, and if it says it doesn’t halt, then it does. Therefore, such a program can’t exist.\nThe halting problem demonstrates an absolute limit on an our ability to know the results of certain computational tasks. The only way to know if any possible program will halt is to run it and see. For many algorithms that do not halt during the time we observe them, we will never know if they do or do not halt. To say for sure, we would need to run them for an infinite amount of time, which is time we do not have. Therefore, we have no way of deciding which programs in the space of all possible programs will hault and which will not."
  },
  {
    "objectID": "docs/epistemic_limits/menagerie/absolute_limits/halting_problem/index.html#introduction",
    "href": "docs/epistemic_limits/menagerie/absolute_limits/halting_problem/index.html#introduction",
    "title": "The Halting Problem",
    "section": "Introduction",
    "text": "Introduction\nImagine a busy day at your desk job. You have dozens of applications running simultaneously on your computer. Suddenly, it begins to slow down, and one application appears to be stuck in an infinite loop, consuming resources and degrading the performance of the system. Will the application eventually resolve this issue and return to normal? Or will it remain in this state indefinitely? This somewhat mundane, somewhat frustrating scenario illustrates the essence of the halting problem: given a computer program and an input, can we determine if the program will eventually halt or continue running forever? The halting problem demonstrates that the answer is no we can’t."
  },
  {
    "objectID": "docs/epistemic_limits/menagerie/absolute_limits/halting_problem/index.html#explanation",
    "href": "docs/epistemic_limits/menagerie/absolute_limits/halting_problem/index.html#explanation",
    "title": "The Halting Problem",
    "section": "Explanation",
    "text": "Explanation\nThe halting problem asserts that no program can be devised to determine if a program and its input will halt or run forever for all possible pairs of programs and inputs.\nWe will demonstrate this with a proof by contradiction. The following explanation borrows heavily from Noson Yanofsky’s excellent book, The Outer Limits of Reason. If epistemic limits interest you, I highly recommend it.\nThe programs we will be examining are everyday computer programs. For example, we could code a program to take a number x and keep adding two to it until the number is greater than or equal to 10, then output the result. Let’s call this program two_adder. So for x = 4, the result would be 10 (4 + 2 \\(\\rightarrow\\) 6 + 2\\(\\rightarrow\\) 8 + 2 \\(\\rightarrow\\) 10). For x = 7, the output would be 11. For x = 15, the output would be 15. This is a nice simple program with a range of input values. We figured out if it halts just by running it in our heads. It’s pretty clear it stops.\nSuppose we can construct a program that can determine if any such program and its inputs halt. We’ll call this program halt_decider. We could feed in our program two_adder and a value for x, say 7, and halt_decider will output “two_adder halts” if it halts and “two_adder doesn’t halt” otherwise. Figure 1 below schematically depicts such a program.\n\n\n\n\nFigure 1. Depiction of the halt_decider program. It takes a program and input and applies an algorithm to decide if the program with the input will halt.\n\n\n\nNow suppose we can make another program that we’ll call halt_contradictor. It will do something weird. It will take a program and an input value then call halt_decider to see if it halts. If halt_decider indicates the program and input halts, then we’ll have halt_contradictor go into an infinite loop printing “{program name} halts.” If halt_decider indicates that the program does not halt, then halt_contradictor will print one time “{program name} doesn’t halt.” Basically, halt_contradictor takes the output of halt_decider and does the opposite. If the input program halts, halt_contradictor loops forever. If the input program doesn’t halt, halt_contradictor halts. Figure 2 should help clarify this confusing setup.\n\n\n\n\nFigure 2. Depiction of the halt_contradictor program. tl;dr: halt_contradictor does the opposite of what halt_decider says. It is built on top of halt_decider and adds the extra step of taking the output of halt_decider and either halts if halt_decider indicates the program doesn’t halt or enters an infinite loop if halt_decider indicates the program halts.\n\n\n\nLet’s feed in two_adder with an input value of 7 to halt_contradictor. We said earlier it will halt, so halt_decider will indicate “two_adder halts”. Halt_contradictor will take that output and then print “two_adder halts” “two_adder halts” “two_adder halts”….. in an infinite loop.\nOne final element we need is to number all of our programs. It doesn’t matter how we number them, but each one needs to have a unique reference number. For program two_adder we could assign the number 123, for halt_contradictor we could assign the number 9, etc. Computers have no problem assigning reference keys to represent programs that run in other programs.\nNow we will break the program. Since halt_contradictor is a program, let’s feed it into itself! Let’s say that halt_contradictor is the program that is going into halt_contradictor along with the numeric encoding for halt_contradictor that we said was 9. What happens when we do this? If halt_decider says that halt_contradictor halts, it says “halt_contradictor halts.” Halt_contradictor will take that output and do the opposite. It will print “halt_contradictor halts” in an infinite loop. If halt_decider says “halt_contradictor doesn’t halt”, then halt_contradictor will print out “halt_contradictor doesn’t halt” once and then…halt."
  },
  {
    "objectID": "docs/epistemic_limits/menagerie/absolute_limits/halting_problem/index.html#contradiction",
    "href": "docs/epistemic_limits/menagerie/absolute_limits/halting_problem/index.html#contradiction",
    "title": "The Halting Problem",
    "section": "Contradiction",
    "text": "Contradiction\nDo you see the contradictions?\n\nIf halt_decider says that halt_contradictor with the halt_contradictor program and input 9 (numeric encoding of itself) halts, then we saw that that halt_contradictor with the halt_contradictor program and input 9 will not halt!\nSimilarly, if halt_decider says that halt_contradictor with these inputs doesn’t halt, then halt_contradictor with those inputs will halt!\n\nThese contradictions arose because we assumed that the halt_decider is a real program. If our assumption leads to a contradiction, then our assumption is false. Therefore, halt_decider cannot exist, and we can conclude there is no program that can solve the halting problem."
  },
  {
    "objectID": "docs/epistemic_limits/menagerie/absolute_limits/halting_problem/index.html#importance",
    "href": "docs/epistemic_limits/menagerie/absolute_limits/halting_problem/index.html#importance",
    "title": "The Halting Problem",
    "section": "Importance",
    "text": "Importance\nThis may seem like a contrived example of self-reference, but its implications are enormous. After the Halting Problem was described, researchers discovered many similar problems. The Halting Problem established the existence of a whole class of problems that are undecidable. Undecidable problems have a clear “yes” or “no” answer, such as “will a program halt?”, but we will never know the answer because no computational method or rational effort can provide the answer. What’s more, if we could solve the Halting Problem, we could solve other undecidable problems such as Goldbach’s Conjecture or compute measures of Kolmogorov complexity, which would representing important advances in the fields of mathematics and probability theory.\nLastly, devastatingly, there are an uncountably infinite number of undecidable problems. There are an uncountably infinite number of questions that no human nor artificial general intelligence can answer. As rational agents navigate the vast landscape of truth, there will forever be a horizon of questions we can ask but have no method of answering. The answers will be out there, beyond us, forever."
  },
  {
    "objectID": "docs/epistemic_limits/menagerie/absolute_limits/halting_problem/index.html#references",
    "href": "docs/epistemic_limits/menagerie/absolute_limits/halting_problem/index.html#references",
    "title": "The Halting Problem",
    "section": "References",
    "text": "References\nThe Outer Limits of Reason\nBrilliant.org Halting Problem\nKhan Academy Halting Problem\nA good YouTube video\nAnother good YouTube video"
  },
  {
    "objectID": "docs/epistemic_limits/information_dynamics/index.html",
    "href": "docs/epistemic_limits/information_dynamics/index.html",
    "title": "Information Dynamics",
    "section": "",
    "text": "Dynamic information is coming"
  },
  {
    "objectID": "docs/epistemic_limits/index.html",
    "href": "docs/epistemic_limits/index.html",
    "title": "Epistemic Limits",
    "section": "",
    "text": "I’ve started a weird collection.\nI’m collecting things that are intrinsically unknowable, facts that are impossible to know with certainty.\nWhat’s the purpose of this collection?\nWell, let me ask you: Have you ever been stumped by a question from a preschool kid? Children are curious creatures by nature, and it seems to be a common pastime of all kids to annoy their parents with what I affectionately call “the infinite ‘why’ regress.”\n“Eat your peas.”\n“Why?” “Because they’re good for you.”\n“Why?” “Because they contain nutrients that your body needs to be healthy.”\n“Why?” “Because, kid, homeostasis depends on proper enzymatic function that can only be achieved through the presence of micronutrients acting as cofactors to catalyze important metabolic reactions.”\n“Why?” “Because the anti-entropic processes that sustain life require electrochemical and physical methods to manage energy gradients by exploiting the thermodynamic properties of different elements that issue from their unique atomic and subatomic constituents.”\n“Why?” “Because God made it that way now eat your peas!”\nThis common experience of tired parents demonstrates that our knowledge is limited. Sooner or later, in the infinite why regress, we come to a question we cannot answer. When we subject our reasoning to unyielding interrogation, we always arrive at the point where we must say, “I don’t know.”\nAs a physician, I am confronted with the limits of our certainty every day. I am a pediatric oncologist. I provide clinical care to children with cancer. When a child is diagnosed with a life-threatening disease like cancer, one of the most common questions a parent asks is, “Will my child survive?” Unfortunately, the honest answer has to be, “We don’t know.” We have gotten very good at curing childhood cancer. Over 80% of all children with cancer in the United States survive, but we cannot cure everyone. I do not know the child’s future. I only have statistics and a promise that we will do everything in our power to cure their disease.\nThese examples are practical demonstrations of the limits of our knowledge. Preschoolers show us that our explanatory abilities bottom out after enough pressure. Clinical medicine demonstrates that there are things we would like to know that are beyond our powers to predict. Everywhere in life, we run into questions who’s answer is “I don’t know.” Uncertainty seems to be a fundamental feature of human existence.\nIn fact, advances in science and mathematics in the last century demonstrate that uncertainty is a fundamental feature of the existence of any creature that possesses knowledge. We are subject to fundamental epistemic limits, boundaries of knowledge that we cannot cross. These limits introduce uncertainty into any claim about knowledge. This uncertainty has radical consequences that shape how we can know anything at all.\nSo, I’m collecting things that are intrinsically unknowable. These unknowables show us our epistemic limits. These limits are ordinarily very difficult to see, but the unknowables highlight them like dust giving concrete form to a ray of light. By making them more substantial, we’ll see that our epistemic limits have a profound impact on our lives. They impact our understanding of epistemology by limiting what knowledge we can possess. They impact our understanding of phenomenology by illuminating the role of the subjective perspective in experience. They impact our science by prescribing empirical methods that deliver reliable information about reality. They even impact our metaphysics by illuminating what beliefs about reality can and cannot be held with certainty.\nThis is a work in progress. My plan is to first grow my collection of unknowables in phase 1 and then draw out the implications of the epistemic limits they represent in phase 2.\nThis is a side project. Progress may be slow.\nMost importantly, this is a living and breathing set of ideas. They will all grow, mature, and maybe even rebel against their former selves. If things are different in a year when you come back here, don’t be surprised. That is the nature of knowledge when certainty is impossible."
  },
  {
    "objectID": "docs/posts/2025-01-25-modeling_philosophy/index.html",
    "href": "docs/posts/2025-01-25-modeling_philosophy/index.html",
    "title": "The Path to Useful and Trustworthy Clinical Prediction Models",
    "section": "",
    "text": "Introduction\nClinical prediction modeling, using data to predict patients’ clinical outcomes to improve medical decision-making, can be incredibly powerful if done well. Modern medicine is awash in unused data. Information that we can use to improve the lives of patients is everywhere if we just knew how to distill it from the muck and mire of the everyday chaos of clinical care. With it, we could:\n\nMore efficiently diagnose diseases without patients suffering through tortuous journeys in the medical system, bouncing from doctor to doctor with no answers.\nMore accurately predict the prognosis of diseases, providing information about what the future will hold.\nMake optimal medical decisions, enabling patients and physicians to wrestle with the risks and benefits of different treatment options to make the best decisions.\nConfirm what we know to clarify what we don’t, accelerating progress in basic and translational science and identifying which clinical trials are actually worth doing.\n\nAll of these benefits are within our reach, if only we could harness the ocean of data generated by the medical system every day to produce trustworthy and reliable clinical prediction models.\n\n\nUnrealized potential\nClassical methods of statistical prediction modeling for clinical applications have existed for decades, such as multivariable linear regression, logistic regression, ordinal regression, survival modeling, etc. There is a vast literature covering all of the relevant statistical and clinical aspects of developing such models, from sample size calculations and model performance to decision theory and implementation. The science of prediction modeling is very well developed.\nYet, these methods have not realized their potential. They have been used well for some applications and terribly for many others. Most models are optimized for publication and not for clinical use. They are developed to advance careers and not improve patient care. Even the good ones are rarely implemented into clinical practice. Running code in statistics software and writing a paper is vastly easier than changing clinical care. Despite a well-developed science of prediction modeling, the health system carries on largely ignorant of it.\n\n\nAI has made the situation worse\nThe rise of machine learning and AI has only worsened the situation. AI has demonstrated blockbuster results in certain use cases, such as for chatbots and applications in radiology and pathology. Yet, these do not translate well to clinical prediction tasks. AI is ravenous for data because it must train an incredible number of parameters. AI also craves stability in the system it is learning to reproduce. If the behavior of the system changes, the titanic algorithms struggles to adapt, like a massive ship that turns in a slow, wide arc. Clinical medicine is the opposite of this. Data are relatively sparse, and the underlying system that generates it is built on quicksand. Structural change is the rule rather than the exception. Moreover, the science of describing the reliability and trustworthiness of AI models is not well developed. The discipline is still grappling with key questions, such as causal inference methods and how to represent uncertainty when data are limited. AI methods that have produced headline success in some applications do not translate to the complex, dynamic, and messy world of clinical practice.\n\n\nUseful and trustworthy clinical prediction models\nTo realize clinical prediction modeling’s potential, we need a method for producing useful predictions that patients and providers can trust.\nUseful predictions:\n\nAnswer a specific and valuable question. Sometimes, prediction models produce answers to different questions than the user has in mind. Other times, models are developed that answer a question that is not valuable because the information makes no difference either in terms of actions that people might take or psychological benefit from the knowledge it gives.\nSupport decisions. People can use the predictions to decide what is the best next step. This process involves more than the predictions, such as utilities of outcomes and the context of actions. The output of the model should be readily usable by someone equipped with this information.\nCan be implemented by the health system. If the model demands too many resources, for example requiring data, IT infrastructure, or specialized expert knowledge that is unavailable, then it cannot practically be implemented. Models must be designed with constraints in mind.\n\nTrustworthy predictions:\n\nProvide the right kind of answers. Predictions can be in the form of discrete categories or probabilities. The question it answers and the decisions it supports determine the best type of answer. The wrong type of answer can produce worse decisions.\nAre accurate. They generally answer the modeling question correctly. This can be demonstrated by measures of overall model performance, discrimination (how well the model separates groups) and calibration (whether the value or probability of an outcome matches the observed values/probabilities).\nHave quantifiable uncertainty. One can say that both the chance of getting heads on a coin toss and the chance of team winning a match for a sport you have never heard of and know nothing about is 50%, but you would be very certain about that statement for the coin and very uncertain for the sports team. That certainty matters as you make a decision, monitor results, and consider changing your mind about the right course of action. It also matters for whether or not the predictions can be improved my more work on the model.\nAre not overfit to the data. Models can be too good at predicting the dataset that is used to train it. They will fall apart when making predictions with new data. Methods for internal validation during model development must be used to demonstrate that the performance remains acceptable when simulating its use on new datasets.\nPerform acceptably over time. The dynamics of the health system that produce outcomes can change slowly or suddenly over time. Processes must be in place to monitor the model and respond when their performance degrades.\nPerform acceptably in new situations. Model performance can vary tremendously when used in new situations, such as in different hospitals or different types of patients. The predictions must be shown to be trustworthy in new settings.\n\nThis is the path to developing valuable prediction modeling that help patients. These are the minimal criteria. Yet classical models and ML/AI can fail spectacularly at these points.\nThe path for ML/AI is more difficult than for the classical methods because implementation in the health system, providing the right kinds of answers, quantifying uncertainty, and not overfitting given the amount of available data are all much greater challenges. Proponents of AI/ML argue that their models are more accurate. Even granting this point, which is debatable, the consequences of doing these things poorly tend to negate any benefit from improved accuracy.\nAchieving all of these properties is difficult regardless of the approach. The time, effort, and money required to do this well generally outweighs the benefit that an academic gains from publishing an analysis and moving on to the next thing. The literature is filled with descriptions of models that are accurate when predicting from the training data but are otherwise useless and not worthy of anyone’s trust.\n\n\nTo the future!\nNone of these are my original thoughts. There are many, meany people who want to do this well and have rigorously worked through these ideas. These points have been articulated and systematized for the wider scientific community (the TRIPOD+AI statement and its associated references are a good starting place). This post is my process of synthesizing these essential concepts for my own practice as both a physician and data scientist.\nThese points should guide the integration of prediction models into clinical practice, but they probably won’t. Or at least they will only partially. The publishing incentives to stop short of anything practical are too great. Hype is much more effective at selling to healthcare executives than a careful evaluation of how well a model performs. This is boring nerd stuff.\nYet, useful and trustworthy modeling can win because reality is unrelenting. Hype will come and go. Expensive AI products will be purchased and found to be useless. Cycles of boom and bust will roll on. Amidst the noise, the future will remain open. We won’t stop craving reliable predictions while we continuously proceed into the unknown. Useful and trustworthy models will demonstrate their virtues as time rolls on, and health systems will become better at recognizing their value and more capable or supporting their implementation. The opportunities are tremendous if only we can avoid the allure of short term payoffs and do the hard work of demonstrating that a model truly improves patient care."
  },
  {
    "objectID": "docs/posts/2022-04-03-htma-1-foundations/index.en.html",
    "href": "docs/posts/2022-04-03-htma-1-foundations/index.en.html",
    "title": "How to measure anything in Global Health: 1. Foundations of Measurement",
    "section": "",
    "text": "Can you measure the effect of adding a dietitian to your clinical service?\nCan you quantify how lonely your patients feel while undergoing treatment?\nCan you estimate the years of life saved from a quality improvement project?\nThe answer to all of these things is yes, it is possible to measure anything in Global Health, although the answers may not be what you expect.\nThis is good news because measurement can help leaders make decisions that efficiently improve the care they provide to patients.\nIn this series of posts, I’m going to work through the techniques to measure anything in global health. These ideas come from the phenomenal book How to Measure Anything (HTMA)by Douglas Hubbard. Buy the book if you are interested in this topic. It’s awesome.\nTo measure anything in Global Health, we need to understand:\n\nThe concept of measurement\nThe objective of measurement\nThe methods of measurement\n\nIf you can understand these three foundations, you are well on your way to measuring anything in global health.\n\nThe Concept of Measurement:\n\n“A quantitatively expressed reduction of uncertainty based on one or more observations.”\n\nMeasurement is not about being right; it is about being less wrong. With the view of making decisions to improve future outcomes, any reduction in the uncertainty of how the future may look will improve the value of your decision.\nHow many mechanical ventilators will the critical care unit require in the next COVID wave? If before measurement you think its somewhere between 5 and 50 and after the measurement you narrow it to 20-30, that will give you a much better chance at providing all patients the resources they need without breaking the bank.\n\n\nThe Objective of Measurement\nMany things seem impossible to measure because they seem “intangible”. For instance, think about how to measure loneliness in patients. That seems like a hard thing to measure. HTMA gives some helpful advice to clarify how to measure the intangible:\n\n\nIf it matters at all, it is detectable/observable\nIf it is detectable, it can be detected as an amount or range of possible amounts\nIf it can be detected as a range of possible amounts, it can be measured\n\n\nLoneliness is an emotion that can be expressed through words and actions. We can develop methods to capture these expressions of loneliness to understand the patient’s internal state.\nAnother key point is that you don’t have to reinvent the wheel - there is a lot of literature about the psychometrics of loneliness where researchers have grappled with 1-3 above.\n\n\nThe Methods of Measurement\nEven if you understand the concept and objective of measurement, the act of measurement may still seem daunting. Many simple methods, such as random sampling, produce useful measurements with few resources. More complex methods can increase the quality of measurement at the cost of more effort. How complex you want to be will be determined by the importance of measurement for your decision.\nHTMA will go through methods that allow you to reliably measure…\n\n\n..with a very small random sample of a very large population\n…when many other, even unknown, variables are involved\n…the size of a mostly unseen population\n…subjective preferences and values\n…the risk of rare events\n\n\nOne simple method example: we can show there is a 93.75% chance that the median value of a population will fall between the smallest and largest values of a random sample of five observations.\nThat is incredibly useful when your uncertainty about the range of possible median values is high.\nThese three foundational concepts will set you up for success the next time you try to measure anything!"
  },
  {
    "objectID": "docs/posts/2020-11-27-gccp-1-2/index.en.html",
    "href": "docs/posts/2020-11-27-gccp-1-2/index.en.html",
    "title": "The Global Childhood Cancer Puzzle - Edge 1.2",
    "section": "",
    "text": "The following originally appeared on my podcast about global PHO - Global Health and Childhood Cancer\n\n\nThis is part two of a three part essay about the global burden of childhood cancer. If you haven’t read from the beginning, you may want to start from part 1. You can find part 3 here.\n\n\nThe probability of surviving cancer\nThe researchers mentioned in the last post did not stop at estimating incident cases and non-diagnosed cases, they also estimated the chance a child has of surviving their cancer after being diagnosed.\\(^2\\) Using methods similar to how they estimated new cases, the researchers reported that 37% of all patients with cancer survive for 5 years after diagnosis (5 years from diagnosis is chosen as a convenient time point for measurement purposes, but there is nothing special about it). Again, we could say, with some rounding, that on average around the world only 4 in 10 kids survive their cancer.\nLike the incident cases, this estimate is actually very different in different parts of the world, except variability of the survival estimates is even larger than the incidence estimates. For instance, the 5-year survival percentage in a high-income country like North America is 83% (more than 8 in 10!), but in an economically-challenged region like eastern Africa, only 8% of all patients survive (roughly 1 in 10). Looking broadly across all countries, the trend that emerges from the data is that kids in LMIC have a much, much lower chance of surviving their disease. As an example, figure 4 shows the percent of mortality by region for acute lymphoblastic leukemia, a cancer of the blood.\n\nFig. 4 - Animated\n\n\n\n\n\n\n\n\n\n\n\nClick for static image\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf these numbers were not depressing enough, there is a twist to the story. These survival estimates are only for patients who are diagnosed with cancer. Remember earlier where we said 4 in 10 patients, on average, are not even diagnosed, and that this proportion is worse in LMIC? The patients who were not diagnosed were not factored in to the survival estimates. To get the true survival, we would have to consider what happens to the patients who are not diagnosed and the patients who are diagnosed and treated. Let me give a very crude example of how we might combine these numbers. It is safe to assume that most patients who are not diagnosed will die from their disease, but to be conservative and factor in some uncertainty to the estimates, let us put the number for patients who are not diagnosed but survive at 10%. Suppose country A demonstrates a 5-year survival of 50% for disease X, but only 50% of kids who develop the disease are diagnosed. The total number who survive would be (0.5 x 0.5) + (0.1 x 0.5) = 0.30 or 30%. The specifics of the math aren’t too important, what is important is that factoring in non-diagnosis drops the survival rate by a large amount. Unfortunately, since patients in LMIC are both less likely to be diagnosed and less likely to survive, looking at the data in this way worsens the numbers of LMICs more than HICs. Figure 5 shows the new estimates when factoring non-diagnosis into the equation.\n\n\nFig. 5 - Animated\n\n\n\n\n\n\n\n\n\n\n\nClick for static image\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor comparison, here are the two static graphs side-by-side when non-diagnosis is not included (left) and when it is (right). You can see the number of who die does not increase much for North America and Europe when includiding non-diagnosis due to the simple fact that most patients are diagnosed. The numbers increase significantly for the rest of the regions, representing high rates of patients who are not diagnosed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Important technical note: It bears repeating that these calculations are very imprecise and there is unaccounted for uncertainty that makes them more difficult to combine than the above procedure suggests. I am using point estimates for the sake of clarity and ignoring the intervals of uncertainty associated with the estimates. These numbers are only for the purpose of illustrating the relationship between income category, non-diagnosis, and survival and should not be reported as precise estimates.)\nIf that wasn’t enough, there is actually a further twist to the story. The estimates for both incident cases and survival are for patients ages 0-14 years only. We should keep in mind that many 15-19 year-olds that develop cancers are considered to be “pediatric” patients that pediatric oncologists (at least in the United States, where I work) will treat. There is even a name for a unique category of patients that includes this age group called “Adolescent and Young Adults”. We won’t go into more detail of this age group because less reliable information is available (an example of problem number 2 in quantifying disease burden from earlier!), but it is important to know that these patients are not included in the above discussion, and the true number of pediatric patients is higher as a result.\nAs you can see, the GPO community has come a long way in quantifying incident cancer cases and 5-year survival, and the story the numbers tell is grim. However, there is another part of the story that needs to be told, that of measuring disease burden from the perspective of the suffering it causes and the years of life it steals from its victims.\n\nContinue reading:\nPart 3\nSubscribe to my podcast about global PHO at GHCCpod.com"
  },
  {
    "objectID": "docs/posts/2022-03-14-improve-howyoumeasure-patient-outcomes/index.en.html",
    "href": "docs/posts/2022-03-14-improve-howyoumeasure-patient-outcomes/index.en.html",
    "title": "Three Steps To Improve How You Measure Patient Outcomes",
    "section": "",
    "text": "Survival outcomes are perhaps the most important metric to measure in most Global Health treatment centers.\nHow do we know how effectively we are delivering care if we cannot measure the outcomes? If 100% survival is the target, we have to know how far we are from it to improve our accuracy.\nUnfortunately, survival outcomes are very difficult to measure accurately! As a pediatric hematologist-oncologist who supports monitoring and evaluation for treatment programs in Africa, I know this all too well. There is one primary reason why, even in clinical trials, these are so difficult to measure:"
  },
  {
    "objectID": "docs/posts/2022-03-14-improve-howyoumeasure-patient-outcomes/index.en.html#survival-outcomes-change-over-time",
    "href": "docs/posts/2022-03-14-improve-howyoumeasure-patient-outcomes/index.en.html#survival-outcomes-change-over-time",
    "title": "Three Steps To Improve How You Measure Patient Outcomes",
    "section": "Survival outcomes change over time",
    "text": "Survival outcomes change over time\nAn alive person can relapse or their disease can progress at any time. A patient may die, but the treatment program is unaware of the event. Patients may suffer financial or social hardships during treatment and stop attending clinic visits completely.\nThere are many reasons these data may not be captured well:\nInsufficient staffing to actively collect data\nNo ability to efficiently identify patient statuses that need to be updated\nInconsistent variable definitions for disease status (remission/relapse/etc.) and patient status (alive/dead/abandoned treatment/etc.)\nData storage is confusing with repeated observations on the same unit over time\nThankfully, all of these problems have sustainable solutions if you design a system that respects the nuances of time-varying data.\nLet me share three essential steps you can take to improve your data quality.\nThese are minimal recommendations for general program monitoring and evaluation. Realize if you’re doing research, you need this and more to assure accuracy."
  },
  {
    "objectID": "docs/posts/2022-03-14-improve-howyoumeasure-patient-outcomes/index.en.html#step-1-prioritize-accurate-data-collection-in-the-organization.",
    "href": "docs/posts/2022-03-14-improve-howyoumeasure-patient-outcomes/index.en.html#step-1-prioritize-accurate-data-collection-in-the-organization.",
    "title": "Three Steps To Improve How You Measure Patient Outcomes",
    "section": "Step 1: Prioritize accurate data collection in the organization.",
    "text": "Step 1: Prioritize accurate data collection in the organization.\nIt is easy to produce survival numbers (60% 5-year OS; see I just did it), but it is exquisitely difficult to produce accurate survival numbers. To do it, your entire Global Health organization has to understand the value of data management and commit to doing it well.\nThat means people with the proper tools have to be given time and support to create frameworks and workflows to record the outcomes over time. It also means the leadership has to be willing to commit enough money and resources to make this happen."
  },
  {
    "objectID": "docs/posts/2022-03-14-improve-howyoumeasure-patient-outcomes/index.en.html#step-2-organize-the-people-tools-frameworks-and-workflows-to-optimize-data-collection",
    "href": "docs/posts/2022-03-14-improve-howyoumeasure-patient-outcomes/index.en.html#step-2-organize-the-people-tools-frameworks-and-workflows-to-optimize-data-collection",
    "title": "Three Steps To Improve How You Measure Patient Outcomes",
    "section": "Step 2: Organize the people, tools, frameworks, and workflows to optimize data collection",
    "text": "Step 2: Organize the people, tools, frameworks, and workflows to optimize data collection\n\nPeople - Depending on the size of the program, the number of patients to be followed, and the frequency of follow-up, you need enough people with dedicated time to do it. Some patient statuses can be captured when they come to the clinic, some have to be called on the phone or tracked down another way. Estimate how long it takes to update 10 statuses for each scenario, apply that rate (total time/10 patients in the scenarios) to the total number of patients to be followed, and estimated the number of person-hours required per month.\nTools - Find a data recording tool that handles repeated observations on the same unit easily and that allows strict enforcement of variable options and date conventions. REDCap is a nice, free, and popular option that does this well. Spreadsheets can do it, but they are limited. Other fancier tools exist that you can explore. The key is to have a clean picture of repeated values for each patient arranged and to have control over data entry.\nFrameworks - To measure “outcomes”, create two variables: “Disease status” and “Patient status”. Define each in a few categories, and maintain a data dictionary that explicitly defines each category. Share the dictionary with all people collecting data and refer to it often. Also at a minimum have a variable for “date of change in status” to capture the event time.\nWorkflows - Create a schedule for when patients in different phases of therapy need status updates. For example, patients actively on treatment need status updates every month, within two years of the end of treatment every quarter, and yearly after that. Now figure out a way to automate it, so that you can run a report each month that shows you who needs updates. I wrote code (#rstats!) to do it for my organization. There are software tools that can do it as well. Or you can see if you teach Excel to do it, I’m sure it can be done with macros."
  },
  {
    "objectID": "docs/posts/2022-03-14-improve-howyoumeasure-patient-outcomes/index.en.html#step-3-check-the-accuracy-of-the-data-over-time",
    "href": "docs/posts/2022-03-14-improve-howyoumeasure-patient-outcomes/index.en.html#step-3-check-the-accuracy-of-the-data-over-time",
    "title": "Three Steps To Improve How You Measure Patient Outcomes",
    "section": "Step 3: Check the accuracy of the data over time",
    "text": "Step 3: Check the accuracy of the data over time\nAnother person, perhaps a medical provider or a data manager, should conduct periodic reviews of the data. Biopsy a certain percentage of the medical charts, say 5-10%, for each quarter and compare what is in the chart or what is confirmed via a patient phone call with what is recorded in the system. Categorize types of errors into classification (e.g. “relapse” should have been classified as “progressed”), date (dates recorded incorrectly), or missed event (an event happened but not recorded). Study the pattern or errors and tune the elements in Step 2 above to reduce these errors."
  },
  {
    "objectID": "docs/posts/2022-03-14-improve-howyoumeasure-patient-outcomes/index.en.html#survival-outcomes-are-hard-to-measure-accurately-but-it-can-be-done",
    "href": "docs/posts/2022-03-14-improve-howyoumeasure-patient-outcomes/index.en.html#survival-outcomes-are-hard-to-measure-accurately-but-it-can-be-done",
    "title": "Three Steps To Improve How You Measure Patient Outcomes",
    "section": "Survival outcomes are hard to measure accurately but it can be done!",
    "text": "Survival outcomes are hard to measure accurately but it can be done!\nWith organizational support, with the right people, tools, frameworks, and workflows, and with accuracy checks over time, your program can produce high-quality data that you can use to improve the care you provide to your patients!"
  },
  {
    "objectID": "docs/posts/2020-11-27-gccp1-3/index.en.html",
    "href": "docs/posts/2020-11-27-gccp1-3/index.en.html",
    "title": "The Global Childhood Cancer Puzzle - Edge 1.3",
    "section": "",
    "text": "The following originally appeared on my podcast about global PHO - Global Health and Childhood Cancer\n\n\nThis is part three of a three part essay about the global burden of childhood cancer. If you haven’t read from the beginning, you may want to start from part 1 or check out part 2 if you missed that.\n\n\nMeasuring the health consequences of cancer\nLet’s think for a minute what the measures that we’ve discussed are telling us. Incident cases tells us about the number of new cases that occur in the world and survival tells us the final outcome for someone who has the disease. These measures tell us about the beginning and the end of the cancer journey, but there are many things that happen between those two points that are worth measuring, particularly the suffering and loss of wellbeing that result from cancer. Now you may be wondering, “how in the world do we measure loss of wellbeing? Loss compared to what?” The answer is that researchers measure it against the expected level of health for a patient if they never developed cancer.\nLet me provide an example to clarify these concepts. Suppose there is a restaurant with 100 people eating in it. Suppose most people have a lovely time drinking and chatting with friends; however, ten unlucky people order the chicken special that was undercooked by the chef and immediately get terrible food poisoning. If we were interested in quantifying the burden of food poisoning in our 100 patrons, we could say the number of incident cases was 10 per evening at the restaurant. Also suppose, tragically, 2 people out of the 10 cases die from their food poisoning but the rest survive and completely recover. Both die within the first month, so that seems like a good timeframe for reference. We can say that 1-month survival of this terrible food poisoning is 8 in 10 or 80%. These numbers give us a good snapshot of the situation, but there is still something missing.\nIf you’ve ever had food poisoning, then you know that there is much more to the experience than these number describe. The illness can bring terrible stomach pains, vomiting, and other messy symptoms. The suffering from it can be intense and may last a few hours to weeks. Suppose you were one of the people who got food poisoning at this restaurant and then recovered. If someone described the impact of the disease as, “10 people got sick that night and 2 died”, you would think this does not describe the experience well for you or anyone else who got sick. To better capture the experience of the illness, the suffering it causes seems like an important part to describe.\nTo measure this experience, researchers have developed a tool called the “Disability-Adjusted Life Year”, or DALY for short. DALYs quantify suffering along two dimensions, the loss of wellbeing for patients living with a disease and the years of life lost for patients who die from it. Let’s take these one at a time.\nTo measure loss of wellbeing (technical term: the years lived with disability), researchers have to first decide what is normal wellbeing. They can make the assumption that, roughly speaking, people who never develop the disease and have similar health to the patient before the patient is diagnosed can act as a good point of reference for the health the patient would have experienced if the patient never developed the disease in the first place. Then they measure the actual health of the patient with the disease against the healthy people. There are many conditional assumptions in this concept, which makes it confusing. We can simplify it to say that researchers measure the difference in health between people without the disease and a person with the disease. How do you put a number on the health state of a person with a disease? The most common way researchers do this is they ask a bunch of people to rate different states of health relative to full health and then they decide on an average “value” of the diseased health state relative to healthy state. Then researchers subtract the “value” of the health state of a person with the disease from the “value” of full health over time to calculate an estimate of lost wellbeing.\nMeasuring years of life lost is more straightforward. Researchers can say that the life expectancy (average lifespan) for a bunch of people similar to the patient but who do not have the disease is the life expectancy of the patient of the patient if they never developed the disease. Researchers then take the life expectancy and subtract the age at which a patient dies, and they have an estimate of the years of life that were lost due to the disease.\nFigure 6 shows an example of the health states for three different types of people in our example. The top line is a healthy person who never develops food poisoning, the middle line is a perspon who develops food poisoning and then recovers, and the bottom line is a person who develops food poisoning and eventually dies. The difference in area between the top line and each of the other two lines are the loss of wellbeing for those patients (pink colored areas on second row of figure 6). The difference between the life expectancy (blue line on the graph) and the age at which the bottom line reaches a health state of zero is the years of life lost.\n\nFig. 6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow that we know how to measure DALYs, the next question is, “what are they used for?” DALYs allow for two important comparisons. The first is that they allow researchers to measure the consequences of a disease relative to where it occurs in a lifespan. The years of life lost for a person who is 100 years old and develops cancer compared to a person who is 5 years old are very different and DALYs give researchers a way to put a number on that difference. The second thing DALYs allow is the comparison of very different diseases. With them, we can think about questions such as, “how do the health consequences of a common cold compare to cancer”. Colds are very, very common, somewhat uncomfortable, and most of the time get better on their own. Cancer is the exact opposite. If we wanted to, we could calculate DALYs for each, which could allow us to compare the health consequences and burden of these two very difference diseases.\nIt is important to point out that DALYs are very rough estimates of the experience of having a disease. They give researchers a way to put a number to this experience, so they can track it over time and compare it across other diseases. What DALYs are not is an actual statement about what living with cancer or any other disease is really like. We could write a 10,000-page novel try to convey the true experience of just one child with cancer, and still not capture the truth. The experience of cancer is a profound trial to endure and no researcher wants to cheapen that. DALYs are imperfect tools used by limited humans to try to measure an experience that defies description for the sake of helping those experiencing it. Furthermore, there are well known critiques of DALYs and what they do and do not measure that are worth exploring to better both what specifically they are measuring and what are their limits.\nLet us now turn to measuring the burden childhood cancer using DALYs. A group of researchers recently published the most comprehensive estimate of DALYs due to childhood cancer to date.\\(^3\\) The most important finding from this paper corrects a common wrong belief that goes “because childhood cancer is rare, its total burden is low compared to other diseases.” The researchers reported that childhood cancer caused 11.5 million DALYs in 2017. While this single number may be difficult to interpret, compared to the other most common childhood diseases, childhood cancer ranked 9th globally. In upper-middle income countries, it was the 3rd largest cause of DALYs, falling only behind congenital birth defects and lower respiratory infections. See figure 7 to see the comparisons. They also compared DALYs to adult cancers and found that, globally, childhood cancer ranks 6th among all types of cancers (see figure 8) and in low and low-middle income countries, it is ranked 1st! Although childhood cancer is less common compared to other disease, the researchers demonstrated it is still responsible for a large amount of disease burden in the world.\n\n\nFig. 7 - DALYs due to pediatric diseases\n\n\n\n\n\n\n\n\n\n\n\nFig. 8 - DALYs due to cancers\n\n\n\n\n\n\n\n\n\nThey also looked at groups of countries in the world are most responsible for the greatest number of DALYs. The researches grouped countries by their “sustainable development index” (SDI), a number that gives a rough assessment of a country’s level of development. Consistent with the findings we’ve discussed so far, they found that lower SDI countries are responsible for the large majority of DALYs. Figure 9 shows total number of DALYs by SDI grou. In this figure, we again see the trend that the regions in the world that are least prosperous are the ones that suffer the highest burden of disease.\n\n\nFig. 9 - Animated\n\n\n\n\n\n\n\n\n\n\n\nClick for static image\n\n\n\n\n\n\n\n\n\n\n\n\n\nLooking at GPO through the lens of DALYs not only helps us better understand the magnitude of the disease, but also informs discussion about how to set health priorities. The sad reality is that we live in a world of limited time, money, and resources. Because of these limitations, health officials, politicians, and other people who set healthcare policies in a country, have to make hard decisions about where to focus their efforts and resources. If decision makers do not have comprehensive information about the burden of diseases in their country, then they will not be able to make the best decision that will benefit the most people. As we have seen, comparisons of DALYs across diseases are a necessary measure to inform compassionate decisions.\nA good example of this situation would be a health planner in an upper-middle income country that may need to decide whether to buy more cancer medicines or tuberculosis medicines for pediatric patients. According to the DALY estimates, for the average upper-middle income country pediatric cancer is the 3rd largest source of DALYs and tuberculosis is the 59th. This seems like an important piece of information for the health planner to know when making the decision. I’m certainly not saying that this is the only component to the decision. She would also need to consider costs, implementation feasibility, social context and a host of other factors that are not mentioned here. But I am saying that making the best decision that does the greatest amount of good is extremely difficult and can only be done when comprehensive information about the burden of the disease is available.\nWe spent time developing an intuition for what DALYs are because they are so important for understanding this edge of the GPO puzzle. Although more rare than other disease of childhood, it is responsible for enormous suffering due to how lethal it is for young patients and how debilitating it can be for those who survive. DALYs help to describe what should be the case - that in 2017 there should have been 11.5 million healthy life years lived that were not. Understanding what the world would look like if every child with cancer receives optimal care helps put the tragedy of GPO in a new perspective that better conveys the magnitude of the problem compared to only looking at the total number of new cases or deaths per year. Lastly, DALYs are helpful to understand because they play an important role in health policy and allow people who have to make really hard decisions to be well informed. Anyone interested in addressing GPO through advocacy and health policy need to be comfortable with these concepts so they can speak the same language as the decision makers they seek to impact.\n\n\n\nSummary of Edge #1 – The global burden of childhood cancer\nWe’ve come a long way in this discussion. We saw that LMICs suffer the most from pediatric cancer, where 90% of children with cancer live. We established that GPO is much more common than was previously thought, with around 400,000 new cases of pediatric cancer per year. We also identified the ongoing catastrophe of non-diagnosis, which results in around 43% of kids with cancer, or 175,000 children, never even being diagnosed. If things do not change, in the next 15 years, 3 million children with cancer will never be diagnosed nor have a chance at cure - the equivalent of 10 school buses of children disappearing every day for 15 years. We also saw that the average 5-year survival around the world is about 37%, but ranges between 8% and 83% with the vast majority of low survival rates in LMICs. The picture only worsens when we add patients who are not diagnosed and the teenage AYA patients. Measuring DALYs due to cancer demonstrated that GPO sits among the top sources of ill health in the world, ranking number 9 globally for all pediatric diseases, number in 3 in middle-income countries, and number 6 globally when compared adult cancers. See table 1 for a summary of these numbers. Thanks to the hard work of the people in the GPO community, we now have a clear picture of the magnitude of the burden of childhood cancer. Fitting these pieces together has shown the world that the suffering due to GPO is enormous and needs to be addressed now.\n\n\n\n\n\n\n\n\n\n\n\nTable 1. Summary statistics for the global burden of childhood cancer\n\n\n\nStatistic\nValue\n\n\n\n\nTotal incident cases of childhood cancer per year\n400,000\n\n\nPercent of kids with cancer that live in low- and middle-income countries\n90%\n\n\nNumber of non-diagnosed cases of childhood cancer per year\n175,000\n\n\nNon-diagnosed cases as a percent of total childhod cancer cases\n43%\n\n\nNumber of children who will not be diagnosed with cancer between 2015-2030\n3,000,000\n\n\nGlobal average 5-year survival for childhood cancer (not including undiagnosed)\n37%\n\n\nGlobal rank of childhood cancer when compared to other sources of pediatric DALYs\n9th\n\n\nGlobal Rank of childhood cance when compared to other sources of cancer-related DALYs\n6th\n\n\n\n\n\n\n\n\nThis exploration of the burden of disease is the first edge in the childhood cancer puzzle. Now that we have an increasingly clear picture of how much cancer there is and the suffering it causes, the next question is what to do about it? It is natural to feel saddened and overwhelmed by these numbers, and it may seem as though the problem is too big to be fixed, but I have good news: for the last three decades, the GPO community has been demonstrating that we can improve care, and we now know the right intervention can significantly improve survival in a short period of time. Answers already exist, it’s just a matter of putting the puzzle pieces together. How to improve outcomes is the second edge to the puzzle and will be the topic of the next visual essay.\n\nReferences\n\nWard ZJ, Yeh JM, Bhakta N, Frazier AL, Atun R. Estimating the total incidence of global childhood cancer: a simulation-based analysis. The Lancet Oncology. 2019;20(4):483-493.\nWard ZJ, Yeh JM, Bhakta N, Frazier AL, Girardi F, Atun R. Global childhood cancer survival estimates and priority-setting: a simulation-based analysis. The Lancet Oncology. 2019.\nForce LM, Abdollahpour I, Advani SM, et al. The global burden of childhood and adolescent cancer in 2017: an analysis of the Global Burden of Disease Study 2017. The Lancet Oncology. 2019;20(9):1211-1225.\n\n\nSubscribe to my podcast about global PHO at GHCCpod.com"
  },
  {
    "objectID": "docs/posts/2025-01-17-external_controls/index.html#introduction",
    "href": "docs/posts/2025-01-17-external_controls/index.html#introduction",
    "title": "External Controls in Oncology Clinical Trials",
    "section": "Introduction",
    "text": "Introduction\nExternal controls are, for better or worse, common in pediatric oncology clinical trials. I don’t love them. They are dangerous. However, there are arguments in their favor. In this post, I’ll briefly discuss what historical controls are, when their use might be justified, and why they’re dangerous."
  },
  {
    "objectID": "docs/posts/2025-01-17-external_controls/index.html#what-are-external-controls",
    "href": "docs/posts/2025-01-17-external_controls/index.html#what-are-external-controls",
    "title": "External Controls in Oncology Clinical Trials",
    "section": "What are external controls?",
    "text": "What are external controls?\nThe purpose of clinical trials is to evaluate the effect of a treatment in some population of patients. In a classic randomized clinical trial, patients who meet the study criteria are randomized to either receive the standard of care for their disease or a new treatment. The outcome between the two groups is compared, and due to the miracle of randomization, the treatment effect can be identified as the difference in the outcomes between the groups.\nExternal controls are a group of patients not enrolled in a clinical trial but used as a comparison group for patients on a trial. The difference between the outcomes for the controls and the treatment group is taken as the effect of treatment, much like the comparison with the standard of care arm in a clinical trial. For valid comparisons, the control group should look as similar as possible to those that could have been enrolled in the trial. The group may be taken from a previous clinical trial, a patient registry, or other real-world data source. The key limitation of this design is that external controls do not benefit from the wonders of randomization, which makes bias a much greater danger when estimating the effect of treatment."
  },
  {
    "objectID": "docs/posts/2025-01-17-external_controls/index.html#why-we-might-use-external-controls",
    "href": "docs/posts/2025-01-17-external_controls/index.html#why-we-might-use-external-controls",
    "title": "External Controls in Oncology Clinical Trials",
    "section": "Why we might use external controls",
    "text": "Why we might use external controls\nExternal controls are generally used when trialists cannot or should not enroll a comparison group for ethical reasons or because the disease is so rare. Below is a table briefly explaining three reasons that might justifying using external controls. I wouldn’t go so far as to call these good reasons, but perhaps they are permissible (Marion and Althouse 2023).\n\nPermissible reasons\n\n\n\n\n\n\nWhen Randomization to Placebo is Unethical In condition has with well-established, effective treatments, then randomizing patients to receive a placebo would be unethical. If investigators wish to measure the effect compared to no treatment then they may opt for an external control.\n\n\nRare Disease ResearchRare diseases make it difficult to recruit sufficient numbers of participants to meet the power requirements for a trial. External controls may be used to reduce the size of the trial.\n\n\nSevere Outcomes or Vulnerable Populations Randomization may be viewed as unacceptable for diseases with particularly severe outcomes (e.g. terminal illness) or that affect vulnerable populations (e.g. children) even if there is no standard of care.\n\n\n\nCollignon et al. (2021) sums up the use cases for historical (external) controls in their conclusion:\n\nThe use of historical controls, therefore, is better suited for cases of high unmet clinical need, where the disease course is well characterized and the primary endpoint is objective.\n\n\n\nSuspect reasons\nThere are also many suspicious reasons for using external control. One is that they reduce the time and costs associated with conducting a trial. The problem with this reasoning is that the savings may be minimal. Enrolling 200 patients in a trial compared to 400 patients does not reduce the budget by half, given the enormous fixed costs associated with designing and opening trials. No matter how many patients are enrolled, the trial may have to be run for the same time when outcomes are measured over years, as with survival in oncology trials. While external controls might suggest savings, the benefits are not as advertised.\nMore importantly, there is an enormous price to pay for the credibility of the trial’s results. Many sources of bias can confound the treatment effect and make clear conclusions impossible. A trial may go on for ten years only to end in a collective, scientific shrug ¯\\_(ツ)_/¯."
  },
  {
    "objectID": "docs/posts/2025-01-17-external_controls/index.html#sources-of-bias-from-external-controls",
    "href": "docs/posts/2025-01-17-external_controls/index.html#sources-of-bias-from-external-controls",
    "title": "External Controls in Oncology Clinical Trials",
    "section": "Sources of bias from external controls",
    "text": "Sources of bias from external controls\nExternal control’s fundamental problem is that they are not randomized, leaving many ways that they can introduce bias into the analysis. Randomization reveals a treatment effect by guaranteeing that only random chance determines which treatment patients receive. This ensures no residual confounding between treatment and the outcome of interest. In the parlance of directed acyclic graphs (DAGs), randomization erases all back door paths between the treatment and the outcome. No such guarantees exist for external controls, and backdoor paths very likely exist between treatment and outcomes. I’ve drawn the DAG below to represent the essential structure of the problem. There are many other more complex structures, but I find this to be a sufficient representation of the basic concerns.\n\n\n\n\n\n\nOne example of how patient groups could affect treatment and outcome is if the external control group is drawn from a registry of sicker and older patients who tend to have worse outcomes than the group in the trial that received the treatment. This situation could make the treatment appear more effective than it truly is. There are many other sources of bias. The table below [adapted from (Burger et al. 2021)], lists some of the main sources of bias and how to mitigate their effect.\n\nBias types\n\n\n\n\n\n\n\nType of Bias and Description\nMitigation Strategy\n\n\n\n\nSelection BiasClinical trial participants often have different characteristics compared to patients in routine care settings\n• Design trial eligibility criteria that can be clearly applied to real-world settings• When selecting external controls, carefully match population characteristics• Use statistical approaches like propensity scoring, inverse probability weighting, or g-computation to account for population differences\n\n\nCalendar Time BiasTreatment outcomes can vary across different time periods due to evolution in medical practices\n• Select control patients from similar time periods as the trial• Provide evidence that standard of care has remained stable• If using historical controls, document stability of outcomes over relevant time periods\n\n\nRegional BiasPatient outcomes may differ between geographical locations due to variations in healthcare delivery\n• Source control patients from comparable geographic regions• Document that care standards are similar across regions• If using different regions, demonstrate comparable outcome patterns\n\n\nAssessment BiasKnowledge of treatment assignment can influence how outcomes are evaluated\n• Focus on objective endpoints where possible• Consider independent outcome review processes• Implement rigorous sensitivity analyses\n\n\nDifferent Endpoint BiasClinical trial endpoints may be measured differently than in routine practice\n• Ensure consistent endpoint definitions• Obtain necessary documentation (e.g., imaging) to allow standardized assessments• Account for differences in assessment frequency\n\n\nImmortal Time BiasChallenges in establishing comparable time zero points between trial and control patients\n• Clearly define and align study entry time points• Carefully evaluate potential biases in time-based analyses\n\n\nRetrospective Selection BiasRisk of selectively choosing external data or analysis approaches after seeing results\n• Pre-specify all selection criteria and analyses• Document selection process transparently\n\n\nStudy BiasTrial participation itself can affect outcomes due to different care patterns\n• Consider selecting controls from similar clinical settings• Document and account for differences in care delivery\n\n\nBetween Study VariabilityHigh unexplained outcome variation across studies suggests presence of important uncontrolled factors\n• Consider randomized design if high unexplained variability exists• Carefully document and account for known sources of variation\n\n\nIntercurrent Event BiasEvents occurring after study entry can affect comparability\n• Apply consistent approaches to handling intercurrent events• Consider multiple analytical approaches to test robustness"
  },
  {
    "objectID": "docs/posts/2025-01-17-external_controls/index.html#how-to-reduce-bias-when-using-external-controls",
    "href": "docs/posts/2025-01-17-external_controls/index.html#how-to-reduce-bias-when-using-external-controls",
    "title": "External Controls in Oncology Clinical Trials",
    "section": "How to reduce bias when using external controls",
    "text": "How to reduce bias when using external controls\nThe best way to reduce bias from external controls is an open area of research. This is a polite way of saying it’s complicated and a mess. It’s complicated because these studies are not quite observational studies, but they definitely aren’t randomized trials. Researchers have, therefore, opted for a range of analytical techniques, including using thresholds, propensity scores, matching, and meta-analytic methods. A full discussion of analytical options is outside the scope of this blog post, but see the references at the end for more. Regardless of the method used, the most important way to control bias is undoubtedly to select an external control group that is a close comparison to the types of patients enrolled in the trial.\nAnalyzing data from these types of studies is also a mess because many untoward motivations can sneak into the results. These studies can be used to justify regulatory approval for a drug or biomedical device. Companies are motivated to represent their product in the best way possible because there is a pot of gold at the end of the quest for regulatory approval. Mixing a company’s profit margins with a high dimensional vector of bias sources and a flexible choice of analytical techniques is a recipe for chaos."
  },
  {
    "objectID": "docs/posts/2025-01-17-external_controls/index.html#conclusion",
    "href": "docs/posts/2025-01-17-external_controls/index.html#conclusion",
    "title": "External Controls in Oncology Clinical Trials",
    "section": "Conclusion",
    "text": "Conclusion\nI think it’s possible to justify the use of external controls and glean useful information from them in certain settings. Regulators have even begun to admit them as evidence to justifty drug approval [see (Mishra-Kalyani et al. 2022) for examples]. But I also think it’s a generally rational policy to increase one’s skepticism about a study in proportion to the number of free researcher degrees of freedom with an additional adjustment for the presence of motivated reasoning. Both are present in abundance with external controls. So, it makes good sense to bring a strong scientific skepticism to these studies. In this blog, we’ve outlined the many ways that bias influences their results. The burden of proof – a rightfully heavy one – should rest on the investigators to demonstrate their study design and analysis overcome the many limitations of external controls."
  },
  {
    "objectID": "docs/posts/2022-03-12-cure-more-kids/index.en.html",
    "href": "docs/posts/2022-03-12-cure-more-kids/index.en.html",
    "title": "The single best way to cure more childhood cancers",
    "section": "",
    "text": "The best way to cure more childhood cancers is to develop better treatments, Right?\n…Right?\nImplicitly or explicitly, this is what most people believe. And it’s no wonder given our astounding success in developing effective therapies over the last six decades. In 1960, survival for acute lymphoblastic leukemia, a type of blood cancer, was &lt;10%. Then a group of heroic parents, doctors, nurses, scientists, philanthropists, and politicians joined forces and conducted the first large collaborative clinical trial to identify better treatments. Over the decades, we saw survival rise to 30% in the early 70s, 75% in the 80s, and over 85% today! It’s not just leukemia. In high-income countries like the United States, we can cure 85% of all cancer cases in kids. This is an astounding medical and scientific success story!\nUnfortunately, this success has created a belief that the most effective way to save lives now is to conduct more trials.\nThis is not true."
  },
  {
    "objectID": "docs/posts/2022-03-12-cure-more-kids/index.en.html#most-children-with-cancer-are-not-cured-because-they-do-not-receive-the-treatments-that-can-cure-them.",
    "href": "docs/posts/2022-03-12-cure-more-kids/index.en.html#most-children-with-cancer-are-not-cured-because-they-do-not-receive-the-treatments-that-can-cure-them.",
    "title": "The single best way to cure more childhood cancers",
    "section": "Most children with cancer are not cured because they do not receive the treatments that can cure them.",
    "text": "Most children with cancer are not cured because they do not receive the treatments that can cure them.\nAround 90% of children with cancer live in low- and middle-income countries where over half of cases may not be diagnosed, where treatment may not be available, and where cure rates can be as low as 10%.\nThis is a stunning disparity.\nWe, the medical community, have developed treatments that can cure up to 85% of cancer cases, yet the vast majority of children do not receive the treatments that we know they need.\nTo save more lives today, the most pressing question to answer is:\n\nHow do we deliver effective treatments to the kids who need them?"
  },
  {
    "objectID": "docs/posts/2022-03-12-cure-more-kids/index.en.html#focusing-on-care-delivery-shows-us-the-most-important-challenges-to-overcome",
    "href": "docs/posts/2022-03-12-cure-more-kids/index.en.html#focusing-on-care-delivery-shows-us-the-most-important-challenges-to-overcome",
    "title": "The single best way to cure more childhood cancers",
    "section": "Focusing on care delivery shows us the most important challenges to overcome",
    "text": "Focusing on care delivery shows us the most important challenges to overcome\n\nMedication availability\nTreatment quality\nSupportive care\nSupply chains\nStaffing\n\nThese are very different types of problems than developmental therapeutics, but the answer to each one can save many lives.\nDon’t get me wrong, we need better therapies so that we can cure every child with cancer. We need clinical trials.\nBut to bring about a day when every child in the world receives the best treatments possible, the global oncology community must answer the treatment delivery question."
  },
  {
    "objectID": "docs/posts/2024-01-01-gratitude/index.html#the-contentification-of-life",
    "href": "docs/posts/2024-01-01-gratitude/index.html#the-contentification-of-life",
    "title": "Gratitude reduces the complexity of modern life to make it more meaningful",
    "section": "The contentification of life",
    "text": "The contentification of life\nThe world is increasingly filled with noise and nonsense, making it difficult to see what is valuable amid a torrent of trivialities. With the rise of social media, our attention has become an economic commodity. Four million new posts are made on Instagram every hour; 3.7 million new YouTube videos are uploaded every day. This is chump change compared to what’s coming. The rate of content production is going to increase exponentially as generative AI matures in consumer applications. And the contentification of the internet for the commodification of our attention has overflowed into the offline world. Everywhere we turn, we’re offered the chance to level up in our jobs, our relationships, our skills, or our goals (and on Facebook, for me, level up my hip mobility because I clicked on an ad about stretching once). There’s always more to see, more to learn, and more to do to maximize the moment and optimize our happiness. Almost all of this is a heap of rubbish. Yet there are always small, buried treasures that keep us coming back to rummage through the garbage."
  },
  {
    "objectID": "docs/posts/2024-01-01-gratitude/index.html#the-exponentiation-of-life",
    "href": "docs/posts/2024-01-01-gratitude/index.html#the-exponentiation-of-life",
    "title": "Gratitude reduces the complexity of modern life to make it more meaningful",
    "section": "The exponentiation of life",
    "text": "The exponentiation of life\nBeyond the value of the content we produce, the speed of production also gives us heartburn. We humans, evolved in the cradle of the natural numbers, nourished as a species by the small positive integers that we found on our fingers and our toes, have difficulty handling the exponentiation of possible human experiences. There are so many things we could do. So many ways to spend our time. So many counterfactual worlds unfolding at ever increasing speeds ahead of us. Navigating life is increasingly complex, increasingly overwhelming. Our attention can’t handle it all."
  },
  {
    "objectID": "docs/posts/2024-01-01-gratitude/index.html#many-options-that-mostly-stink",
    "href": "docs/posts/2024-01-01-gratitude/index.html#many-options-that-mostly-stink",
    "title": "Gratitude reduces the complexity of modern life to make it more meaningful",
    "section": "Many options that mostly stink",
    "text": "Many options that mostly stink\nSo, we have tons of ways we could spend our time, energy, and attention. Too many. Most of these options are garbage. But it’s hard to tell upfront what is worthless and what is valuable because attention is money, so everything is marketing. Everything has a new coat of paint and has been spritzed with perfume. What do we do when there are so many things we could do, but it’s hard to tell which are wastes of time? How do we reduce the dimensionality of our options with so much noisy data?"
  },
  {
    "objectID": "docs/posts/2024-01-01-gratitude/index.html#gratitude-can-help",
    "href": "docs/posts/2024-01-01-gratitude/index.html#gratitude-can-help",
    "title": "Gratitude reduces the complexity of modern life to make it more meaningful",
    "section": "Gratitude can help",
    "text": "Gratitude can help\nGratitude can help us focus on what is valuable. Being grateful for something implies a host of magnificent and healthy epistemic processes. For example, I’m grateful for my dog, Rosie. I’m particularly grateful that she’s acting less like a puppy and more like a productive member of my family’s household society. To tell you about my gratitude for my dog’s behavior, I had to first search through all sorts of things in my life I could tell you about. Rosie was easy to find in this search process. However the algorithm works, it was efficient and accurate - it didn’t take me long to come up with an example and I’m certain about my feeling here. To articulate my gratitude, I then had to particularize Rosie as an object of my attention. Rosie is not an abstract concept or just an idea. She became an object of my attention with properties and characteristics that I can describe. In other words, I focused on her to the exclusion of anything else. Once presented in my awareness, my relationship with her takes center stage. I’m grateful for her because of how we interact. Not just because of who she is or who I am but because of who we are together. We’re correlated statistically. What I think is associated with who she is. We’re coupled cybernetically. How she exists influences what I do. My gratitude is about our existence together - a dynamic, fluid, abstract-yet-influential relationship that helps to direct my experiences and actions over time."
  },
  {
    "objectID": "docs/posts/2024-01-01-gratitude/index.html#gratitude-as-a-signal-of-value",
    "href": "docs/posts/2024-01-01-gratitude/index.html#gratitude-as-a-signal-of-value",
    "title": "Gratitude reduces the complexity of modern life to make it more meaningful",
    "section": "Gratitude as a signal of value",
    "text": "Gratitude as a signal of value\nRosie has helped me find what is valuable. My gratitude acted as both search and filter for finding something that has meaning to me. Easy, quick, accurate. It doesn’t matter how many more things I add to search through or how much more content humanity produces; it will still be easy to identify her - and a host of other good things in my life - as something I’m grateful for. This salience in my focus, this worthiness of attention that gratitude produces, is something I can only describe as meaningful. It fills that epistemic moment with a sense of value. Gratitude helps us find the signals of value in the noise of modern life."
  },
  {
    "objectID": "docs/posts/2024-01-01-gratitude/index.html#gratitude-as-dimensionality-reduction",
    "href": "docs/posts/2024-01-01-gratitude/index.html#gratitude-as-dimensionality-reduction",
    "title": "Gratitude reduces the complexity of modern life to make it more meaningful",
    "section": "Gratitude as dimensionality reduction",
    "text": "Gratitude as dimensionality reduction\nTo be grateful for Rosie, I also had to focus on her to the exclusion of anything else. There are many, perhaps uncountably infinite, things I could be grateful for. There are also many, perhaps uncountably infinite, ways that I could be grateful for Rosie. Relationships are dynamic. They change in quality over time. But from all of these possibilities, I could wrap my attention around a thing known as Rosie and describe her in a way that is comprehensible to you in just a few words. I could envelop the intricate complexities of our relationship, an immaterial thing, to communicate the positive experience I take from it. Considering how Rosie and I can move through life together, there are yet infinitely more ways that we could act and interact, and yet my gratitude at where we’ve been suggests possible futures that are valuable and that would make me grateful anew. In a sea of infinite options, gratitude has reduced the dimensions I must consider, helping me reflect on where I’ve been and chart a course for the future."
  },
  {
    "objectID": "docs/posts/2024-01-01-gratitude/index.html#gratitude-in-2024",
    "href": "docs/posts/2024-01-01-gratitude/index.html#gratitude-in-2024",
    "title": "Gratitude reduces the complexity of modern life to make it more meaningful",
    "section": "Gratitude in 2024",
    "text": "Gratitude in 2024\nThis has been a meditation on part of life I find overwhelming. I want to do good stuff. I want to be good at the things I do. I want to have meaningful relationships. I want to live my life well. Many and more are the ways this could happen or could go wrong. There are so many dang options for how to spend my time. I’m extremely grateful for my dog, obviously, as well as my family, friends, job, and God. Gratitude helps me slow down, find those things that are meaningful, and savor the good dynamics of those relationships. It also helps me to think about how to nourish those good things so they bloom into something beautiful. Being grateful for what I’ve experienced in 2023 helps me plan what to do in 2024. I have goals, but gratitude suggests the purpose of planning isn’t to achieve goals but to help what is true, beautiful, and good in my life flourish."
  },
  {
    "objectID": "docs/posts/2022-03-15-monitor-catchment-area/index.en.html",
    "href": "docs/posts/2022-03-15-monitor-catchment-area/index.en.html",
    "title": "A simple method to monitor a catchment area and improve disease diagnosis rates",
    "section": "",
    "text": "In a recent article, I argued that Global Health needs Data Science. Now I want to give an easily implemented example of how Data Science capabilities can improve treatment delivery in low-resource treatment settings.\nNote: I’m using the term “Data Science” because it is flashy enough to catch your eye and broad enough to encompass the many specializations that are the key to success: data collection and management, informatics tools, data engineering, operational analytics, statistics, and machine learning.\nI’m a pediatric hematologist-oncologist, and my examples are curated from my experience supporting Data Science capabilities in Global Oncology programs. You may need to adjust how think about this example ff the framing doesn’t make sense for your discipline.\nWith that said, let’s dive in."
  },
  {
    "objectID": "docs/posts/2022-03-15-monitor-catchment-area/index.en.html#example-monitor-catchment-area-referral-patterns",
    "href": "docs/posts/2022-03-15-monitor-catchment-area/index.en.html#example-monitor-catchment-area-referral-patterns",
    "title": "A simple method to monitor a catchment area and improve disease diagnosis rates",
    "section": "Example: Monitor catchment area referral patterns",
    "text": "Example: Monitor catchment area referral patterns\nIt is estimated that over half of cancers in children in low- and middle-income countries are never diagnosed. If a treatment program can monitor where children are coming from and compare that to the expected rates of cancer cases in the area, then programs can focus outreach efforts on areas where there are fewer diagnoses than what is expected based on a reasonable estimate of cancer incidence.\n\nRequired data\nPatient address or town/district/province of origin, date of diagnosis, disease diagnosis (high-level).\n\n\nBrief description of methods\nCalculate counts of diagnoses in a medium-long period of time, for example, 1 year by some geographic unit such as town or district of origin. Find data on estimated expected diagnoses in each geographic unit by finding a source that estimates disease incidence (The Global Burden of Disease 2019 study is a good source). Estimate the total at-risk population in the catchment area (there is normally publicly available population information, or you can generally find the country’s most recent census on the ministry of health website). If you need to stratify by age category, you can download population data on the UN Population Dynamics page and then multiply the ratio of people in the age categories compared to the total country population by the total population in the catchment area.\n\n\nResults\nThese methods should give the observed number of patients by disease type in each geographic area of interest and the expected number. Now you just subtract the observed how far the observed is from the expected. The geographic units with the largest difference are under performing and should be the target of outreach by your program."
  },
  {
    "objectID": "docs/posts/2022-03-15-monitor-catchment-area/index.en.html#all-it-takes-is-three-variables",
    "href": "docs/posts/2022-03-15-monitor-catchment-area/index.en.html#all-it-takes-is-three-variables",
    "title": "A simple method to monitor a catchment area and improve disease diagnosis rates",
    "section": "All it takes is three variables",
    "text": "All it takes is three variables\nWe can learn a lot with three variables and publicly available information. And all we did was add and subtract. It’s not the fanciest analytical method in the world, but if it gets more patients in treatment then it meets our needs."
  },
  {
    "objectID": "docs/posts/2022-04-07-htma-5-calibration-techniques/index.en.html",
    "href": "docs/posts/2022-04-07-htma-5-calibration-techniques/index.en.html",
    "title": "How to measure anything in Global Health: 5. Calibration techniques",
    "section": "",
    "text": "In my previous essay, I discussed how calibrating your judgments is a crucial step when measuring anything in global health.\nFor instance, if you want to estimate the likelihood a new cancer treatment causes severe side effects, then a calibrated estimator can offer a 90% uncertainty interval about a plausible range of side effect rates as a starting place for measurement.\nAlthough it is essential in the measurement process, calibration can be a tricky state to achieve."
  },
  {
    "objectID": "docs/posts/2022-04-07-htma-5-calibration-techniques/index.en.html#here-are-5-recommendations-for-how-to-achieve-calibration-with-your-judgment.",
    "href": "docs/posts/2022-04-07-htma-5-calibration-techniques/index.en.html#here-are-5-recommendations-for-how-to-achieve-calibration-with-your-judgment.",
    "title": "How to measure anything in Global Health: 5. Calibration techniques",
    "section": "Here are 5 recommendations for how to achieve calibration with your judgment.",
    "text": "Here are 5 recommendations for how to achieve calibration with your judgment.\nThis material comes from Douglas Hubbards marvelous book, How to Measure Anything. I recommend you read the book if these posts interest you. You will find much more thorough explanations of these concepts there.\n\n1. The equivalent bet test\nAsk yourself, would you rather accept a bet to win $1000 if the true value falls in your uncertainty interval or accept a bet to spin a roulette wheel with a wedge of 10% of its area where you win nothing and 90% of its area where you win $1000. If you are calibrated, you should be indifferent between these bets. If you prefer your uncertainty interval, you’re overconfident. If you prefer the wheel, you’re underconfident.\n\n\n2. The premortem test\nExplain what would be wrong with your interval if the true value turns out to be outside your interval. If you can come up with a plausible failure mode, then perhaps your interval is too narrow.\n\n\n3. Test the limits\nExam the lower limit, then the upper limit of the interval. For a 90% interval, 95% of the plausible values should be above the lower limit. Does your limit reflect that estimate? Then move to the upper limit and assess it its plausible that 95% of values are below that limit. This helps to avoid anchoring bias, something I found myself susceptible to as I started exploring calibration.\n\n\n4. The absurdity test\nStart with an absurdly wide range and progressively narrow it, explaining why it can’t be that wide along the way. This test helps to locate the edge of our knowledge about the estimate by eliminating things that are obviously not correct.\n\n\n5. Repetition and feedback\nGo to the HTMA website and download the calibration tests to take, or make up your own questions, estimate the answers, and grade them. Calibration is not a skill that comes naturally to most people, but is a skill that must be practiced. There’s a large body of literature on the topic you can also reference, something I was unfamiliar with until recently.\nIt takes a little time and effort, but soon you can calibrate your judgment to measure anything in global health that will improve the care you provide to your patients!"
  },
  {
    "objectID": "docs/posts/2022-03-24-two-types-of-uncertainty/index.en.html",
    "href": "docs/posts/2022-03-24-two-types-of-uncertainty/index.en.html",
    "title": "Understanding the two types of uncertainty will help you live more confidently in a stochastic world",
    "section": "",
    "text": "We live in a complex and often chaotic world. To live confidently, we have to know how to navigate uncertain paths.\nWe make decisions every single day. What do you eat in the morning? What route should you take to work? Do you invest in a certain stock? Do you hire a new employee? Etc…\nHow do we make good decisions when the outcomes are uncertain? We have to first understand the two types of uncertainty that are present in any decision because each type requires its own response."
  },
  {
    "objectID": "docs/posts/2022-03-24-two-types-of-uncertainty/index.en.html#aleatory-uncertainty",
    "href": "docs/posts/2022-03-24-two-types-of-uncertainty/index.en.html#aleatory-uncertainty",
    "title": "Understanding the two types of uncertainty will help you live more confidently in a stochastic world",
    "section": "Aleatory uncertainty",
    "text": "Aleatory uncertainty\nAleatory uncertainty is uncertainty due to the inherent randomness in a process. Think of a single flip of a fair coin. There is a 50% chance it comes up heads and a 50% chance it comes up tales. Or think of the thoroughly shaken-up role of a fair die. There is a one in six chance of getting a 1 on every role. These probabilities are inherent in the system generating the outcome (coin flips and dice rolls) and there is no way to reduce the uncertainty about it."
  },
  {
    "objectID": "docs/posts/2022-03-24-two-types-of-uncertainty/index.en.html#epistemic-uncertainty",
    "href": "docs/posts/2022-03-24-two-types-of-uncertainty/index.en.html#epistemic-uncertainty",
    "title": "Understanding the two types of uncertainty will help you live more confidently in a stochastic world",
    "section": "Epistemic uncertainty",
    "text": "Epistemic uncertainty\nEpistemic uncertainty is your uncertainty about the outcome of a decision. This uncertainty is reducible by gaining knowledge. It is uncertainty that exists in the mind of the observer and could be reduced through empirical evidence or logical argument. What is the atomic number of zenon? How long is your left 3rd toe? Who was the first cartoonist to draw Donal Duck? These are all things one is likely uncertain about right now but could Google or measure if needed."
  },
  {
    "objectID": "docs/posts/2022-03-24-two-types-of-uncertainty/index.en.html#bringing-the-two-types-together",
    "href": "docs/posts/2022-03-24-two-types-of-uncertainty/index.en.html#bringing-the-two-types-together",
    "title": "Understanding the two types of uncertainty will help you live more confidently in a stochastic world",
    "section": "Bringing the two types together",
    "text": "Bringing the two types together\nSuppose you have a bag of 20 marbles, and I ask you to predict the color of the next marble you pull out. You know some are blue and some red, but you don’t know the relative proportions of each in the bag. There is both aleatory and epistemic uncertainty present in this scenario. If you were to start drawing marbles and put them to the side, you could count the relative proportions of what you draw out as you go and your epistemic uncertainty about that slowly reduce with each draw. Suppose you see 80% of the marbles are red and then dump them back in the bag. Now you know there is an 80% chance you will pull out a red marble on the next draw, which is the aleatory uncertainty inherent in the system. You’ve reduced your epistemic uncertainty as far as it can go, and now the rest is left up to aleatory chance."
  },
  {
    "objectID": "docs/posts/2022-03-24-two-types-of-uncertainty/index.en.html#living-in-light-of-uncertainty",
    "href": "docs/posts/2022-03-24-two-types-of-uncertainty/index.en.html#living-in-light-of-uncertainty",
    "title": "Understanding the two types of uncertainty will help you live more confidently in a stochastic world",
    "section": "Living in light of uncertainty",
    "text": "Living in light of uncertainty\nDifferentiating the two types of uncertainty is important because each has its own approach. When faced with high epistemic uncertainty, the answer is to get more information. Reduce the epistemic uncertainty in decisions as far as you are able within your means and time constraints. If you hit mostly aleatory uncertainty, then there is nothing you can do to reduce it further unless you break the whole system, which is rarely an option but something to consider. If you can’t intervene in the system, then you have to decide what benefits do you get from each potential outcome. If the benefits of a lower probability outcome far outway the benefits of a higher probability outcome, then it might make sense to go for the former. You’ve reached the point where you have to know what you want and decide if the probability of the outcome described by the aleatory uncertainty gives you enough payoff so that you should go for it.\nUnderstanding these two types of uncertainty will help you take steady steps along uneven ground and navigate successfully to your utility-maximized future.\n…Of course, nothing is ever that neat and tidy. Especially in this uncertain world. This is a good start but someday we’ll have to talk about monsters that emerge from the dark abyss of radical uncertainty."
  },
  {
    "objectID": "docs/posts/2020-12-04-winners-curse/index.en.html",
    "href": "docs/posts/2020-12-04-winners-curse/index.en.html",
    "title": "The Winner’s Curse",
    "section": "",
    "text": "When reading a study, have you ever found yourself skeptical of the “statistically significant” findings for reasons you can’t quite identify? You’ve read the methods section, and it seems technically sound. The study question was interesting, the alternative hypothesis was plausible, the variables were sensibly chosen, the data was collected appropriately for the type of study with no apparent selection bias, and the statistical analysis was thoughtfully performed and interpreted with great care to not overgeneralize the findings. Despite all of the study’s virtues, the low buzz of skepticism continued to rumble in the back of your mind. Where does this skepticism come from?\nIt may be that you have an intuitive grasp of an inherent problem with significance testing that makes inconsequential findings seem impressive. Filtering results based on significance testing can induce a bias that inflates effect sizes, even if the study itself is conducted according to the highest standards of rigor. This overconfidence can produce large, publishable findings that will never replicate in follow-up investigations. This phenomenon is called “The Winner’s Curse”, which takes its name from the finding that winners of auctions tend to overpay for their winnings. Even if you are a thoughtful researcher who cares about replicability, you can sill lose when you think you’ve won due to the possible overconfidence induced by significance testing.\nLet’s look at an example to build intuition about when a finding may be at risk from suffering The Winner’s Curse.\nThis example is taken from Andrew Gelman’s and colleagues’ wonderful book Regression and Other Stories. For a different take on a similar phenomenom, see Gelman’s blog. For a more technical treatment of the subject, see this paper."
  },
  {
    "objectID": "docs/posts/2020-12-04-winners-curse/index.en.html#that-fuzzy-feeling",
    "href": "docs/posts/2020-12-04-winners-curse/index.en.html#that-fuzzy-feeling",
    "title": "The Winner’s Curse",
    "section": "",
    "text": "When reading a study, have you ever found yourself skeptical of the “statistically significant” findings for reasons you can’t quite identify? You’ve read the methods section, and it seems technically sound. The study question was interesting, the alternative hypothesis was plausible, the variables were sensibly chosen, the data was collected appropriately for the type of study with no apparent selection bias, and the statistical analysis was thoughtfully performed and interpreted with great care to not overgeneralize the findings. Despite all of the study’s virtues, the low buzz of skepticism continued to rumble in the back of your mind. Where does this skepticism come from?\nIt may be that you have an intuitive grasp of an inherent problem with significance testing that makes inconsequential findings seem impressive. Filtering results based on significance testing can induce a bias that inflates effect sizes, even if the study itself is conducted according to the highest standards of rigor. This overconfidence can produce large, publishable findings that will never replicate in follow-up investigations. This phenomenon is called “The Winner’s Curse”, which takes its name from the finding that winners of auctions tend to overpay for their winnings. Even if you are a thoughtful researcher who cares about replicability, you can sill lose when you think you’ve won due to the possible overconfidence induced by significance testing.\nLet’s look at an example to build intuition about when a finding may be at risk from suffering The Winner’s Curse.\nThis example is taken from Andrew Gelman’s and colleagues’ wonderful book Regression and Other Stories. For a different take on a similar phenomenom, see Gelman’s blog. For a more technical treatment of the subject, see this paper."
  },
  {
    "objectID": "docs/posts/2020-12-04-winners-curse/index.en.html#losing-while-winning",
    "href": "docs/posts/2020-12-04-winners-curse/index.en.html#losing-while-winning",
    "title": "The Winner’s Curse",
    "section": "Losing while winning",
    "text": "Losing while winning\nSuppose I conduct a study looking at a blood pressure lowering medicine in kids less than 3 years old. I’ll run a randomized, double-blind, placebo controlled trial and look at the difference of means of the two groups. I’m only interested in the medicine’s ability to lower blood pressure, so I plan to do a 1-sided t-test for the hypothesis that the placebo group’s blood pressure minus the treatment group’s is greater than 0 (i.e. treatment lowers blood pressure) and test for significance at the \\(\\alpha = 0.05\\) level.\nSince we have omniscient control of this fake study, suppose the true effect of the medicine will on average lower the blood pressure by 2 mm Hg. So, in reality the treatment “works” even if the effect is small. In the real world the null hypothesis of no association should be rejected since we know the true effect of the treatment is different than the placebo. Crucially, suppose I won’t care that the children scream bloody murder half the time I try to measure the blood pressure (the joys of pediatrics) and will accept whatever readings I get. No one’s got time to try to convince a 2-year-old that blood pressure machines aren’t scary. I anticipate that the standard deviation will be rather large in both groups.\nStill in statistician god-mode, let’s suppose under these conditions with the sample size and the variability of the data, the standard error of the mean for this study will be 8 mm Hg.\nNow the central questions: given the true effect of 2 mm Hg and the study standard error of 8 mm Hg, how large of an effect will we need to estimate to reject the null hypothesis? Let’s look at the following figure to find out.\n\n\n\n\n\n\n\n\n\nThe black curve in the figure is the sampling distribution for the estimated difference in means between the placebo and treatment groups. This is the curve we would get if we ran the exact same study 10,000 times with different samples from the same population and calculated the average difference between the groups each time. Since we can’t do this is in real life, I simulated it on my computer. The true difference that we set is 2 mm Hg (blue line) and the standard error from the study is 8 mm Hg, which is why the curve is so spread out. Since the kids scream half the time the blood pressure is measured, the difference in means can vary from one simulation to the next. This is why some studies could give us a negative effect estimate (i.e. the treatment raises blood pressure) or why the size of the effect estimate could be much larger than 2 mm Hg. In fact, the shaded red region starting at 12 mm Hg represents the estimates at or greater than the 95th percentile of all possible estimates if the null hypothesis was true. This area represents “statistically significant” results and comprises 8% of possible studies. In other words, there is an 8%, or roughly 1 in 12, chance my study estimates an effect size in this region.\nSuppose I complete this study and get very excited because I got an average difference of 16 mm Hg (p = 0.04)! I’ll publish the finding straight away in the fanciest journal I can find. Why is this a problem? If we take an average difference of 16 mm Hg as the effect size, then it is 8 times larger than the real effect of 2 mm Hg and in reality this impressive looking finding is clinically meaningless! Lowering the blood pressure by 16 mm Hg for a 3 year old can be the difference between severe hypertension and a safe level, but lowering it by 2 mm Hg doesn’t mean much at all.\nThis finding also won’t replicate on follow-up investigations. Or even worse it will replicate in a way that reifies the inflated estimates. Suppose 12 similar follow-up studies were done, then it’s likely 1 of the other 12 will also give “significant” findings of effect. It’s possible that, due to publication bias, only the two significant studies are published, or at least published in a journal of note. One can read the literature and then conclude that these two technically sound studies that each giving similar effect estimates must represent good evidence of meaningful effect!"
  },
  {
    "objectID": "docs/posts/2020-12-04-winners-curse/index.en.html#avoiding-the-curse",
    "href": "docs/posts/2020-12-04-winners-curse/index.en.html#avoiding-the-curse",
    "title": "The Winner’s Curse",
    "section": "Avoiding the Curse",
    "text": "Avoiding the Curse\nThe Winner’s Curse can cause the size of “statistically significant” effect estimates to be inflated and can turn meaningless true differences into meaningful-but-false-but-publishable findings. A study is more at risk of suffering from a Winner’s Curse when the true effect size is small or the variability of the outcome variable is large. If you work in or read the literature from a field where effect sizes are small and/or measurement error is large, such as psychology, sociology, political science, or epidemiology, then you should take note of this danger!\nTo look for markings of the Curse, first think about how precisely can the outcome of interest be measured. Can it be measured precisely like a physics experiment? Or is there expected noise in the measurement, like measuring blood pressure in a screaming 2-year-old? If the outcome tends toward the imprecise range of the spectrum, the Curse is more likely. Also estimate for yourself a range of plausible effect sizes from the intervention being studied. If you think most plausible effect estimates are small, again the Curse is more likely. Finally think about how large is the possible measurement error compared to the true effect. If the supposed measurement error is many times larger than the plausible true effect size (like 8:2 in our example), the Winner’s Curse is going to haunt any of your “significant” findings."
  },
  {
    "objectID": "docs/posts/2022-06-04-epistemic-arc-intro/index.en.html",
    "href": "docs/posts/2022-06-04-epistemic-arc-intro/index.en.html",
    "title": "The Epistemic Arc - a conceptual map about how to learn from data in the presence of uncertainty",
    "section": "",
    "text": "Learning from data is hard.\nThe world is full of noise and nonsense masquerading as “insights” from data analyses.\nIf its so easy to go wrong, how can we reliably learn from data and avoid the many common analytical shenanigans that can ruin our results?\nI recently purchased and watched the videos for Frank Harrell’s incredible statistics short course, Regression Modeling Strategies(for the more advanced stats enthusiasts, this course outlines an incredible method for principled statistical analyses). As part of the course, Drew Levy gave a wonderful presentation about model selection using causal models, and, almost as an aside, he included a graphic about his framework for how he conceptualizes analyses of observational data.\nI was struck by the elegance of his framework. It concisely summarizes the steps in the learning process and identifies the many danger zones that can ruin your learning (click here to read more from Drew Levy’s website). It functions as a great conceptual roadmap for how to reliably learn from data while avoiding analytical shenanigans\nIn this post, I will adapt and expand the framework to explain how it provides a unified roadmap for learning from data. This framework is flexible and applies just as well to observational epidemiological studies as to randomized trials, physical experiments, any other branch of science, and even in our daily lives.\nThis will be a high level overview. In future posts, I will explore specific danger zones in more detail. Beyond organizing important statistical principals, this framework has important philosophical and psychological consequences. I also aspire to explore theses areas more in future posts."
  },
  {
    "objectID": "docs/posts/2022-06-04-epistemic-arc-intro/index.en.html#introduction",
    "href": "docs/posts/2022-06-04-epistemic-arc-intro/index.en.html#introduction",
    "title": "The Epistemic Arc - a conceptual map about how to learn from data in the presence of uncertainty",
    "section": "",
    "text": "Learning from data is hard.\nThe world is full of noise and nonsense masquerading as “insights” from data analyses.\nIf its so easy to go wrong, how can we reliably learn from data and avoid the many common analytical shenanigans that can ruin our results?\nI recently purchased and watched the videos for Frank Harrell’s incredible statistics short course, Regression Modeling Strategies(for the more advanced stats enthusiasts, this course outlines an incredible method for principled statistical analyses). As part of the course, Drew Levy gave a wonderful presentation about model selection using causal models, and, almost as an aside, he included a graphic about his framework for how he conceptualizes analyses of observational data.\nI was struck by the elegance of his framework. It concisely summarizes the steps in the learning process and identifies the many danger zones that can ruin your learning (click here to read more from Drew Levy’s website). It functions as a great conceptual roadmap for how to reliably learn from data while avoiding analytical shenanigans\nIn this post, I will adapt and expand the framework to explain how it provides a unified roadmap for learning from data. This framework is flexible and applies just as well to observational epidemiological studies as to randomized trials, physical experiments, any other branch of science, and even in our daily lives.\nThis will be a high level overview. In future posts, I will explore specific danger zones in more detail. Beyond organizing important statistical principals, this framework has important philosophical and psychological consequences. I also aspire to explore theses areas more in future posts."
  },
  {
    "objectID": "docs/posts/2022-06-04-epistemic-arc-intro/index.en.html#the-epistemic-arc",
    "href": "docs/posts/2022-06-04-epistemic-arc-intro/index.en.html#the-epistemic-arc",
    "title": "The Epistemic Arc - a conceptual map about how to learn from data in the presence of uncertainty",
    "section": "The Epistemic Arc",
    "text": "The Epistemic Arc\n\nThe above figure depicts the Epistemic Arc, which describes the process that anyone must follow when learning from data in the presence of uncertainty.\nThe first highlight is that there is a main character to this story: you. You are the hero or heroine of this epistemic adventure! You are a player on the statistical stage. You are an inseparable part of the learning process. You are the Learner.\nEach box represents a specific object that is required for the learning process (a noun). Each arrow depicts a step that must be performed to move from one object to the next (a verb). Let’s zoom in on each step to define the objects involved and the actions performed during it.\n\nStep 1. Nature -&gt; Population\n\n\nDefinitions\nStep 1: The process of nature or a physical process producing a population or system the Learner wishes to learn more about.\nNature or Physical Process: I define nature as whatever physical stuff exists upon which the drama of physical reality unfolds. Nature is the generator of the physical world the Learner encounters and wishes to know more about. A physical process is a mechanistic derivative of nature, perhaps built by humans, that may itself produce interesting phenomena to learn about. Examples: Nature - quarks, neurons, cats, quasars; Physical process - factory that produces bowling pins, a pendulum swinging in a clock.\nPopulation or System: A population is a distinct collection of entities derived from nature that is interesting to the Learner. A system is a set of interacting entities carved out of the larger system of physical reality. Examples: Population - Orange cats in Austin, Texas, children with ADHD, all the weeds that grow between sidewalk cracks; System - trees that talk to each other using fungi, academic medical education, ATP synthesis in your body from the electron transport chain.\n\n\nExplanation\nGoal: The Learner identifies what he or she wishes to learn about either in nature or in a population or system that emerges from it. This is where the Learner formulates the questions to answer with data. (e.g. For the weed between sidewalk cracks example, perhaps the question is: which type of pesticide most effectively kills weeds between sidewalk cracks?)\nCaution: Note this is a highly philosophical step, even if it seems straightforward to the Learner. There are no populations or systems in nature. These are concepts that the Learner overlays onto nature. Why does this matter? Because we, the Learners, are making analytical decisions about the most fundamental elements in the epistemic arc by defining the population or system to study ourselves (What is the definition of a weed? What is a crack? What is a sidewalk? Does a driveway count? Where will study the weeds?).\nBeware: This step sets the entire trajectory of the epistemic arc the Learner will follow. Precise questions yield precise answers. Vague questions produce vacuous answers that can be stretched, misshapen, and filled with whatever meaning one desires. Squishy questions may help get publications, but we won’t be any closer to learning truth.\nBonus: Sander Greenland recently argued that the discipline of statistics has explicitly causal foundations. In his paper, he says, “Whether answering the most esoteric scientific questions or the most mundane administrative ones, and whether the question is descriptive, causal, or purely predictive, causal reasoning will be crucially involved (albeit often hidden to ill effect in equations and assumptions used to get the”results”). I think this is correct and this step demonstrates and contextualizes his argument.\n\n\n\nStep 2. Population -&gt; Sample\n\n\nDefinitions\nStep 2: The process of identifying (no data collection yet) a sample that can be further scrutinized and designing a process that can deliver appropriate inferences about the population or the natural process that generates it from the data collected from the sample.\nSample (noun): A subset of the population or a toy model of the system. These are the members of the population for whom the Leaner has the ability to collect data and directly learn from.\n\n\nExplanation\nGoal: The goal is for the Learner to identify what sample will generate data that is most informative for the questions and then to design the process to learn from the data in a way that will account fo the limitations of using an imperfect sample to learn about a (possibly theoretical) population. In science, this is study design - e.g. does the Learner need to conduct an RCT, case-control study, cross-sectional study, etc.\nCaution: Randomness plays a big role in in our ability to learn about our population of interest from the sample. A random sample from the population as in a survey helps to ensure the sample is reflective of the population. A sample that is randomized to a treatment as in a randomized clinical trial helps to ensure what we learn about the causal process we are studying. Often randomness cannot be ensured at the level of the sample. In this case, study design and methods of causal inference help to make the sample reflects the population well enough to answer our question.\nCaution 2: Confusing terminology - to sample (verb) is one method of obtaining a sample (noun), but that’s only one way. Sometimes your sample is given to you, such as obtaining all medical records for a given disease from an electronic medical record. In this case you have a sample, a subset of a larger population (all patients with the disease), but you didn’t sample the population for it.\nBeware: Many crippling problems arise in this step even though you have not collected any data! Selection bias occurs when there’s a systematic reason your sample does not reflect your population. Confounding occurs when you choose a sample for whom data about an important feature of the sample that affects the relationship between the treatment/exposure and the outcome will be unavailable or uncollected. If your analysis does not have data about this feature, then what you learn could be systematically different than the truth.\nBonus: Directed acyclic graphs (DAGs) are a great way to graphically encode relevant information about the casual system, population, and sample from steps 1 and 2. I recommend their development in this step. DAGs can display not just confounding, but also selection bias (and here) and many other design considerations.\n\n\nStep 3. Sample -&gt; Data\n\n\n\nDefinitions\nStep 3: The process of measuring features of the sample (i.e. collect data).\nData: A collection of measurements of variables (features) that describe different interesting aspects of the sample.\n\n\nExplanation\nGoal: The Learner measures values of the variables accurately!\nCaution: Which variables are measured matters both because of the potential for confounding as discussed in step 2, as well as the potential to induce collider bias that could emerge in step 5. How variables are measured also matters. For example, continuous variables contain much more information than binary variables, so if the Learner chooses to classify a continuous variable as a binary variable (e.g. blood pressure as high or low instead of the systolic/diastolic measurements), then the Learner loses information before any analysis has been performed.\nBeware: More crippling problems arise here. Measurement error refers to different ways of inaccurately collecting or recording data (e.g. measuring blood pressure in a screaming 3-year-old will not be accurate). Information biases are a related and overlapping group of problems that occur during the conduct of the study that will make the analysis inaccurate. Incomplete or missing data can ruin an otherwise well designed study. Dichotomania (as discussed in the caution) can be so damaging that it prevent the Learner from answering his or her question when they would have been able to if the variables were measured as continuous or ordinal.\n\n\n\nStep 4. Data -&gt; Analysis\n\n\nDefinitions\nStep 4: The process of wrangling and transforming the raw data into a format that can be fed into an analysis.\nAnalysis: The method that will be used to answer the question (in the next step) comprised of the transformed data, the relationships between the data (e.g. which is a predictor and which is the outcome variable), and the mathematical and algorithmic machinery that will perform the computational operations.\n\n\nExplanation\nGoal: The Learner transforms the data accurately and in a way that maximally preserves information so that it can be analyzed.\nCaution: How data are transformed matters, just like how they are measured matters. If a variable is measured as continuous and than transformed into a binary variable, that will harm the analysis. Also, many errors are induced in this step because of disorganization, clumsy human hands, and non-reproducible data management. Reproducible research methods are very practical and very valuable for preventing these errors.\nBeware: Dichotomania can crop up again here! Also, if you happen upon an interesting data set, it is very tempting to start at this step. This is a fatal mistake. Many of the concerns we’ve discussed (e.g. selection bias, confounding, measurement error) occur before or during data generation. Steps 1-3 always proceed step 4, no matter how big is your data or how fancy is your machine learning technique, and if you are unsure of the quality of steps 1-3, consider step 4 fatally compromised.\n\n\n\nStep 5. Analysis -&gt; Inference\n\n\nDefinitions\nStep 5: The process of conducting the analysis and answering the question using the data collected from the sample.\nInference or Prediction: Inference is the result of using data from the sample to learn about the population or system of interest. Prediction is using the data from the sample to understand what may be true about the population in the future. These are generally perceived as separate but related learning tasks, and the Learner’s goal may only be one or the other.\n\n\nExplanation\nGoal: The Learner conducts the analysis to produce inferences or predictions that he or she will use to answer the question.\nCaution: This is where technical statistical and mathematical considerations dominate. Whole PhD theses are written over very specific technical details in this step. Let the Learner be careful and seek appropriate technical support.\nBeware: Due to the technical nature of this step, many sneaky risks lurk here. Model assumptions, model specification, and model selection refer to the process of developing statistical models that reflect in math the most important concepts required to answer the question. The bias-variance trade off is an important decision about how flexible a model should be to reflect the observed data. P-hacking and researchers degrees of freedom/garden of forking paths are all non-mathematical methods of massaging the data (intentionally or unintentionally) to get an interesting or publishable result (here is a blog post that discusses many of the preceding points). Also this is where you can induce bias by adding variables into your analysis, called collider bias(are you frightened?). Analysis workflows (Bayesian workflows here, and here, RMS workflow - chapter 4, particularly 4.12 - and here) can help guide you safely through this treacherous valley of death and deceit.\n\n\n\nStep 6. Inference -&gt; Decision\n\n\nDefinitions\nStep 6. The process of using the inference along with other information that the Learner knows to draw conclusions about the question and then decide how to act given what has been learned.\nDecision & Action: These are the decisions and actions that result from what was learned. These are the answers to the questions “now what do I do with what I learned?”\n\n\nExplanation\nGoal: The Learner considers the inferences or predictions that result from the analysis against other information that the Learner knows (i.e. the results of many other epistemic arcs) and against the costs and benefits of different outcomes of decisions to decide what to is the best course of action based on the new knowledge.\nCaution: Inferences and predictions are not decisions. These are very different concepts that are often conflated together. Their conflation leaves out two crucial elements. One is that that you, the Leaner, bring many other learned things to this step (I conceptualize it as the intersection of many intersecting epistemic arcs). This additional information is relevant for decision making. The second is that costs and benefits from outcomes of decisions are nowhere reflected in this learning process (they also require their own epistemic arcs!; this blog gives a helpful primer on decision curve analysis).\nBeware: This is where human psychology with all of its frailty and biases can dominate. We all can fall prey to confirmation bias and many other cognitive biases, to motivated reasoning, and to dishonest reasoning. The consequence of us being an inseparable part of the learning process is that we bring our psychological and moral baggage along with us :/"
  },
  {
    "objectID": "docs/posts/2022-06-04-epistemic-arc-intro/index.en.html#conclusion",
    "href": "docs/posts/2022-06-04-epistemic-arc-intro/index.en.html#conclusion",
    "title": "The Epistemic Arc - a conceptual map about how to learn from data in the presence of uncertainty",
    "section": "Conclusion",
    "text": "Conclusion\n\nStep by step, we’ve walked along the Epistemic Arc to learn from data in the presence of uncertainty. Along the way, we’ve noted some areas where we must use caution and many explosive areas we must navigate with extreme care. We’ve also seen how the Learner is not only the primary agent doing the learning, but also a necessary actor in the learning process. The Epistemic Arc unifies these disparate philosophical, psychological, and statistical concepts into a single framework that we can use as a roadmap to conduct principled data analysis and avoid producing noise and nonsense that masquerade as “insights”."
  },
  {
    "objectID": "docs/posts/2022-03-20-quality-control-gh/index.en.html",
    "href": "docs/posts/2022-03-20-quality-control-gh/index.en.html",
    "title": "Three essential quality control practices to improve the reliability of Data Science in Global Health",
    "section": "",
    "text": "Quality control (QC) of data is a grind, particularly for healthcare facilities with paper charts requiring manual collection. QC takes dedicated effort sustained over the life of the project, which may extend indefinitely for data that inform the operations of a health facility.\nQC practices assure the accuracy of the data and are absolutely essential to produce useful analyses from your data science efforts. After all, data science has a garbage problem, and besides designing a data management system to collect beautiful data, QC is the best way to keep your data useful and trustworthy.\nHere are three essential practices that every organizations that uses data to improve care should adopt."
  },
  {
    "objectID": "docs/posts/2022-03-20-quality-control-gh/index.en.html#practice-1-enforced-data-conventions-and-automated-logic-checks",
    "href": "docs/posts/2022-03-20-quality-control-gh/index.en.html#practice-1-enforced-data-conventions-and-automated-logic-checks",
    "title": "Three essential quality control practices to improve the reliability of Data Science in Global Health",
    "section": "Practice 1: Enforced data conventions and automated logic checks",
    "text": "Practice 1: Enforced data conventions and automated logic checks\nPut your software to work. Control the variable types and formats as much as possible. Enforce a common date format for all date variables, have discrete answer choices as often as possible, do your best to avoid free text fields. For numeric data, enforce low and high ranges to avoid mistypes (e.g. age should be between 0.0001 - 100 years). Use the internal logic of the variables to have sanity checks, such as checking that people are not listed as dead before they are born. Your data storage software should be able to handle this, and if it can’t then you need to get new software!"
  },
  {
    "objectID": "docs/posts/2022-03-20-quality-control-gh/index.en.html#practice-2-conduct-frequent-multidisciplinary-qc-reviews",
    "href": "docs/posts/2022-03-20-quality-control-gh/index.en.html#practice-2-conduct-frequent-multidisciplinary-qc-reviews",
    "title": "Three essential quality control practices to improve the reliability of Data Science in Global Health",
    "section": "Practice 2: Conduct frequent multidisciplinary QC reviews",
    "text": "Practice 2: Conduct frequent multidisciplinary QC reviews\nThe reviews should include at the very least a person who collected the data, a clinical person (for clinical data; pharmacist for pharmacy data, etc.), and an analyst who will be using the data to answer key questions. Reviewing the data for all new patients (or other observational units) is ideal. If there are too many data points, then reviewing key variables (e.g. diagnoses) and visually checking for inconsistencies (e.g. treatments are appropriate for the recorded diagnoses) should be the goal of the reviews. Errors and systematic inconsistencies should be addressed with the folks collecting the data. The frequency of the review depends on the volume, but every other week to monthly is a good starting place."
  },
  {
    "objectID": "docs/posts/2022-03-20-quality-control-gh/index.en.html#practice-3-periodic-source-material-biopsies",
    "href": "docs/posts/2022-03-20-quality-control-gh/index.en.html#practice-3-periodic-source-material-biopsies",
    "title": "Three essential quality control practices to improve the reliability of Data Science in Global Health",
    "section": "Practice 3: Periodic source material biopsies",
    "text": "Practice 3: Periodic source material biopsies\nData conventions, logic checks, and QC reviews are very good at catching error patterns, but they do not assess the accuracy of the data extracted from the source material. To do that, you will need to conduct periodic reviews of the charts. Sample a percentage of the charts, perhaps 10%, and have someone other than the person who collected the data review them for accuracy. If there are systematic errors, it will be worth expanding the biopsied number to assess the extent of the errors and the conditions under which they occur. This can be labor-intensive, so consider doing this quarterly or every 6 months.\nWith these three practices, you can overcome the QC grind and build a data pipeline that produces high-quality data that your organization can use to improve the lives of the patients you serve."
  },
  {
    "objectID": "docs/posts/2022-04-05-htma-3-definition-of-risk/index.en.html",
    "href": "docs/posts/2022-04-05-htma-3-definition-of-risk/index.en.html",
    "title": "How to measure anything in Global Health: 3. The Definition of Risk",
    "section": "",
    "text": "What are the chances that survival from a cancer decrease after introducing a new treatment in your clinic?\n\nWhen trying to measure anything in global health, we need to understand the definition of risk.\nSuppose you want to improve the outcome of Burkitt lymphoma (BL) using drug X. This drug is well researched in the literature and has been proven to increase survival in high-income countries. You also know a side effect of drug X causes patients to be at risk for severe infections. Clinics in resource-limited settings may have difficulty providing the same level of supportive care as in high-income countries, and mortality may increase due to the risk of infection.\nIs there a way to a priori estimate the likelihood that drug X actually decreases survival due to the toxicity it causes?\nWe will work through this problem in the next few essays. Today, we need to more precisely define the risk of introducing drug X. These definitions come from Douglas Hubbard’s amazing book, How to Measure Anything. It is worth your time to read if this subject interests you.\nFirst, we will define uncertainty, then risk.\n\n\nUncertainty: The existence of more than one possibility, and the true outcome/state/result/value is unknown.\nFor our example, we are uncertain because the drug either does or does not improve survival.\n\n\nRisk: A state of uncertainty where some of the possibilities involve a loss, catastrophe, or other undesirable outcome.\nThe important part of this definition is that it requires a way to attach values to outcomes. Outcomes with positive values are commonly called benefits, and with negative values, risks.\nTo make decisions in the face of uncertainty and risk, we attach probabilities to the possibilities of our uncertain value so that we have a measure of uncertainty. The total probability of the outcomes with negative values are the measure of risk.\nFor example, we might think there is anywhere between 1%-50% chance that patients experience severe toxicities from drug X. This is highly uncertain.\nMeasurements will improve our decision by reducing our uncertainty about the probability of risk or harm from drug X. For example, conducting a small pilot study of 40 patients demonstrates the incidence of severe toxicities is around 5%.\nIt is nearly certain that 5% will not be the incidence of severe toxicities in all the patients, but this is a very useful measurement that can help you narrow your assessment of the true chance of severe toxicity to be between, say 2%-10%. This much narrower range will aid you in making an evidence-based decision about whether the risks outweigh the benefits.\nThere is much more to the process, but this is a good demonstration of the fundamental purpose of measuring uncertainty and risk:\n\n\nGood measurements make you less wrong about important questions."
  },
  {
    "objectID": "docs/research/index.html",
    "href": "docs/research/index.html",
    "title": "Research Projects",
    "section": "",
    "text": "TL;DR: We modeled future von Willebrand Factor (VWF) levels based on previous VWF levels and other covariates to describe a decision threshold for retesting for von Willebrand disease (VWD) in adolescent females. We found that for an initial level of 80%, there was around an 8% chance the patient could still be found to have VWD, and at an initial level of 90%, there was around a 3% chance of VWD on retesting. These findings can help physicians decide who and when to retest for VWD. I made the above graph and find it quite pleasing.\nCitation: Cohen CT, Zobeck M*, Powers JM. Initial von Willebrand factor antigen values in adolescent females predict future values. Haemophilia 2023;29:1547–55. https://doi.org/10.1111/hae.14865. (*I was co-first author)\n\n\n\n\n\n\n\n\n\nTL;DR: We described single nucleotide polymorphisms that were associated with creatinine increased after high-dose methotrexate or with delayed methotrexate clearance. Some of these associations had not been previously described, others replicated previous findings (a rarity in the pharmacogenomics literature!). This project advance our understanding of who is at most risk for methotrexate toxicity and will hopefully help with the development of strategies to avoid toxicities before they develop. I made the above graphics for the paper and really like style.\nCitation: Zobeck M, Bernhardt MB, Kamdar KY, Rabin KR, Lupo PJ, Scheurer ME. Novel and replicated clinical and genetic risk factors for toxicity from high-dose methotrexate in pediatric acute lymphoblastic leukemia. Pharmacotherapy: The Journal of Human Pharmacology and Drug Therapy 2023;43:205–14. https://doi.org/10.1002/phar.2779.\n\n\n\n\n\n\n\n\n\nTL;DR: We described single nucleotide polymorphisms that were associated with nephrotoxicty from high-dose methotrexate that was severe enough to require glucarpidase, an extremely expensive drug that helps us avoid kidney failure from methotrexate. We found a novel association between a variant in the ABCC4 gene and requiring glucarpidase (this association was reproduced in the other methotrexate study above). We also found that Hispanic patients are at much higher risk for requiring glucarpidase compared to non-Hispanic patients, an association we are still trying to unravel.\nCitation: Zobeck M, Bernhardt MB, Kamdar KY, Rabin KR, Lupo PJ, Scheurer ME. Novel risk factors for glucarpidase use in pediatric acute lymphoblastic leukemia: Hispanic ethnicity, age, and the ABCC4 gene. Pediatric Blood & Cancer 2021;68:e29036. https://doi.org/10.1002/pbc.29036."
  },
  {
    "objectID": "docs/research/index.html#spotlight-publications",
    "href": "docs/research/index.html#spotlight-publications",
    "title": "Research Projects",
    "section": "",
    "text": "TL;DR: We modeled future von Willebrand Factor (VWF) levels based on previous VWF levels and other covariates to describe a decision threshold for retesting for von Willebrand disease (VWD) in adolescent females. We found that for an initial level of 80%, there was around an 8% chance the patient could still be found to have VWD, and at an initial level of 90%, there was around a 3% chance of VWD on retesting. These findings can help physicians decide who and when to retest for VWD. I made the above graph and find it quite pleasing.\nCitation: Cohen CT, Zobeck M*, Powers JM. Initial von Willebrand factor antigen values in adolescent females predict future values. Haemophilia 2023;29:1547–55. https://doi.org/10.1111/hae.14865. (*I was co-first author)\n\n\n\n\n\n\n\n\n\nTL;DR: We described single nucleotide polymorphisms that were associated with creatinine increased after high-dose methotrexate or with delayed methotrexate clearance. Some of these associations had not been previously described, others replicated previous findings (a rarity in the pharmacogenomics literature!). This project advance our understanding of who is at most risk for methotrexate toxicity and will hopefully help with the development of strategies to avoid toxicities before they develop. I made the above graphics for the paper and really like style.\nCitation: Zobeck M, Bernhardt MB, Kamdar KY, Rabin KR, Lupo PJ, Scheurer ME. Novel and replicated clinical and genetic risk factors for toxicity from high-dose methotrexate in pediatric acute lymphoblastic leukemia. Pharmacotherapy: The Journal of Human Pharmacology and Drug Therapy 2023;43:205–14. https://doi.org/10.1002/phar.2779.\n\n\n\n\n\n\n\n\n\nTL;DR: We described single nucleotide polymorphisms that were associated with nephrotoxicty from high-dose methotrexate that was severe enough to require glucarpidase, an extremely expensive drug that helps us avoid kidney failure from methotrexate. We found a novel association between a variant in the ABCC4 gene and requiring glucarpidase (this association was reproduced in the other methotrexate study above). We also found that Hispanic patients are at much higher risk for requiring glucarpidase compared to non-Hispanic patients, an association we are still trying to unravel.\nCitation: Zobeck M, Bernhardt MB, Kamdar KY, Rabin KR, Lupo PJ, Scheurer ME. Novel risk factors for glucarpidase use in pediatric acute lymphoblastic leukemia: Hispanic ethnicity, age, and the ABCC4 gene. Pediatric Blood & Cancer 2021;68:e29036. https://doi.org/10.1002/pbc.29036."
  },
  {
    "objectID": "docs/research/index.html#other-publications",
    "href": "docs/research/index.html#other-publications",
    "title": "Research Projects",
    "section": "Other Publications",
    "text": "Other Publications\nFor most of these, I served as the primary analyst who conducted the statistical analysis and helped to interpret the results.\n\nMcEvoy MT, Siegel DA, Dai S, Okcu MF, Zobeck M, Venkatramani R, et al. Pediatric rhabdomyosarcoma incidence and survival in the United States: An assessment of 5656 cases, 2001–2017. Cancer Medicine 2023;12:3644–56. https://doi.org/10.1002/cam4.5211.\nHolmes DM, Matatiyo A, Mpasa A, Huibers MHW, Manda G, Tomoka T, et al. Outcomes of Wilms tumor therapy in Lilongwe, Malawi, 2016–2021: Successes and ongoing research priorities. Pediatric Blood & Cancer 2023;70:e30242. https://doi.org/10.1002/pbc.30242.\nHarris RD, Bernhardt MB, Zobeck M, Taylor OA, Gramatges MM, Schafer ES, et al. Ethnic-specific predictors of neurotoxicity among patients with pediatric acute lymphoblastic leukemia after high-dose methotrexate. Cancer 2023;129:1287–94. https://doi.org/10.1002/cncr.34646.\nCohen CT, Zobeck M, Kim TO, Sartain SE, Raffini L, Srivaths L. Adolescent acquired thrombotic thrombocytopenic purpura: An analysis of the Pediatric Health Information System database. Thrombosis Research 2023;222:63–7. https://doi.org/10.1016/j.thromres.2022.12.011.\nCohen CT, Zobeck M, Han H, Spinner JA, Powers JM, Lee-Kim Y, et al. Bleeding outcomes and management of supratherapeutic episodes secondary to warfarin in children: A single center 10-year experience. Thrombosis Research 2023;228:148–50. https://doi.org/10.1016/j.thromres.2023.06.013.\nPrudowsky ZD, Bledsaw K, Staton S, Zobeck M, DeJean J, Johnson-Bishop L, et al. Chlorhexidine gluconate (CHG) foam improves adherence, satisfaction, and maintains central line associated infection rates compared to CHG wipes in pediatric hematology-oncology and bone marrow transplant patients. Pediatric Hematology and Oncology 2022;0:1–13. https://doi.org/10.1080/08880018.2022.2090644.\nHoward TS, Valdes SO, Zobeck M, Lam WW, Miyake CY, Rochelson E, et al. Ripple mapping: A precise tool for atrioventricular nodal reentrant tachycardia ablation. Journal of Cardiovascular Electrophysiology 2022;33:1183–9. https://doi.org/10.1111/jce.15491.\nAgarwal S, Cohen CT, Zobeck M, Jacobi PM, Sartain SE. Downregulation of thrombomodulin-thrombin-activated protein C pathway as a mechanism for SARS-CoV-2 induced endotheliopathy and microvascular thrombosis. Thrombosis Update 2022;8:100116. https://doi.org/10.1016/j.tru.2022.100116.\nMullikin D, Ranch D, Khalfe Y, Lucari B, Zobeck M, Assanasen C, et al. Hispanic ethnicity is associated with prolonged clearance of high dose methotrexate and severe nephrotoxicity in children and adolescents with acute lymphoblastic leukemia. Leukemia & Lymphoma 2020;61:2771–4. https://doi.org/10.1080/10428194.2020.1783445.\nKnadler JJ, Zobeck M, Masand P, Sartain S, Kyle WB. In Utero Aortic Arch Thrombosis Masquerading as Interrupted Aortic Arch: A Case Report and Review of the Literature. Pediatr Cardiol 2019;40:658–63. https://doi.org/10.1007/s00246-019-02068-5."
  },
  {
    "objectID": "docs/posts/2022-04-06-htma-4-calibrate-your-judgement/index.en.html",
    "href": "docs/posts/2022-04-06-htma-4-calibrate-your-judgement/index.en.html",
    "title": "How to measure anything in Global Health: 4. Calibrate your judgment",
    "section": "",
    "text": "Your personal calibration is one of the most important measurement skills you’ve never heard of.\nDifficult measurement problems in Global Health require experts to provide an initial estimate of uncertainty about the variables in the process that produce an outcome. For instance, suppose to want to use a new treatment, drug X, for a cancer, but we are concerned the drug will cause harm through serious side effects. As a first step to measure this concern, an expert should estimate her 90% uncertainty interval of the probability that the drug causes harm.\nWhat in the world is an uncertainty interval, and how will we know if the expert’s estimates are any good?\nLike the last few posts, this content is from the delightful book How to Measure Anything by Douglas Hubbard.\n\nUncertainty intervals are ranges of estimated variable values\nThese ranges should capture all of the plausible values that a particular variable may have. If we say it is a 90% uncertainty interval, then the range of values should be wide enough so that if 100 such intervals were constructed, 90 of them would contain the correct value. These intervals should capture the expert’s uncertainty about the value while still signifying the range of plausible values the variable may take.\nFor the example with drug X, our expert may use her clinical experience with the drug in other contexts to estimate the rate of rare but serious side effects is between 1 per 100 doses to 10 per 100 doses.\nGreat, we have an interval, but how do we know if it is truly a 90% uncertainty interval?\n\n\nExperts’ judgment should be calibrated\nCalibration means that the uncertainty interval performs as expected.\nCalibrated 70% uncertainty intervals mean that truly 7 in 10 such intervals contain the true value. If 4/10 intervals contained the true value, then the expert is overconfident. If 9/10 intervals contain the true values, then the expert might be underconfident.\nSimilarly, for true/false questions, calibration can be estimated by assigning your subjective probability that your answer choice is correct. If you answer questions that you are 60% confident in, that means you believe you would answer 6 in 10 such questions correctly. If you answer 4/10 such questions correctly, you’re overconfident, if you answer 9/10 correctly, you’re underconfidence.\n\n\nCalibrate yourself\nHow do you calibrate yourself?\nTest your calibration. No literally, I mean answer questions and produce 90% uncertainty intervals or estimate your confidence in your true/false answers and see how you do.\nHere are two questions from HTMA to get you started:\n\nAssing a 90% uncertainty interval: How many inches long is the average business card?\nIndicate true or false and the probability you’re correct: A gallon of oil weighs less than a gallon of water.\n\nRandom questions, but if you think through what you already know, you can come up with a range of plausible values or a probabilistic estimate of your confidence.\nThese are just example questions. At HTMA’s website, you can find several tests of both types of questions and their answers so that you can assess your own calibration!\nThere is a lot more to this story and calibration can certainly go wrong. We will discuss more in future posts, but for now, I encourage you to go take a test and assess your own calibration.\nI’ll admit, I was woefully overconfident on my first go around.\nBut that is why we calibrate!"
  },
  {
    "objectID": "docs/posts/2022-03-25-stochastic-beasts-1/index.en.html",
    "href": "docs/posts/2022-03-25-stochastic-beasts-1/index.en.html",
    "title": "Stochastic beasts and how to slay them - Part 1",
    "section": "",
    "text": "What do the Balrogs, huge earthquakes, and nuclear war have in common?\nThey are all unpredictable demons that emerge from the abyss to destroy your future. While Balrogs may be mythical beasts from Middle Earth, earthquakes and nuclear war are very real threats.\nIn my last post, I discussed the two types of uncertainty, aleatory and epistemic. I described them with nice and tidy examples, consistent with how they are usually presented in classroom settings. These tame illustrations were a good start to illustrate the concepts so that you can recognize them when you come across them.\nUnfortunately, real life is not tame. There are radical forms of both aleatory and epistemic uncertainty. These are events that are completely unpredictable, although in slightly different ways.\nIt is good to understand these forms of uncertainty so that you can face them confidently when they come.\nIn this post, we will discuss slaying aleatory beasts and in the next post cover epistemic monsters."
  },
  {
    "objectID": "docs/posts/2022-03-25-stochastic-beasts-1/index.en.html#radical-aleatory-uncertainty",
    "href": "docs/posts/2022-03-25-stochastic-beasts-1/index.en.html#radical-aleatory-uncertainty",
    "title": "Stochastic beasts and how to slay them - Part 1",
    "section": "Radical aleatory uncertainty",
    "text": "Radical aleatory uncertainty\nThe frequency of earthquakes follows an inverse power law as a function of its strength. In other words, stronger earthquakes happen much less frequently than weaker earthquakes. This means you can live in faulty California your whole life and never experience a strong earthquake, although you will probably experience many small and medium-size earthquakes in that time.\nThis is radical aleatory uncertainty: big events that happen infrequently but conform to mathematical laws. There are underlying physical reasons why earthquakes conform to an inverse power law, which means we can make scientific models that describe their occurrence. Their infrequency also makes them unpredictable, and therein lies the problem.\nRadical aleatory uncertainty can lull you to sleep. It’s easy to think that because you’ve done fine in small earthquakes, you’ll be fine in a big one, but those are two completely different beasts. It’s tempting to save money by building less resilient housing when an earthquake hasn’t knocked anything down in a while.\nAlso, you can take a false sense of security from the fact that we can model the earthquake scientifically, therefore we must have some special insight into how to predict them. Describing their frequency and predicting the next occurrence are two wildly different activities, and we cannot do the latter."
  },
  {
    "objectID": "docs/posts/2022-03-25-stochastic-beasts-1/index.en.html#slaying-aleatory-beasts",
    "href": "docs/posts/2022-03-25-stochastic-beasts-1/index.en.html#slaying-aleatory-beasts",
    "title": "Stochastic beasts and how to slay them - Part 1",
    "section": "Slaying aleatory beasts",
    "text": "Slaying aleatory beasts\nThere is nothing we can do about earthquakes. We aren’t going to geoengineer them away. This applies to many things that exhibit radical aleatory uncertainty. So what to do?\nStay awake. The threats are real and describable mathematically, so take reasonable precautions against them. Take out insurance to protect against rare but serious catastrophes. Keep emergency supplies stocked if you live in an area exposed to natural disasters. Hedge risky in investments with safer bets. Do not over-optimize to your present situation but spend a little extra effort to build in robustness. Advocate for policies and vote for politicians that take small-but-consequential risks seriously and who won’t give in to political expediency.\nAlso, don’t let these uncertainties dominate your life. Stay alert to their possibility, but take seriously the fact that they are rare. If you’re thinking about the structural integrity of the restaurant you’re at with your date, then the beast is winning. Aleatory anxiety is a very real threat to your well-being. To ward it off, I find it helpful to understand the beast in its rare-but-serious mathematical context.\nThese stochastic beasts are hard to slay, there is no doubt about it. The experience of radical uncertainty is a fundamental part of what makes us human. It may not always be fun to face them, but it is possible to live confidently in the knowledge that you can handle the aleatory beasts come your way."
  },
  {
    "objectID": "docs/posts/2022-03-17-data-science-garbage/index.en.html",
    "href": "docs/posts/2022-03-17-data-science-garbage/index.en.html",
    "title": "Data Science has a garbage problem",
    "section": "",
    "text": "Data science has a data problem.\nThe vast edifice of technologies, visualizations, statistics, machine learning algorithms that make up the data science empire are built on a foundation that can either be as solid as a rock or as unstable as shifting sand."
  },
  {
    "objectID": "docs/posts/2022-03-17-data-science-garbage/index.en.html#the-foundation-of-all-of-data-science-is-data-quality",
    "href": "docs/posts/2022-03-17-data-science-garbage/index.en.html#the-foundation-of-all-of-data-science-is-data-quality",
    "title": "Data Science has a garbage problem",
    "section": "The foundation of all of data science is data quality",
    "text": "The foundation of all of data science is data quality\nIf your data are garbage, then your analyses may as well be toxic waste. You are better off hiring a hard Sci-Fi novelist to imagine what insights you might have gotten from possible analyses than performing any of your own. You can literally produce negative information (information that moves you further from your goal) using bad data."
  },
  {
    "objectID": "docs/posts/2022-03-17-data-science-garbage/index.en.html#garbage-data-is-a-big-problem-in-global-health",
    "href": "docs/posts/2022-03-17-data-science-garbage/index.en.html#garbage-data-is-a-big-problem-in-global-health",
    "title": "Data Science has a garbage problem",
    "section": "Garbage data is a big problem in Global Health",
    "text": "Garbage data is a big problem in Global Health\nInformation technologies are limited or nonexistent, and data collection is performed manually. There may be limited time and resources to dedicate toward the inglorious task of data management. Leadership may not understand the value of clean data and may encourage shortcuts that compromise quality. To put it bluntly, data management is boring, hard to do well, and just not sexy compared to the rest of data science.\nThis is a problem when resources and time are scarce because the wasted effort to collect garbage data is effort that could have been spent on something more valuable."
  },
  {
    "objectID": "docs/posts/2022-03-17-data-science-garbage/index.en.html#garbage-data-today-will-poison-data-science-tomorrow",
    "href": "docs/posts/2022-03-17-data-science-garbage/index.en.html#garbage-data-today-will-poison-data-science-tomorrow",
    "title": "Data Science has a garbage problem",
    "section": "Garbage data today will poison data science tomorrow",
    "text": "Garbage data today will poison data science tomorrow\nAs bad analyses accrue over time, a perception will emerge in the organization that data collection is administrative busywork, and the data are untrustworthy and useless for practical applications. This will make future efforts toward data science even more difficult."
  },
  {
    "objectID": "docs/posts/2022-03-17-data-science-garbage/index.en.html#the-good-news-is-that-garbage-data-can-be-cleaned-up",
    "href": "docs/posts/2022-03-17-data-science-garbage/index.en.html#the-good-news-is-that-garbage-data-can-be-cleaned-up",
    "title": "Data Science has a garbage problem",
    "section": "The good news is that garbage data can be cleaned up!",
    "text": "The good news is that garbage data can be cleaned up!\nWith a little effort, your organization can collect beautiful, high-quality data! Thoughtful data management designed to work within the constraints of the organization’s data collection infrastructure is key. Even if time and resources are limited, you can still ask and answer incredibly important questions from the data in a way that is useful for practice application and builds trust among people who lost confidence in data-driven insights.\nIn my next post, I’ll give you a few key steps to collect and maintain beautiful, gloriously-insightful data."
  },
  {
    "objectID": "docs/posts/2022-03-18-unsexy-data-management/index.en.html",
    "href": "docs/posts/2022-03-18-unsexy-data-management/index.en.html",
    "title": "How to walk the unsexy path toward sophisticated Data Science in Global Health: Data Management",
    "section": "",
    "text": "Data science has a data problem.\nIn my last essay, I argued that because data management is boring, hard to do well, and just not sexy compared to the rest of data science, garbage data is a big problem in global health.\nThe good news is that with a little effort your organization can collect beautiful, high-quality data! Here are 5 steps to design an optimal system for low-resource settings, where data are manually collected from paper charts by limited numbers of people.\n\nStep 1: Identify the key questions your data need to answer\nQuestions are the shining light at the end of the data management path. They should guide you every step of the way. Let the questions come from the people who will use your insights. Talk to your leadership, healthcare providers, program staff, and even patients. Even if you only answer one question, if it is valuable enough to the organization then the cost required to answer it will be worth its weight in gold. Find the right question to answer.\n\n\nStep 2: Develop a framework to understand how to answer the question\nYour team needs to understand the various factors that influence the question you are trying to answer. There are several types of frameworks that can be helpful. Google “conceptual framework for monitoring and evaluation”, “results framework”, or “logic models” to learn more. Content experts should be intimately involved in the construction of the framework.\n\n\nStep 3 - KEY STEP!: Select the right variables to answer the question\nThis step is crucial for your success. From the framework, you should understand which variables will be important to answer the question. If you have limited resources to collect data, then you need to be extremely choosy about which variables to collect. Here are four points of advice for variable choice:\n\nKeep the variables per patient limited. An extremely rough guestimate is 20-30 data points per patient x 15 patients per week is probably the max for one person, although this can vary widely depending on the complexity of the data, how well the data are stored in the chart, and how easy the data are to enter into your database.\nChoose high signal, low noise variables. Dates are great. Easy to define, low measurement error, and chock full of information so that they can be combined in a lot of ways to give precise time spans. Things that are easy to identify and count are also nice (new cancer patients, easy to count; number of new pneumonia diagnoses each week, hard to count).\nFind the right level of specificity. If your data are too high level, they won’t give a useful answer to your question. If they are too granular, the complexity will grind down the quality over time unless you have a lot of help. It is better to have a few accurate data points than a lot of garbage.\nBeware dynamic variables. Variables that change over time are a different animal to collect than variables you can collect once and store forever. Patient outcomes are a necessary dynamic variable that is worth collecting. Collect more at your own risk.\n\n\n\nStep 4: Make a data dictionary and share it widely\nAs precisely as you are able, define each variable. This is important especially when nonclinical personnel are collecting data. Precise definitions cut down on measurement error, which will make your analyses more accurate and precise. Provide standard answer choices as much as possible. Enforce a data convention. I recommend YYYY-MM-DD for all dates.\n\n\nStep 5: Create a database. Avoid spreadsheets.\nSpreadsheets always seem like a good idea at first, but they have too many degrees of freedom and are highly likely to mutate over a year or two of use. Investigate Microsoft Access, REDCap, or DHIS2 to see if they meet your needs. The latter two are free and amazing. There are other software solutions out there as well. Make sure you have API access (your future sophisticated data science self will thank you). It is worth your time and effort to learn to use one and integrate it into your organization’s operations.\nEach of these steps are powerful and incredibly effective in isolation, but together they will produce beautiful, high-quality data that will answer many questions to improve the care you provide to your patients."
  },
  {
    "objectID": "docs/posts/2020-11-27-gccp-1-1/index.en.html",
    "href": "docs/posts/2020-11-27-gccp-1-1/index.en.html",
    "title": "The Global Childhood Cancer Puzzle - Edge 1.1",
    "section": "",
    "text": "The following originally appeared on my podcast about global PHO - Global Health and Childhood Cancer\n\n\nThis is part one of a three part essay about the global burden of childhood cancer. You can find part 2 here and part 3 here.\n\n\nThe Global Oncology Puzzle\nHave you ever completed a very difficult puzzle? Perhaps it was thousands of small pieces with colors that blend together, so that most of the pieces appeared very similar. Do you remember the overwhelmed feeling you had when you first dumped the pieces onto the table? “Good grief”, you may have thought, “this puzzle is impossible.” But, after the moment of defeat, you look at the picture on the box and see what the puzzle could be. Perhaps it will be an amazing waterfall, or city illuminated at night, or a basket full of kittens, and you know that at the end of all your hard work the pieces will come together, and the picture will be complete. So, thinking of that future day when you will feel the satisfying snap of the last piece falling into place, you sit down and get to work.\nFor many people, global pediatric oncology (GPO) is like a giant, overwhelming puzzle. The complexity of the problem and the way the pieces are scattered may leave one feeling lost how to begin. However, despite its difficulty, a community of dedicated doctors, nurses, cancer survivors, parents, politicians, and many others has been working to solve it. Over the last three decades, this community has made considerable progress toward understanding the necessary conditions for successful childhood cancer treatment anywhere in the world. Now, new research has completely changed the perception of childhood cancer as a global health concern.\nAs a result of this new information, the shape of the puzzle is changing. Whereas before, many pieces had been put together, but there was no structure that unified the whole, now most of the edge pieces are in place, and the outline of the puzzle can be seen in its entirety. The edges have given the puzzle a definite structure, which helps the global oncology community understand both how the different completed parts sit in relation to each other and what important information is still missing.\nIn a series of essays, I want to present to you a broad understanding of what I’m calling the edges of the GPO puzzle: those essential concepts without which one cannot fully understand pediatric oncology as a global health concern. Concretely, we will review the emerging research that clarifies the magnitude of the problem of childhood cancer, demonstrates effective solutions exist that can save lives today, quantifies the costs associated with treatment, and charts a clear path forward. To discuss the first edge, we will directly address the question, “what is the global burden of pediatric cancer?”\nMuch of the information in these essays will not be new to people who are involved in the field. Even if he or she hasn’t read the research, much of it is in line with the experience and intuition of the GPO community. If you consider yourself in this group, my hope is that you take from these essays a distilled set of concepts that can serve as visual and viral representations of the field, which you can use to communicate its importance to others.\nAs for the interested but unfamiliar reader, this information may be surprising. When one sits down at the table to work on the puzzle and looks at the chaos of the scattered pieces, it’s easy to be overwhelmed. This feeling may even be accompanied by belief in the futility of completing the puzzle. I have certainly had the thought that the goal of successfully treating children with cancer anywhere in the world is too complex, too big, and too expensive. It also may be hard to see how you can contribute to completing the puzzle. Being overwhelmed at the problem is understandable, which is why I hope to demonstrate that significant progress has already been made and more lives can be saved, through the GPO community’s tireless efforts in caring for kids, an effort in which anyone can take part\nWith that said, let’s go tackle the first edge. I’ll address the rest of the edges in future essays.\n\n\n\n\n\n\n\n\n\n\n\nEdge #1: The Global Burden of Pediatric Cancer\nNo one would deny that childhood cancer is a terrible disease, and people broadly agree that it is horrible that children have to endure it. However, it is also a rare disease, and one may justly wonder how big a problem it is compared to the many other concerning diseases that confront the global health community. This is a reasonable question that serves as the starting place for constructing a more complete picture of GPO. In this visual essay will ask and answer the question what is the total burden of disease due to pediatric cancer?\nRecent research has provided a striking answer to this question, which is: a whole lot more than we previously thought! I’ll explain what this surprising answer means in a minute, but first let me explain why this question is very difficult to answer and why the global cancer community’s understanding of disease burden is changing.\n\n\nHow do you measure cancer burden?\nHow can we answer this question? Should we count the total number of kids diagnosed with cancer who are alive in the world right now? Or maybe we should only count new cases this year? Maybe we should we look at the total number of deaths due to cancer this year? But what about the kids who develop cancer but are never diagnosed, how do we measure those cases if we don’t directly count them? Also, cancer causes much more suffering than just loss of life. Is there a way to factor in the amount of suffering when trying to quantify the burden of disease? You can see from these questions that the first thing researchers must do is to decide what aspect of cancer burden is worth measuring.\nThese questions are very difficult for researchers to answer for a single country, and they are even more complex when considering the entire world. To answer these questions correctly, researchers require a large amount of information. It would be nice if complete data about the number of cancer diagnoses or deaths due to cancer were available, but the unfortunate truth is that there are many holes in the available data. Comprehensive data collection requires a large and expensive system, and there are many countries that are limited in their ability to start and maintain one. Other countries track cases diagnosed at different hospitals, but they do not count every hospital and every patient in the country, so their numbers are also incomplete. To make things even more complicated, researchers know that many cases around the world are not even diagnosed, and if they are to produce reliable numbers about the amount of pediatric cancer in the world, then they must be able to measure the cases they know are happening but never identified.\nFrom these problems, you can see that there are two big reasons why quantifying cancer burden is hard to do: 1. We could measure it in different ways that mean different things. 2. We are missing much of the information we need to make good measurements.\nBecause of these difficulties, estimates of the total burden of childhood cancer have changed over the years. The GPO community has worked hard to improve their methods, and we now have more reliable numbers that tell a very surprising story.\n\n\nWhat is the total burden of disease due to pediatric cancer?\nAs I said above, the answer from our improved estimates is: there is a whole lot more cancer than we previously thought! Let me explain what has changed and why this is so surprising.\n\nThe total number of new cases every year\nDue to the limitations in measuring cancer burden previously described, the best estimates for how many children from the age 0-14 developed cancer each year was 200,000. The technical term for this measure is incident cases. However, recently researchers developed a new method to estimate incident cases and reported that there are in fact 400,000 children who develop cancer every year. In other words, the previous estimates were half as much as the newer estimate (see figure 1)!\n\nFig. 1 - Animated\n\n\n\n\n\n\n\n\n\n\n\nClick for static image\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy are the numbers so different? Part of the reason is the way the researchers calculated the number of cases for countries with missing data by comparing them to the number of cases for countries with really good data differently than previous estimates. Also, with the newer method, researchers were able to better account for the patients who developed cancer but were never diagnosed. Previous methods had trouble estimating this number well, but the new method is much better at it. In fact, the researchers found that about 175,000 kids with cancer are not even diagnosed every year! That means they develop cancer, but either they never seek medical care or the fact that they have cancer is never recognized by healthcare personnel. Another way to look at this number is that more than 4 out of every 10 kids with cancer in the world are never diagnosed.\nThese numbers summarize what we know about the number of new cases per year for the entire world. We can say that the average proportion of kids who are not diagnosed is 4 out of 10. However, that actual proportion of kids who are not diagnosed in a specific country or regions can be very different. For example, the researchers estimated that in western Africa 57% of cases, almost 6 out of every 10 children with cancer, were never diagnosed and in south Asia it was 5 out of 10 kids. Figure 2 shows the estimates of total number of new cases per year by different region and how many cases are diagnosed and undiagnosed.\n\n\nFig. 2 - Animated\n\n\n\n\n\n\n\n\n\n\n\nClick for static image\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs the researchers looked at the trends in proportion of kids who are diagnosed across countries, they noticed that countries that are more economically challenged, labeled as low-income and middle-income countries (LMIC for short), tend to have higher proportions of kids with cancer who are not diagnosed. This is an important fact to notice because 9 out of every 10 kids with cancer in the world live in LMICs! Putting these pieces of evidence together, we can say that the vast majority of children who develop cancer live in countries where they have a high chance of never being diagnosed.\nThe researchers of this paper didn’t stop with these estimates. They calculate that if the rates for non-diagnosis do not change between 2015 and 2030, then about 3 million children will not receive a diagnosis for their life-threatening disease. 3 million children, most with diseases that can be cured, will never have the opportunity to receive treatment, never have the chance at living a long healthy life, and never even know what was breaking their bodies. This is a daily average around 550 nondiagnosed children with cancer. To put this in perspective, if an average school bus holds 55 kids, then this number is the rough equivalent of 10 school buses filled with children driving away and disappearing every single day for 15 years.\n\n\nFig. 3 - Animated\n\n\n\n\n\n\n\n\n\n\n\nClick for static image\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Technical Note: it is worth pointing out that the numbers mentioned above and below are estimates and there is a degree of uncertainty about them that is not represented in the displayed data. The fact of the matter is that the lack of reliable data leaves a lot of questions. For instance, only 5% of kid in Africa live in countries with comprehensive registries. How do we decide what to do with the remaining 95%? The researches mentioned above had a specific modeling strategy to fill in the gaps, but that is not the only way. For instance the International Incidence of Childhood Cancer (IICC3) has a different methodolgy that produces slightly different numbers. For the purposes of this essay it is sufficient to acknowledge these difference, and continue exploring the estimates the Harvard researchers produced.).\n\nContinue reading:\nPart 2\nPart 3\nSubscribe to my podcast about global PHO at GHCCpod.com"
  },
  {
    "objectID": "docs/posts/2022-03-16-use-routine-data/index.en.html",
    "href": "docs/posts/2022-03-16-use-routine-data/index.en.html",
    "title": "10 ways Global Health organizations can use routine data to improve patient care",
    "section": "",
    "text": "Global Health needs Data Science because efficient and effective healthcare delivery is the key to success.\nHere are 10 examples of how Data Science capabilities can improve your organization’s operations and the care you provide to patients.\nAll of these ideas are possible to do well even if you don’t have an electronic medical record. All it takes is careful data collection of a limited number of variables. You don’t need big data to do Data Science successfully in Global Health, but you do need good data."
  },
  {
    "objectID": "docs/posts/2022-03-16-use-routine-data/index.en.html#data-science-global-health-ideas",
    "href": "docs/posts/2022-03-16-use-routine-data/index.en.html#data-science-global-health-ideas",
    "title": "10 ways Global Health organizations can use routine data to improve patient care",
    "section": "Data Science + Global Health Ideas",
    "text": "Data Science + Global Health Ideas\n\nForecast patient volumes, drug requirements, and medical equipment use\nIdentify risk factors for patients who might not return for needed care\nMonitor health outcomes disparities by age, sex, race, religion, etc.\nCreate a real-time dashboard of statistical process control charts\nAnalyze the referral pathways for patients to arrive at the facility\nUse geospatial information to map and monitor disease clusters\nCreate high-level dashboards of key indicators for leadership\nMeasure inpatient census and calculate nurse:patient ratios\nMonitor safety event frequency and flag increases early\nUse algorithms to predict clinical deterioration events"
  },
  {
    "objectID": "docs/posts/2022-03-16-use-routine-data/index.en.html#simple-measurements-create-powerful-insights",
    "href": "docs/posts/2022-03-16-use-routine-data/index.en.html#simple-measurements-create-powerful-insights",
    "title": "10 ways Global Health organizations can use routine data to improve patient care",
    "section": "Simple measurements create powerful insights",
    "text": "Simple measurements create powerful insights\nThere are tremendous opportunities to improve healthcare delivery in resource-constrained settings through the careful collection of a handful of variables. With a little time and diligence, you too can wrangle your data into useful insights about how to improve care for your patients."
  },
  {
    "objectID": "docs/posts/2022-03-13-gh_needs_ds/index.en.html",
    "href": "docs/posts/2022-03-13-gh_needs_ds/index.en.html",
    "title": "Three Reasons Global Health needs Data Science",
    "section": "",
    "text": "Global Health needs Data Science.\nThese two disciplines were made for each other. While each field accomplishes amazing things in isolation, when combined together their complementary strengths unlock progress that was previously impossible.\nI’m using the term “Data Science” because it is flashy enough to catch your eye and broad enough to encompass the many specializations that are the key to success: data collection and management, informatics tools, data engineering, operational analytics, statistics, and machine learning.\nNow, let me give you three reasons why Global Health needs Data Science (and one reason why it’s so hard to do well)."
  },
  {
    "objectID": "docs/posts/2022-03-13-gh_needs_ds/index.en.html#efficient-health-organization-operations-save-lives",
    "href": "docs/posts/2022-03-13-gh_needs_ds/index.en.html#efficient-health-organization-operations-save-lives",
    "title": "Three Reasons Global Health needs Data Science",
    "section": "1. Efficient health organization operations save lives",
    "text": "1. Efficient health organization operations save lives\nIn resource-constrained settings, every resource needs to be used well.\nHealthcare providers need to use their knowledge to care for the patients that most need their expertise, medications need to be given where they are most effective (and before they expire!), respiratory support equipment needs to be available as soon as a patient needs it, and money needs to be spent on what is useful and not wasted on what is useless.\nData science gives organizations the capabilities to use resources well. Many data scientists work in companies where their primary job is to perform an alchemical transformation of data into efficient operational insights using the magic of math. In companies, this translates to increased profits. In global health, this translates to saved lives."
  },
  {
    "objectID": "docs/posts/2022-03-13-gh_needs_ds/index.en.html#learning-health-systems-accelerate-progress",
    "href": "docs/posts/2022-03-13-gh_needs_ds/index.en.html#learning-health-systems-accelerate-progress",
    "title": "Three Reasons Global Health needs Data Science",
    "section": "2. Learning health systems accelerate progress",
    "text": "2. Learning health systems accelerate progress\nTo improve health outcomes, it is key to have a system where healthcare providers, administrators, and others are equipped with the tools and knowledge required to learn about what is not going well and conduct improvement projects to change it\nUnfortunately, this sounds good on paper but is very difficult to do. The reality of working to improve a system that may not want to improve when you are already very busy with other responsibilities means progress is slow and frustrating.\nData Science can help push projects forward even in systems that resist change. Data Science provides the tools to efficiently collect data, accurately measure what matters, and effectively communicate it to people who need to know. These advantages are even more important in resource-constrained settings where time and organizational support are forever in short supply."
  },
  {
    "objectID": "docs/posts/2022-03-13-gh_needs_ds/index.en.html#healthcare-is-an-unstable-complex-adaptive-system",
    "href": "docs/posts/2022-03-13-gh_needs_ds/index.en.html#healthcare-is-an-unstable-complex-adaptive-system",
    "title": "Three Reasons Global Health needs Data Science",
    "section": "3. Healthcare is an unstable complex adaptive system",
    "text": "3. Healthcare is an unstable complex adaptive system\nThere is nothing linear or simple about working in healthcare. The provision of care is complex because it requires many people with specialized knowledge to work together in a system that has the physical infrastructure they need. Treatment is nonlinear because health status can change rapidly or in unexpected ways and providers need to be able to respond accordingly. Improving health systems is difficult because any small perturbation of the system may result in no change at all or in large, unexpected, and sometimes harmful changes.\nThe best way to make decisions in a complex environment is to probe the system (induce small changes), sense the changing state of the system, and then respond to the changes. To do this, decision-makers need the tools to probe-sense-respond efficiently. Data Science equips organizations with tools needed to create this signal network."
  },
  {
    "objectID": "docs/posts/2022-03-13-gh_needs_ds/index.en.html#data-science-capabilities-and-data-scientists-take-a-lot-of-care-and-feeding",
    "href": "docs/posts/2022-03-13-gh_needs_ds/index.en.html#data-science-capabilities-and-data-scientists-take-a-lot-of-care-and-feeding",
    "title": "Three Reasons Global Health needs Data Science",
    "section": "Data Science capabilities (and Data Scientists) take a lot of care and feeding",
    "text": "Data Science capabilities (and Data Scientists) take a lot of care and feeding\nThe reason why high-caliber Data Science is not available in many global health organizations is that it takes a lot of time, money, and investment from leadership to make it successful. Collecting clean data (especially if there is no EMR available) is incredibly hard. Assuring the accuracy of the data is even harder. As data are stored over time, people’s trust in the data wanes because their knowledge of its nuances decays. Data analyses are often done poorly because the devil is in the details. While people pay lip service to the importance of statistics, few have the patience or fortitude to engage the epistemic rodeo that is wrangling and taming uncertainty from observational data. And…machine learning isn’t very useful in Global Health right now. Sorry to be the bearer of bad news."
  },
  {
    "objectID": "docs/posts/2022-03-13-gh_needs_ds/index.en.html#data-science-global-health",
    "href": "docs/posts/2022-03-13-gh_needs_ds/index.en.html#data-science-global-health",
    "title": "Three Reasons Global Health needs Data Science",
    "section": "Data Science + Global Health",
    "text": "Data Science + Global Health\nAlthough it is hard, Data Science can be done well in Global Health. With time, effort, and the right perspective, Global Health organizations can reap the incredible rewards from careful data management and thoughtful analytics.\nOver the next 30 days, I will be writing essays that explore how to best do Data Science in Global Health well. Follow along if you want to learn more and reach out to let me know more about your experiences with Data Science + Global Health!"
  },
  {
    "objectID": "docs/posts/2022-03-21-let-the-data-flow/index.en.html",
    "href": "docs/posts/2022-03-21-let-the-data-flow/index.en.html",
    "title": "Let the data flow: how to move information to the people who need it in a Global Health organization",
    "section": "",
    "text": "If a health facility is adequately staffed with doctors, nurses, and other providers, and it is well supplied with medicines and medical equipment, then it has everything it needs to deliver care effectively, right?\nUnfortunately, this is wrong. If effective healthcare delivery only required sufficient numbers of providers who possess sufficient equipment, then the prescription to include care in resource-constrained settings would be clear: more people and more stuff!\nExamples abound where care is delivered ineffectively despite sufficient people with the right stuff. Patients may not be referred to the correct treatment facility. Doctors may not be able to diagnose patients effectively despite the presence of adequate laboratory and radiographic equipment. Hospital floor staff may not know which patients are sickest and need acute treatment despite vital signs being regularly obtained by nurses. While people are stuff are necessary for effective healthcare delivery, they are not sufficient. There is a missing ingredient that ties all of these examples of dysfunction together."
  },
  {
    "objectID": "docs/posts/2022-03-21-let-the-data-flow/index.en.html#information-flow-is-key-to-effective-healthcare-delivery",
    "href": "docs/posts/2022-03-21-let-the-data-flow/index.en.html#information-flow-is-key-to-effective-healthcare-delivery",
    "title": "Let the data flow: how to move information to the people who need it in a Global Health organization",
    "section": "Information flow is key to effective healthcare delivery",
    "text": "Information flow is key to effective healthcare delivery\nInformation flow means that the right information comes to the right person at the right time so that effective healthcare decisions can be made.\nInformation flow during the referral process means that the clinical information about flows from the patient to the doctor, who in turn delivers the needed referral information to the patient and to the referring center, which can also reach out with visit information to the patient to assure timely arrival.\nTo obtain diagnostic information, doctors need to send patients to the lab where information can flow from the patient to a machine that transforms the results into diagnostic information that is passed back to the doctor.\nTo intervene on an acutely decompensating patient, information about the patient’s health state needs to flow to the nurses who transmit the information to physicians in a timely manner for acute intervention."
  },
  {
    "objectID": "docs/posts/2022-03-21-let-the-data-flow/index.en.html#information-flow-is-like-the-nervous-system-of-the-health-organization",
    "href": "docs/posts/2022-03-21-let-the-data-flow/index.en.html#information-flow-is-like-the-nervous-system-of-the-health-organization",
    "title": "Let the data flow: how to move information to the people who need it in a Global Health organization",
    "section": "Information flow is like the nervous system of the health organization",
    "text": "Information flow is like the nervous system of the health organization\nInformation flow transmits both sensory and motor information throughout the functional units of the healthcare organism. A body can have the healthiest heart possible, but if its actions and outputs cease to be coordinated with the activity of the rest of the organs, then the organism soon falls into a very poor state of health. So too, a health system with well-trained doctors but uncoordinated actions with other providers or with laboratory equipment quickly becomes dysfunctional."
  },
  {
    "objectID": "docs/posts/2022-03-21-let-the-data-flow/index.en.html#information-flow-reframes-healthcare-as-a-dynamic-complex-system",
    "href": "docs/posts/2022-03-21-let-the-data-flow/index.en.html#information-flow-reframes-healthcare-as-a-dynamic-complex-system",
    "title": "Let the data flow: how to move information to the people who need it in a Global Health organization",
    "section": "Information flow reframes healthcare as a dynamic, complex system",
    "text": "Information flow reframes healthcare as a dynamic, complex system\nInformation flows show that healthcare is a dynamic system as the state of each part is constantly changing and reacting to other parts in the system. It also shows that healthcare is a complex system with many interlocking parts causing unexpected outcomes.\nInformation flow shows the connections between the parts of the system. You still may not be able to predict the long-term outcomes of a complex system, but if you can see the information flow, you can see how the system grows and changes over time so that you can slowly shape its evolutionary trajectory."
  },
  {
    "objectID": "docs/posts/2022-03-21-let-the-data-flow/index.en.html#people-and-stuff-are-insufficient-to-deliver-effective-care-if-information-is-not-flowing-freely-in-the-system.",
    "href": "docs/posts/2022-03-21-let-the-data-flow/index.en.html#people-and-stuff-are-insufficient-to-deliver-effective-care-if-information-is-not-flowing-freely-in-the-system.",
    "title": "Let the data flow: how to move information to the people who need it in a Global Health organization",
    "section": "People and stuff are insufficient to deliver effective care if information is not flowing freely in the system.",
    "text": "People and stuff are insufficient to deliver effective care if information is not flowing freely in the system.\nThe goal for health systems to deliver effective care over time for all patients cannot be achieved if information does not flow in the system to coordinate and inform its activities.\nThis is why data science is a vital capability for health systems in low-resource settings. Data science can move information and provide insights as fast as the date are able to be gathered and collected. With commitment and steady effort, it is possible to build the informational nervous system that the healthcare organism needs to survive and thrive."
  },
  {
    "objectID": "docs/posts/2022-03-19-aristotle-and-data-science/index.en.html",
    "href": "docs/posts/2022-03-19-aristotle-and-data-science/index.en.html",
    "title": "Aristotle’s sage advice to improve Data Science in Global Health",
    "section": "",
    "text": "Aristotle, the legendary Greek philosopher, has some advice for anyone trying to use data science in global health.\nHere are 3 reasons why this quote is so insightful."
  },
  {
    "objectID": "docs/posts/2022-03-19-aristotle-and-data-science/index.en.html#reason-1.-the-nature-of-your-subject-determines-how-precisely-you-can-analyze-it",
    "href": "docs/posts/2022-03-19-aristotle-and-data-science/index.en.html#reason-1.-the-nature-of-your-subject-determines-how-precisely-you-can-analyze-it",
    "title": "Aristotle’s sage advice to improve Data Science in Global Health",
    "section": "Reason 1. The nature of your subject determines how precisely you can analyze it",
    "text": "Reason 1. The nature of your subject determines how precisely you can analyze it\nHealthcare delivery in general, and in global health in particular, is hard to measure precisely. If you’ve ever tried to conduct a quality improvement project, you know what I mean. The project starts off trying to improve a health outcome (e.g. “improve asthma control”), but health states are never easily captured and instead a process measure is used (“improved inhaler adherence to improve asthma control”). The further away what you want to measure is from what you can measure, the more imprecise you will be. By itself, this imprecision doesn’t prevent good analyses, but good analyses can only be done if you understand this problem."
  },
  {
    "objectID": "docs/posts/2022-03-19-aristotle-and-data-science/index.en.html#reason-2.-design-analysis-to-accommodate-imprecision",
    "href": "docs/posts/2022-03-19-aristotle-and-data-science/index.en.html#reason-2.-design-analysis-to-accommodate-imprecision",
    "title": "Aristotle’s sage advice to improve Data Science in Global Health",
    "section": "Reason 2. Design analysis to accommodate imprecision",
    "text": "Reason 2. Design analysis to accommodate imprecision\nImprecision is the rule and not the exception, but there are things you can do to combat it. Measure variables as continuously as possible because more information is contained in continuous variables (continuous &gt; ordinal &gt; binary). Make sure you have enough observations to have any hope of answering the question. Learn about “directed acyclic graphs” and use them to identify which variables you need to reduce confounding and increase the precision of your analysis. These will help to maximize both the accuracy and precision of your analysis."
  },
  {
    "objectID": "docs/posts/2022-03-19-aristotle-and-data-science/index.en.html#reason-3.-dont-overfit-your-insights-to-the-imprecise-data",
    "href": "docs/posts/2022-03-19-aristotle-and-data-science/index.en.html#reason-3.-dont-overfit-your-insights-to-the-imprecise-data",
    "title": "Aristotle’s sage advice to improve Data Science in Global Health",
    "section": "Reason 3. Don’t overfit your insights to the imprecise data",
    "text": "Reason 3. Don’t overfit your insights to the imprecise data\nOverfitting is a major problem to avoid not only when developing statistical models, but also with any analysis of imprecise data. For example, a systolic blood pressure in a 2-year-old child may be measured as 140 (super high), but a wise doctor does not react urgently to it because 2-year-olds often scream like the world is ending when the blood pressure cuff squeezes their arm. The wise doctor will understand the context (was the child screaming?) and gather more data (retake the blood pressure when the kid is calm). If the doctor had immediately given medicine to reduce the blood pressure, she would have overfit her response to the data. The same is true for many analyses in global health. The data will be noisy, tune your insights and actions accordingly.\nAristotle may have lived in 350 BC, but his ancient wisdom can inform data science practice today. His insight has endured through the ages because it makes a fundamental point about how we learn from data in the presence of uncertainty. Anyone engaged in learning from data would do well to listen."
  },
  {
    "objectID": "docs/posts/2021-11-20-s-values/index.en.html",
    "href": "docs/posts/2021-11-20-s-values/index.en.html",
    "title": "S-Values - Part 1",
    "section": "",
    "text": "Imaging you find a coin on the ground. Ever the thrifty type, you are delighted to find this treasure, so you pick it up and give it three celebratory flips. You notice it lands on heads after each flip, so you decide to give it one more. It lands on heads again! You do quick mental math and decide that the probability of getting four heads in a row if the coin is fair is \\(\\frac{1}{16}\\). Curious, but not impossible. You give it another flip. Heads. You get more suspicious. Another flip. Heads! Now you’re fairly confident something funny is going on with the coin. You continue to flip and get heads until you feel your level of confidence that you found a trick coin reaches some level of certainty, at which point your confidence grows no further and you begin to expect heads with each flip.\nReady for the big reveal in this story? What you actually encountered while walking down that street is a p-value!\nYes, THAT p-value. The one where we try really hard to have values less than 0.05 so that you we can imbue our research with the authority of statistics and have it published. The reason why the situation doesn’t look like a p-value is because it is mathematically transformed and conceptually situated away from the context where you normally encounter it, in journal articles usually accompanied by stars (***) and hyperbolic causal conclusions (although more research is necessary). The version of the p-value in this story is called an s-value, and it is an incredibly helpful tool to build intuition about what p-values mean. The best part is that s-values will help you more accurately interpret p-values while doing almost no math!"
  },
  {
    "objectID": "docs/posts/2021-11-20-s-values/index.en.html#introduction",
    "href": "docs/posts/2021-11-20-s-values/index.en.html#introduction",
    "title": "S-Values - Part 1",
    "section": "",
    "text": "Imaging you find a coin on the ground. Ever the thrifty type, you are delighted to find this treasure, so you pick it up and give it three celebratory flips. You notice it lands on heads after each flip, so you decide to give it one more. It lands on heads again! You do quick mental math and decide that the probability of getting four heads in a row if the coin is fair is \\(\\frac{1}{16}\\). Curious, but not impossible. You give it another flip. Heads. You get more suspicious. Another flip. Heads! Now you’re fairly confident something funny is going on with the coin. You continue to flip and get heads until you feel your level of confidence that you found a trick coin reaches some level of certainty, at which point your confidence grows no further and you begin to expect heads with each flip.\nReady for the big reveal in this story? What you actually encountered while walking down that street is a p-value!\nYes, THAT p-value. The one where we try really hard to have values less than 0.05 so that you we can imbue our research with the authority of statistics and have it published. The reason why the situation doesn’t look like a p-value is because it is mathematically transformed and conceptually situated away from the context where you normally encounter it, in journal articles usually accompanied by stars (***) and hyperbolic causal conclusions (although more research is necessary). The version of the p-value in this story is called an s-value, and it is an incredibly helpful tool to build intuition about what p-values mean. The best part is that s-values will help you more accurately interpret p-values while doing almost no math!"
  },
  {
    "objectID": "docs/posts/2021-11-20-s-values/index.en.html#s-values",
    "href": "docs/posts/2021-11-20-s-values/index.en.html#s-values",
    "title": "S-Values - Part 1",
    "section": "S-values",
    "text": "S-values\nMathematically an s-value is defined as \\(s = -log_2(p)\\) (the mathematical details of the development of this transformation are outside the scope of this post, but you can see Cole, Edwards, and Greenland (2021) for a more robust discussion). The result of this transformation can be interpreted as saying, “if I were to flip a fair coin s times, I would expect to get heads on all flips with probability p.” For example, if I flip a fair coin and get heads 4 times in a row, that corresponds to a probability of getting that outcome of 0.06 . Another way to phrase this: if I did 100 trials of 4 coin flips, only 6 would result in 4 heads. This transformation also works if we start with probability. If I tell you that I saw someone flip a coin and get heads each time and that the probability of this outcome using a fair coin was only around 0.01, you could plug it into the equation tell me that probability corresponds to roughly 7 heads in a row (with rounding because fraction of coin flips are hard to visualize).\nAn s-value can also be described as a measure of surprise that you feel when flipping a coin that you expect to be fair and seeing it land on heads some number of times in a row. Yes, it is best described as a feeling. Your brain and your body have been tuned to create expectations of outcomes. When an outcome happens that defies your expectations, you feel surprised! The surprise in coin flipping is that you expect some mixture of heads and tails, but when only heads keeps coming up, that violation of your expectation creates a feeling of surprise. Another way to say it is that s-values quantify your feeling of surprise when your data and your expectation about a hypothesis about a binary outcome do not match. The hypothesis in the coin flipping example is that the coin is fair. Amazingly, this scenario can teach us about p-values because feeling surprised at a mismatch between a hypothesis and data applies to any situation where p-values are used to reason about a hypothesis.\n\nExample\nWe will work through an example and then explore some very useful properties of s-values.\nLet’s say we read a journal article where the researchers claimed to have surveyed a random sample of 120 people and found that more people prefer cats to dogs and reported the difference with p= 0.045. Under the Null Hypothesis Significance Testing methods, the p-value is telling you is that if we live in a world where people do not in fact have a preference between cats or dogs, we would expect to see the difference that was reported in this study or a more extreme difference 4.5% of the time. For Reasons, a p-value of less than 0.05 has long been celebrated as the “significance” threshold, which is shorthand for “publishable” and also for “I can extend my life in academia a few more months.”\nS-values can help improve our interpretation of this number and move us away from a threshold for significance/publishable/keeping-my-job. For p=0.045, the corresponding s-value is 4.47 which can round down to 4 . This p-value provides roughly the same amount of surprise against the null hypothesis (no difference between cat and dog preferences) as flipping a fair coin and seeing it land on heads 4 times in a row. Is that a lot of evidence against the null hypothesis? It’s something, but it doesn’t feel like a lot.\n\n\nS-values change slowly until they don’t\nOne of the most important insights from the s-value perspective is that the difference between a “significant” and “non-significant” finding is very small. Suppose in the cat-dog preference study the p=value was 0.055. That corresponds to an s-value of 4.18 , which again rounds to 4. Although there is a small fractional difference in s-values, the intuition is that both p-values of 0.045 and 0.055 are roughly similar to the surprise you feel when flipping a fair coin and seeing heads 4 times in a row. Why then is a threshold of 0.05 special? It’s not. It’s just one value along a continuum of evidence that corresponds to a continuous amount of surprise against the null hypothesis.\nLet’s now consider the possible s-values you could get along the whole range of p-values from 0 to 1. The graph below demonstrates this relationship.\n\n\n\n\n\n\n\n\n\nWe start with p-values on the x-axis, and with a p-value of 1 on the left because it produces an s-value of 0. Arranging the graph this way demonstrates that evidence against the null hypothesis accumulates as p-values gets smaller. The blue line on the chart represents p=0.05. Notice how slowly the s-values rise over the interval from 1 to 0.25. In this range, an absolute difference of 0.75 on the probability scale corresponds to just two coin tosses/s-value units (technically known as bits). Even though the value of the p-value varies quite a bit in this range, there just isn’t much to make you feel surprised about the null hypothesis that is contained in these numbers. On the right side of the graph you can see that evidence accumulates exponentially as the p-value gets smaller. Let’s zoom in on that region in the graph below.\n\n\n\n\n\n\n\n\n\n\n\nHere we’re starting on the left with a p-value of 0.1. I’ve added a blue horizontal line to depict the s-value of 4.32 that corresponds to a p-value of 0.05. Zooming in to this area again shows that s-values increase relatively slowly in the region of 0.05 compared to much smaller values on the right. Below is a table of p/s-values in this region to make the increase clear.\n\n\n\n\n\n\n\n\n\nP-Value\nS-Value\n\n\n\n\n0.10\n3.32\n\n\n0.09\n3.47\n\n\n0.08\n3.64\n\n\n0.07\n3.84\n\n\n0.06\n4.06\n\n\n0.05\n4.32\n\n\n0.04\n4.64\n\n\n0.03\n5.06\n\n\n0.02\n5.64\n\n\n0.01\n6.64\n\n\n\n\n\n\n\n\nThe graph and table demonstrate that a 0.01 decrease in p-values starting from 0.10 still results in a relatively small incremental increase in s-values. Invoking coin flips again, 3.64 heads in a row with a fair coin (p=0.08) and 4.32 heads in a row (p=0.05) just don’t feel very different. They both round to 4 heads in a row. When I said “a relatively small increase”, I meant relative to the scale of meaningful evidence against the null hypothesis. By the time the p-value is 0.01, the number of heads in a row round to 7, which does feel like a much more surprising result compared to 4 heads in a row.\nNotice that as the p-values get smaller, the s-value increase starts to accelerate. Going from p=0.1 to p=0.01 (an absolute difference of 0.09) results in a 3.3 increase in s-values. This is very different than in the range of p-values from 1 to 0.25 (absolute difference of 0.75) that resulted in an increase in s-values of 2. Let’s make this trend clearer in the table below.\n\n\n\n\n\n\n\n\n\nP-Value\nS-Value\n\n\n\n\n0.1\n3.3\n\n\n0.01\n6.6\n\n\n0.001\n10.0\n\n\n0.0001\n13.3\n\n\n0.00001\n16.6\n\n\n\n\n\n\n\n\nThis table demonstrates that with small p-values, even though the absolute differences in p-values is shrinking (for example, the difference between 0.01 and 0.001 is 0.009 ), evidence is accumulating much more rapidly. When deciding about evidence against the null, there is a BIG difference between a p-value of 0.1 and 0.01. There is also a BIG difference between 0.01 and 0.001. Even though both of these numbers feel “small,” 0.001 contains 50% more evidence against the null hypothesis than 0.01. Humans are bad about reasoning with small numbers. Once we get to a certain number of decimals, we throw up our hands and think “what’s the difference between small and smaller?” When talking about feelings of surprise against a null hypothesis, the difference is huge because the information lives in these small values, which are the easiest for us humans to overlook!"
  },
  {
    "objectID": "docs/posts/2021-11-20-s-values/index.en.html#conclusion",
    "href": "docs/posts/2021-11-20-s-values/index.en.html#conclusion",
    "title": "S-Values - Part 1",
    "section": "Conclusion",
    "text": "Conclusion\nThis post defined s-values and discussed four important tasks that they accomplish.\nS-values transform p-values into a measure of surprise against a hypothesis. In this post, we focused primarily on the null hypothesis.\n\nS-values correspond to the psychologically familiar situation of the surprise that you feel when flipping a coin that you think is fair and getting heads a certain number of times in a row. This builds intuition of how p-values provide evidence against the null hypothesis.\nS-values demonstrate that the magnitude of the absolute difference in p-values do not correspond to the difference in s-values across the range of possible p-values.\nS-values increase slowly over the range of p-values that surround the traditional cutoff values of p=0.05, demonstrating that p-value cutoffs are largely arbitrary and values in this range provide similar feelings of surprise against a null hypothesis.\nThe increase in S-values will accelerate as p-values become smaller, demonstrating that information against the hypothesis accumulates in small p-values, contrary to the naive intuition that small absolute differences on the probability scale when the values are small are not overly meaningful.\n\nI hope this discussion is helpful to calibrate your intuition about what p-values signify. See these references below to learn more (Rafi and Greenland 2020; Greenland 2019)."
  },
  {
    "objectID": "docs/posts/2022-04-02-distributed_intelligence/index.en.html",
    "href": "docs/posts/2022-04-02-distributed_intelligence/index.en.html",
    "title": "Distributed intelligence is the key to success for high-performing health systems in Global Health",
    "section": "",
    "text": "Who would you rather have care for you: the world’s smartest doctor in a poorly-function health system or a mediocre doctor in a high-functioning health system?\nTake the high-functioning health system every time.\nGenius doctors are still humans who can move and think only at the speed of a single person.\nEven if the doctor has limitless energy, she can only see so many patients in one day, make so many diagnoses, and fill so many prescriptions. Be ready for the long wait to see her.\nOf course, even the best doctor makes mistakes, but without a system to back her up, there is no safety net when she fails without a system to back her up.\nEvery doctor occasionally needs management advice from colleagues, but being the best of the bunch there will be nowhere to whom she can turn.\nShe needs backup. She needs a system."
  },
  {
    "objectID": "docs/posts/2022-04-02-distributed_intelligence/index.en.html#what-is-distributed-intelligence",
    "href": "docs/posts/2022-04-02-distributed_intelligence/index.en.html#what-is-distributed-intelligence",
    "title": "Distributed intelligence is the key to success for high-performing health systems in Global Health",
    "section": "What is distributed intelligence?",
    "text": "What is distributed intelligence?\nThe International Society of Learning Sciences gives a helpful outline of the concept:\n\nDistributed intelligence means that the resources that enable and mediate activity are distributed in configuration across people, environments, situations, and time.\nIntelligence is assembled and accomplished rather than possessed.\nTherefore, the boundary unit of analysis for learning is different with this orientation since intelligence “comes to life” in human activity.\n\nThis view acknowledges that no one person is smart enough to know everything that needs to be known or can move fast enough to do everything that needs to be done. Instead, each person acts together with others to accomplish the system’s goal.\nIntelligence is found in the motion of the whole system and not its constituent parts. In such systems, information is often distributed in the form of workflows, best practices, reference documents, technologies, teams, and cultures."
  },
  {
    "objectID": "docs/posts/2022-04-02-distributed_intelligence/index.en.html#distributed-intelligence-in-healthcare.",
    "href": "docs/posts/2022-04-02-distributed_intelligence/index.en.html#distributed-intelligence-in-healthcare.",
    "title": "Distributed intelligence is the key to success for high-performing health systems in Global Health",
    "section": "Distributed intelligence in healthcare.",
    "text": "Distributed intelligence in healthcare.\nThe totality of clinical knowledge in existence far exceeds the expertise of any single physician. When a doctor encounters a diagnosis he is unfamiliar with, he must call for help from a specialist.\nEven for specialists, there are times when a diagnosis requires a rarely used therapy, and she must consult institutional guidelines on how to order and administer a specific medicine.\nIf a patient is taking other medications, guidelines may not be sufficient to dose the medicine properly, and a pharmacist must be consulted who references an online interaction database.\nThe patient must be scheduled in an infusion room to receive the medication. The presence of the patient, the medicine, a nurse who can give it, and equipment for the infusion must be coordinated by the facilities staff.\nWhile the patient receives the medication, the nurse must monitor his vital signs and alerts a physician if any measurement exceeds the boundaries derived from research and past clinical experience encoded into clinical guidelines.\nWhere is the intelligence here? It is in the entire description. The patient was diagnosed and received the needed therapy because of the interacting actions between the doctor, specialist, guidelines, pharmacist, database, scheduling system, nurse, and clinical guidelines.\nThe system knew how to diagnose and treat the patient even though no single person did."
  },
  {
    "objectID": "docs/posts/2022-04-02-distributed_intelligence/index.en.html#data-science-distributes-intelligence-in-a-health-system.",
    "href": "docs/posts/2022-04-02-distributed_intelligence/index.en.html#data-science-distributes-intelligence-in-a-health-system.",
    "title": "Distributed intelligence is the key to success for high-performing health systems in Global Health",
    "section": "Data science distributes intelligence in a health system.",
    "text": "Data science distributes intelligence in a health system.\nAs I have written previously, data science is essential in global health because it allows information to flow where it is needed in the system, unlocking its intelligence and allowing patients to receive the care they require."
  },
  {
    "objectID": "docs/posts/2022-03-22-data-science-and-the-epistemic-rodeo/index.en.html",
    "href": "docs/posts/2022-03-22-data-science-and-the-epistemic-rodeo/index.en.html",
    "title": "Data science is a saddle for the epistemic rodeo: healthcare as a complex system",
    "section": "",
    "text": "Healthcare is complicated. Healthcare is also complex. To use data science to improve treatment delivery in low-resource settings, it is important to understand in what way healthcare is complicated and in what was it is complex.\nWe will turn to the Cynefin (pronounced ku-nev-in) framework to understand the difference between complicated and complex contexts."
  },
  {
    "objectID": "docs/posts/2022-03-22-data-science-and-the-epistemic-rodeo/index.en.html#clinical-care-is-complicated",
    "href": "docs/posts/2022-03-22-data-science-and-the-epistemic-rodeo/index.en.html#clinical-care-is-complicated",
    "title": "Data science is a saddle for the epistemic rodeo: healthcare as a complex system",
    "section": "Clinical care is complicated",
    "text": "Clinical care is complicated\nDoctors and nurses require incredible expertise to do their job well. We know a lot about the human body and about a multitude of disease states. Providers use that knowledge to gather data about the patient’s clinical presentation to arrive at a correct diagnosis or to identify effective treatments. Expert knowledge and experience is the key.\nImportantly, although each individual patient’s body is unique and ever changing, the application of biological knowledge in the in the context of diagnosis and treatment is a stable process. Diseases act in more-or-less in predictable ways. When diseases are unpredictable, the steady application of clinical knowledge can provide further answers and treatments."
  },
  {
    "objectID": "docs/posts/2022-03-22-data-science-and-the-epistemic-rodeo/index.en.html#healthcare-systems-are-complex",
    "href": "docs/posts/2022-03-22-data-science-and-the-epistemic-rodeo/index.en.html#healthcare-systems-are-complex",
    "title": "Data science is a saddle for the epistemic rodeo: healthcare as a complex system",
    "section": "Healthcare systems are complex",
    "text": "Healthcare systems are complex\nThe healthcare system that is built to deliver care to patients is comprised of many moving parts that are hierarchically nested within functional units that interact in nonlinear ways. Hospitals, clinics, government agencies, nongovernment organizations are all key types of entities that comprise the system. Within each entity there are doctors, nurses, politicians, administrators, and many other types of actors. These entities and actors must perform many functions to deliver medical goods and services to patients. Some actors can react to the activity of others, such as changes in insurance coverage or new legislation, causing unpredictable perturbations throughout the system.\nFrom the perspective of an individual clinic or hospital, these outside forces influence the treatment centers ability to care for patients in many ways. Medications must be secured, equipment must be serviced, staffing must be adequately maintained, and information systems must be kept up to date. The center’s ability to maintain these services depends on both the proper functioning of internal operations and the continued management of the ever changing external forces. Add on top of all of that the occasional pandemic or other chaotic force that emerges from the darkness to completely disrupt operations, and it can feel is if you are in an epistemic rodeo, holding on for dear life as new information tosses you about."
  },
  {
    "objectID": "docs/posts/2022-03-22-data-science-and-the-epistemic-rodeo/index.en.html#data-science-helps-you-navigate-complex-systems",
    "href": "docs/posts/2022-03-22-data-science-and-the-epistemic-rodeo/index.en.html#data-science-helps-you-navigate-complex-systems",
    "title": "Data science is a saddle for the epistemic rodeo: healthcare as a complex system",
    "section": "Data science helps you navigate complex systems",
    "text": "Data science helps you navigate complex systems\nTaking another insight from the Cynefin framework, in complex contexts the best way to act is to probe, sense, and respond. Probing implies that you have sensors in the system that provide information on how it evolves over time. This equips you with the ability to experiment with small changes to see how the system responds. Sensing is the act of gathering the information about the change in the system to the small changes. This should give sufficient information to know how to respond more effectively to guide the system to the desired outcome. The probe-sense-respond process is similar to several PDSA cycles in QI, but rapid sensing and flexibility with responses are key.\nData science provides all of the required equipment to succeed in the epistemic rodeo by allowing you to probe-sense-respond across all of your organizational complexities. Like a saddle on a bull, data science will stabilize your knowledge and give you something to hold on to as you start to feel the kick from the next challenge. Soon you’ll understand how your complex systems behaves and be able to direct it to gallop wherever you need it to go."
  },
  {
    "objectID": "docs/posts/2022-04-10-pediatric_abandonment/index.en.html",
    "href": "docs/posts/2022-04-10-pediatric_abandonment/index.en.html",
    "title": "Pediatric cancer treatment abandonment: a tragic but preventable event",
    "section": "",
    "text": "A tragic reality that pediatric oncology providers in resource-constrained commonly encounter: after a child with cancer starts treatment, many families leave and never return for care.\nThe global oncology community has given this event a specific label: treatment abandonment.\nTo explain why treatment abandonment happens and how to prevent it, let me tell you a story. The narrative is fictional, but it is based on the common experiences that families report while their child is undergoing cancer treatment.\nPablo was a five-year-old boy who lived with his mom and two younger sisters in a rural village in Central America.\nHe liked to do things most little boys like to do – kick a ball around in the street, chase his sisters with bugs, and help his mom care for their chickens. One day he developed a fever and became very pale. Over the next two weeks, the fevers didn’t stop, so his mom took him to the clinic in his village, where he saw a nurse who decided to send them for blood tests at the nearest hospital.\nIt was two days before his mom could secure a ride to make the normally two-hour journey to the hospital that took four hours because rain turned the dirt roads leading out of the village to mud. Workers at the hospital took blood from Pablo and after a few hours, a somber-looking doctor told his mom the bad news.\n\nThe doctor was concerned Pablo had a life-threatening blood cancer called “leukemia”.\nPablo’s mom had a 3rd-grade education. She primarily spoke the indigenous language of her village and was merely conversational in Spanish, her doctor’s primary language. The doctor spent an hour talking to her about many things, much of which she didn’t understand. She knew her son had a disease in his blood, and she knew it was life-threatening, but she still wasn’t clear how he got it and feared negative spiritual forces were at work.\nThe doctors started treatment in the hospital, and after a few days, Pablo started looking much better, almost back to normal. He was discharged from the hospital and told he had appointments to come back every week for the next several months. Their family had government insurance that covered part of the hospital stay, but they would still have to pay the costs of travel, food, and lodging for every visit.\nPablo’s aunt and uncle lived in the city where the hospital was located.\n\n\nPablo and his mom moved in with his aunt and uncle, their 4 kids, 2 dogs, and a coop full of chickens to save money.\nHis sisters had to stay with their grandmother back in the village.\nAfter five weeks of treatment, his aunt was convinced that he was cured. “He looks fine,” she would tell his mom, “how could he still possibly be sick?” She would also insist that many doctors are greedy and give people medicine only to make money. Over time, his mother did notice that Pablo seemed to look sicker after visiting the doctor and receiving treatment. Perhaps his aunt was right, she would think to herself.\n\n\nBy week 12 of treatment, Pablo’s mom felt she was near a breaking point.\nShe lost her job because of her long absence, and they were dependent on financial support from extended family members who had little disposable income. Living in the full house with their relatives was cramped and noisy, and the family felt like a burden. Pablo was sick most of the time and would cry at night because he missed his sisters.\nHis mom tried to talk to the doctors several times about their difficulties, but she felt she didn’t get her point across in her broken Spanish, and it appeared to her that the busy doctors did not have time to understand her problems.\nEventually, his mother heard about a traditional religious healer near their village whose treatments were much cheaper. She talked with several family members who told her the traditional healer had helped them when they were sick. This was enough to help her make the decision.\n\n\nThat day they packed up and returned to their home. They said nothing to their doctors, nor did they return to finish treatment.\nCancer treatment can be a brutal journey, but if the entire treatment is not finished, the patient is at extremely high risk for the aggressive return of their disease.\nThis narrative illustrates the corrosive effects of cancer on the whole family.\n\nFinancial difficulties\nGeographic barriers to care\nDisrupted life rhythms and social strain\nPoor communication with the healthcare team\nPoor understanding of cancer and how to successfully treat it\nReliance on extended family for material and financial support\n\n\n\nThe answer to treatment abandonment is to understand the difficulties of a family’s journey and support them through it.\nThere is nothing overly complex here. Families need support during the treatment journey. Financial assistance, material support with food and housing assistance, improved family education and communication with the healthcare team, and initiatives about how communities can support families through treatment all help to dramatically reduce treatment abandonment rates. There is a significant body of published research that demonstrates the efficacy of all of these interventions.\nThe link between family support and the successful completion of treatment is so strong, that many providers argue it should be prioritized as important as medicine or surgery.\n\n\nSupporting families financially, psychologically, socially, and spiritually is an essential part of pediatric cancer treatment."
  },
  {
    "objectID": "docs/posts/2022-04-04-htma-2-universal_approach/index.en.html",
    "href": "docs/posts/2022-04-04-htma-2-universal_approach/index.en.html",
    "title": "How to measure anything in Global Health: 2. The universal approach to measurement",
    "section": "",
    "text": "If you want to use data to improve treatment delivery in global health, all of your measurements should support decisions.\nFor example, suppose you run an oncology clinic and consider introducing a new treatment, drug X, into your current treatment regimen for Burkitt lymphoma (BL). It has been proven in studies in the United States to improve outcomes by 10% in BL. It is also expensive, has side effects that may increase toxicity, and will take additional clinic capacity and new workflows to administer. Should you start to offer drug X for BL?\nTo decide, use the universal approach to measurement to reduce uncertainty about the possible outcomes.\nThis material comes from How to Measure Anything (HTMA), the legendary book by Douglas Hubbard that, you guessed it, teaches you to measure anything. Check out the book if you like this material."
  },
  {
    "objectID": "docs/posts/2022-04-04-htma-2-universal_approach/index.en.html#define-a-decision-problem-and-the-relevant-uncertainties",
    "href": "docs/posts/2022-04-04-htma-2-universal_approach/index.en.html#define-a-decision-problem-and-the-relevant-uncertainties",
    "title": "How to measure anything in Global Health: 2. The universal approach to measurement",
    "section": "1. Define a decision problem and the relevant uncertainties",
    "text": "1. Define a decision problem and the relevant uncertainties\nPrecisely specify the decision you are trying to make - “Should we add drug X to the treatment for intermediate and high-risk BL or continue with current therapy?” Also, specify the unknowns that will impact the decision - “We need to know the improvement in survival, cost, possible toxicities and required supportive therapies, and requirements and cost associated with administration of the drug.” Relate how the values of the variables combine to produce the decision. Be very specific."
  },
  {
    "objectID": "docs/posts/2022-04-04-htma-2-universal_approach/index.en.html#determine-what-you-know-now",
    "href": "docs/posts/2022-04-04-htma-2-universal_approach/index.en.html#determine-what-you-know-now",
    "title": "How to measure anything in Global Health: 2. The universal approach to measurement",
    "section": "2. Determine what you know now",
    "text": "2. Determine what you know now\nUse the content experts to describe and quantify the current level of uncertainty. HTMA recommends calibrating the content experts through a series of exercises to have them estimate their 90% uncertainty intervals for a variety of estimation tasks. We’ll cover this more in upcoming essays."
  },
  {
    "objectID": "docs/posts/2022-04-04-htma-2-universal_approach/index.en.html#compute-the-value-of-additional-information",
    "href": "docs/posts/2022-04-04-htma-2-universal_approach/index.en.html#compute-the-value-of-additional-information",
    "title": "How to measure anything in Global Health: 2. The universal approach to measurement",
    "section": "3. Compute the value of additional information",
    "text": "3. Compute the value of additional information\n“Information has value because it reduces risk in decisions.” Identify high-value variables to measure by estimating how much value you will gain by being more certain about the variables listed in step 1. How would it affect your decision if your uncertainty about the total cost of the drug went from $500-$5,000 per patient to $800-$1500 per patient? HTMA has much to say on this, which we will cover in the future."
  },
  {
    "objectID": "docs/posts/2022-04-04-htma-2-universal_approach/index.en.html#apply-the-relevant-measurement-instruments-to-high-value-measurements",
    "href": "docs/posts/2022-04-04-htma-2-universal_approach/index.en.html#apply-the-relevant-measurement-instruments-to-high-value-measurements",
    "title": "How to measure anything in Global Health: 2. The universal approach to measurement",
    "section": "4. Apply the relevant measurement instruments to high-value measurements",
    "text": "4. Apply the relevant measurement instruments to high-value measurements\nApply simple measurement methods to reduce your uncertainty about the high-value variables identified in steps 1-4. There are many excellent, low-cost ways to accomplish this task, which I will address in time."
  },
  {
    "objectID": "docs/posts/2022-04-04-htma-2-universal_approach/index.en.html#make-a-decision-and-act-on-it",
    "href": "docs/posts/2022-04-04-htma-2-universal_approach/index.en.html#make-a-decision-and-act-on-it",
    "title": "How to measure anything in Global Health: 2. The universal approach to measurement",
    "section": "5. Make a decision and act on it",
    "text": "5. Make a decision and act on it\nAfter measurement, integrate what you know into the decision pathways in step 1 and estimate the balance of risk versus reward for the different decisions.\nThen decide and have confidence in your glorious theory-informed, evidence-based, measurement-backed decision to improve your patients’ care."
  },
  {
    "objectID": "docs/epistemic_limits/into_the_unknown/index.html",
    "href": "docs/epistemic_limits/into_the_unknown/index.html",
    "title": "Into the Unknown",
    "section": "",
    "text": "Into the unknowwwwwwwwwwn!"
  },
  {
    "objectID": "docs/epistemic_limits/limit_zoo/index.html",
    "href": "docs/epistemic_limits/limit_zoo/index.html",
    "title": "The Limit Zoo",
    "section": "",
    "text": "It’s a zoo full of cages, and we are in each one."
  },
  {
    "objectID": "docs/epistemic_limits/menagerie/index.html",
    "href": "docs/epistemic_limits/menagerie/index.html",
    "title": "The Menagerie of Unknowables",
    "section": "",
    "text": "I’m starting a collection!"
  },
  {
    "objectID": "docs/epistemic_limits/menagerie/absolute_limits/index.html",
    "href": "docs/epistemic_limits/menagerie/absolute_limits/index.html",
    "title": "Absolute Limits",
    "section": "",
    "text": "Explanation: These are limits for any Learner within this universe, no matter their intellectual capacity. These are fundamental limits that govern all relationships that obtain in this universe.\n\n\n\n\n\n\n\n\nType\nDescription\nExamples\n\n\n\n\nLogical Limits\nLimits to what can be proven or known through formal reasoning\nThe liar paradox, barber paradox\n\n\nPhysical Limits\nLimits to what can be observed or measured due to the laws of physics\nThe uncertainty principle, the speed of light, the no-cloning theorem\n\n\nMathematical Limits\nLimits to what can be proven or calculated using mathematical methods\nGödel’s incompleteness theorems, detecting randomness?\n\n\nComputational Limits\nLimits to what can be computed or predicted by an algorithm\nThe Halting Problem, factoring large numbers, solving the traveling salesman problem\n\n\nPhenomenological Limits\nLimits on what can be known in experience.\nBrain in a vat, solipsism"
  },
  {
    "objectID": "docs/epistemic_limits/menagerie/absolute_limits/index.html#first-order-absolute-limits",
    "href": "docs/epistemic_limits/menagerie/absolute_limits/index.html#first-order-absolute-limits",
    "title": "Absolute Limits",
    "section": "",
    "text": "Explanation: These are limits for any Learner within this universe, no matter their intellectual capacity. These are fundamental limits that govern all relationships that obtain in this universe.\n\n\n\n\n\n\n\n\nType\nDescription\nExamples\n\n\n\n\nLogical Limits\nLimits to what can be proven or known through formal reasoning\nThe liar paradox, barber paradox\n\n\nPhysical Limits\nLimits to what can be observed or measured due to the laws of physics\nThe uncertainty principle, the speed of light, the no-cloning theorem\n\n\nMathematical Limits\nLimits to what can be proven or calculated using mathematical methods\nGödel’s incompleteness theorems, detecting randomness?\n\n\nComputational Limits\nLimits to what can be computed or predicted by an algorithm\nThe Halting Problem, factoring large numbers, solving the traveling salesman problem\n\n\nPhenomenological Limits\nLimits on what can be known in experience.\nBrain in a vat, solipsism"
  },
  {
    "objectID": "docs/epistemic_limits/menagerie/absolute_limits/index.html#second-order-absolute-limits",
    "href": "docs/epistemic_limits/menagerie/absolute_limits/index.html#second-order-absolute-limits",
    "title": "Absolute Limits",
    "section": "Second-Order Absolute Limits",
    "text": "Second-Order Absolute Limits\nExplanation: These are limits for any Learner within this universe, no matter their intellectual capacity. These are derivative of the Absolute Epistemic Limits in that they would not obtain in this universe if the absolute epistemic limits did not obtain. However, the absolute epistemic limits could still obtain without these. Learners may wish to engage in the activities represented by these limits, but their knowledge will always be limited due to interaction between these activities at the first-order absolute limits.\n\n\n\n\n\n\n\n\nType\nDescription\nExamples\n\n\n\n\nRetrodictive Limits\nLimits to what can be known or understood about past events or cultures\nThe limits of historical knowledge, the bias of historical sources\n\n\nPredictive Limits\nLimits to how accurately we can predict future events or behaviors\nThe limits of prediction, chaos theory\n\n\nScientific Limits\nLimits to what can be known or understood through scientific inquiry\nThe limits of scientific knowledge, the difficulty of studying complex phenomena\n\n\nTechnological Limits\nLimits imposed by the technology used to measure or observe phenomena\nThe limits of resolution, signal-to-noise ratio\n\n\nCommunication Limits\nLimits to what can be communicated or understood through symbolic representation\nThe limits of translation, the difficulty of expressing certain concepts in language"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Doctor's Dialectic",
    "section": "",
    "text": "Dī-ə-ˈlek-tik: A method of examining and discussing opposing ideas in order to find the truth.\n\nA dialectic is a conversation between people with opposing views who wish to find truth through reasoned arguments.\nThis is an ancient form of collective reasoning. Popularized by Socrates in 400 BC, it remains a crucial method for forming social consensus and driving scientific progress.\nAmid the noise and nonsense of modern life, a thoughtful dialectic can help us discover truth and meaning in a time when both are increasingly difficult to find."
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "The Doctor's Dialectic",
    "section": "",
    "text": "Dī-ə-ˈlek-tik: A method of examining and discussing opposing ideas in order to find the truth.\n\nA dialectic is a conversation between people with opposing views who wish to find truth through reasoned arguments.\nThis is an ancient form of collective reasoning. Popularized by Socrates in 400 BC, it remains a crucial method for forming social consensus and driving scientific progress.\nAmid the noise and nonsense of modern life, a thoughtful dialectic can help us discover truth and meaning in a time when both are increasingly difficult to find."
  },
  {
    "objectID": "docs/posts/2025-01-29-trial/index.html#the-contentification-of-life",
    "href": "docs/posts/2025-01-29-trial/index.html#the-contentification-of-life",
    "title": "Gratitude reduces the complexity of modern life to make it more meaningful",
    "section": "The contentification of life",
    "text": "The contentification of life\nThe world is increasingly filled with noise and nonsense, making it difficult to see what is valuable amid a torrent of trivialities. With the rise of social media, our attention has become an economic commodity. Four million new posts are made on Instagram every hour; 3.7 million new YouTube videos are uploaded every day. This is chump change compared to what’s coming. The rate of content production is going to increase exponentially as generative AI matures in consumer applications. And the contentification of the internet for the commodification of our attention has overflowed into the offline world. Everywhere we turn, we’re offered the chance to level up in our jobs, our relationships, our skills, or our goals (and on Facebook, for me, level up my hip mobility because I clicked on an ad about stretching once). There’s always more to see, more to learn, and more to do to maximize the moment and optimize our happiness. Almost all of this is a heap of rubbish. Yet there are always small, buried treasures that keep us coming back to rummage through the garbage."
  },
  {
    "objectID": "docs/posts/2025-01-29-trial/index.html#the-exponentiation-of-life",
    "href": "docs/posts/2025-01-29-trial/index.html#the-exponentiation-of-life",
    "title": "Gratitude reduces the complexity of modern life to make it more meaningful",
    "section": "The exponentiation of life",
    "text": "The exponentiation of life\nBeyond the value of the content we produce, the speed of production also gives us heartburn. We humans, evolved in the cradle of the natural numbers, nourished as a species by the small positive integers that we found on our fingers and our toes, have difficulty handling the exponentiation of possible human experiences. There are so many things we could do. So many ways to spend our time. So many counterfactual worlds unfolding at ever increasing speeds ahead of us. Navigating life is increasingly complex, increasingly overwhelming. Our attention can’t handle it all."
  },
  {
    "objectID": "docs/posts/2025-01-29-trial/index.html#many-options-that-mostly-stink",
    "href": "docs/posts/2025-01-29-trial/index.html#many-options-that-mostly-stink",
    "title": "Gratitude reduces the complexity of modern life to make it more meaningful",
    "section": "Many options that mostly stink",
    "text": "Many options that mostly stink\nSo, we have tons of ways we could spend our time, energy, and attention. Too many. Most of these options are garbage. But it’s hard to tell upfront what is worthless and what is valuable because attention is money, so everything is marketing. Everything has a new coat of paint and has been spritzed with perfume. What do we do when there are so many things we could do, but it’s hard to tell which are wastes of time? How do we reduce the dimensionality of our options with so much noisy data?"
  },
  {
    "objectID": "docs/posts/2025-01-29-trial/index.html#gratitude-can-help",
    "href": "docs/posts/2025-01-29-trial/index.html#gratitude-can-help",
    "title": "Gratitude reduces the complexity of modern life to make it more meaningful",
    "section": "Gratitude can help",
    "text": "Gratitude can help\nGratitude can help us focus on what is valuable. Being grateful for something implies a host of magnificent and healthy epistemic processes. For example, I’m grateful for my dog, Rosie. I’m particularly grateful that she’s acting less like a puppy and more like a productive member of my family’s household society. To tell you about my gratitude for my dog’s behavior, I had to first search through all sorts of things in my life I could tell you about. Rosie was easy to find in this search process. However the algorithm works, it was efficient and accurate - it didn’t take me long to come up with an example and I’m certain about my feeling here. To articulate my gratitude, I then had to particularize Rosie as an object of my attention. Rosie is not an abstract concept or just an idea. She became an object of my attention with properties and characteristics that I can describe. In other words, I focused on her to the exclusion of anything else. Once presented in my awareness, my relationship with her takes center stage. I’m grateful for her because of how we interact. Not just because of who she is or who I am but because of who we are together. We’re correlated statistically. What I think is associated with who she is. We’re coupled cybernetically. How she exists influences what I do. My gratitude is about our existence together - a dynamic, fluid, abstract-yet-influential relationship that helps to direct my experiences and actions over time."
  },
  {
    "objectID": "docs/posts/2025-01-29-trial/index.html#gratitude-as-a-signal-of-value",
    "href": "docs/posts/2025-01-29-trial/index.html#gratitude-as-a-signal-of-value",
    "title": "Gratitude reduces the complexity of modern life to make it more meaningful",
    "section": "Gratitude as a signal of value",
    "text": "Gratitude as a signal of value\nRosie has helped me find what is valuable. My gratitude acted as both search and filter for finding something that has meaning to me. Easy, quick, accurate. It doesn’t matter how many more things I add to search through or how much more content humanity produces; it will still be easy to identify her - and a host of other good things in my life - as something I’m grateful for. This salience in my focus, this worthiness of attention that gratitude produces, is something I can only describe as meaningful. It fills that epistemic moment with a sense of value. Gratitude helps us find the signals of value in the noise of modern life."
  },
  {
    "objectID": "docs/posts/2025-01-29-trial/index.html#gratitude-as-dimensionality-reduction",
    "href": "docs/posts/2025-01-29-trial/index.html#gratitude-as-dimensionality-reduction",
    "title": "Gratitude reduces the complexity of modern life to make it more meaningful",
    "section": "Gratitude as dimensionality reduction",
    "text": "Gratitude as dimensionality reduction\nTo be grateful for Rosie, I also had to focus on her to the exclusion of anything else. There are many, perhaps uncountably infinite, things I could be grateful for. There are also many, perhaps uncountably infinite, ways that I could be grateful for Rosie. Relationships are dynamic. They change in quality over time. But from all of these possibilities, I could wrap my attention around a thing known as Rosie and describe her in a way that is comprehensible to you in just a few words. I could envelop the intricate complexities of our relationship, an immaterial thing, to communicate the positive experience I take from it. Considering how Rosie and I can move through life together, there are yet infinitely more ways that we could act and interact, and yet my gratitude at where we’ve been suggests possible futures that are valuable and that would make me grateful anew. In a sea of infinite options, gratitude has reduced the dimensions I must consider, helping me reflect on where I’ve been and chart a course for the future."
  },
  {
    "objectID": "docs/posts/2025-01-29-trial/index.html#gratitude-in-2024",
    "href": "docs/posts/2025-01-29-trial/index.html#gratitude-in-2024",
    "title": "Gratitude reduces the complexity of modern life to make it more meaningful",
    "section": "Gratitude in 2024",
    "text": "Gratitude in 2024\nThis has been a meditation on part of life I find overwhelming. I want to do good stuff. I want to be good at the things I do. I want to have meaningful relationships. I want to live my life well. Many and more are the ways this could happen or could go wrong. There are so many dang options for how to spend my time. I’m extremely grateful for my dog, obviously, as well as my family, friends, job, and God. Gratitude helps me slow down, find those things that are meaningful, and savor the good dynamics of those relationships. It also helps me to think about how to nourish those good things so they bloom into something beautiful. Being grateful for what I’ve experienced in 2023 helps me plan what to do in 2024. I have goals, but gratitude suggests the purpose of planning isn’t to achieve goals but to help what is true, beautiful, and good in my life flourish."
  }
]